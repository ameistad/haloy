This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    build.yml
cmd/
  cli/
    main.go
  manager/
    main.go
dev/
  manager/
    test-container/
      .current-version
      build.sh
      Dockerfile
      index.html
      run.sh
    build.sh
    Dockerfile.dev
    run.sh
internal/
  cli/
    commands/
      completion.go
      deploy.go
      init.go
      list.go
      rollback.go
      root.go
      secrets.go
      start.go
      status.go
      stop.go
      validate.go
      version.go
  config/
    config.go
    env.go
    helpers.go
    labels.go
    paths.go
    secrets.go
    source_test.go
    source.go
    validate_test.go
    validate.go
  deploy/
    constants.go
    deploy.go
    rollback.go
    types.go
  docker/
    client.go
    container.go
    image.go
    network.go
    services.go
  embed/
    init/
      containers/
        error-pages/
          404.html
        docker-compose.yml
        entrypoint.sh
      test-app/
        Dockerfile
        haloy-curlew.svg
        health.html
        index.html
    templates/
      apps-with-test-app.yml
      apps.yml
      haproxy.cfg
    embed.go
    types.go
  helpers/
    debouncer.go
    networkutils.go
    sanitize_test.go
    sanitize.go
    validators.go
  logging/
    logging.go
  manager/
    certificates.go
    deployments.go
    haproxy.go
    manager.go
    updater.go
  ui/
    ui.go
  version/
    version.go
scripts/
  build-upload-cli-manager.sh
  build-upload-cli.sh
  del-tags.sh
  local-build-cli.sh
  remove-haloy.sh
.gitignore
go.mod
LICENSE
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/build.yml">
name: Build and Release

on:
  push:
    tags: [ 'v*' ]
  pull_request:
    branches: [ main ]

permissions:
  contents: write
  packages: write

jobs:
  test:
    name: Test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.24'
          cache: false  # Disabling cache to avoid tar extraction errors

      - name: Run tests
        run: go test -v ./...

  build-and-publish:
    name: Build and Publish
    needs: test
    # Only run this job for tag pushes, not branch pushes or PRs
    if: startsWith(github.ref, 'refs/tags/')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.24'
          cache: false  # Disabling cache to avoid tar extraction errors

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Extract metadata for Docker
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ghcr.io/${{ github.repository }}-manager
          tags: |
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,format=long
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push manager image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./build/manager/Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build CLI for multiple platforms
        env:
          VERSION: ${{ github.ref_name }}
        run: |
          mkdir -p dist
          # PLATFORMS=("linux/amd64" "linux/arm64" "darwin/amd64" "darwin/arm64" "windows/amd64")
          PLATFORMS=("linux/amd64")

          for platform in "${PLATFORMS[@]}"; do
            IFS="/" read -r -a parts <<< "$platform"
            GOOS="${parts[0]}"
            GOARCH="${parts[1]}"

            # Set the output filename based on OS
            if [ "$GOOS" == "windows" ]; then
              OUTPUT="dist/haloy-${GOOS}-${GOARCH}.exe"
            else
              OUTPUT="dist/haloy-${GOOS}-${GOARCH}"
            fi

            echo "Building for $GOOS/$GOARCH..."
            GOOS=$GOOS GOARCH=$GOARCH go build -ldflags="-X 'github.com/ameistad/haloy/internal/version.Version=${VERSION}'" -o "$OUTPUT" ./cmd/cli
          done

          # Create checksums
          cd dist
          sha256sum * > checksums.txt
          cd ..

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: haloy-binaries
          path: dist/

  release:
    name: Create Release
    needs: build-and-publish
    if: startsWith(github.ref, 'refs/tags/')
    runs-on: ubuntu-latest
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          name: haloy-binaries
          path: dist/

      - name: Create release
        uses: softprops/action-gh-release@v1
        with:
          files: |
            dist/haloy-linux-amd64
            # dist/haloy-linux-arm64
            # dist/haloy-darwin-amd64
            # dist/haloy-darwin-arm64
            # dist/haloy-windows-amd64.exe
            dist/checksums.txt
          draft: false
          prerelease: ${{ contains(github.ref, 'alpha') || contains(github.ref, 'beta') || contains(github.ref, 'rc') }}
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path="cmd/cli/main.go">
package main

import (
	"fmt"
	"os"

	"github.com/ameistad/haloy/internal/cli/commands"
)

func main() {
	rootCmd := commands.NewRootCmd()
	if err := rootCmd.Execute(); err != nil {
		// Print error once, then exit
		fmt.Fprintln(os.Stderr, err)
		os.Exit(1)
	}
}
</file>

<file path="dev/manager/test-container/.current-version">
20250526202615
</file>

<file path="dev/manager/test-container/build.sh">
#!/bin/bash

# Generate a version tag based on current timestamp
VERSION=$(date +%Y%m%d%H%M%S)

# Build the image with both latest and versioned tags
docker build -t my-nginx:latest -t my-nginx:v${VERSION} .

echo "Built my-nginx:latest and my-nginx:v${VERSION}"

# Optional: Save the version to a file for the run script to use
echo "${VERSION}" > .current-version

echo "Version ${VERSION} saved to .current-version"
</file>

<file path="dev/manager/test-container/Dockerfile">
FROM nginx:alpine

# Copy the main website content
COPY index.html /usr/share/nginx/html/

# Create a health check endpoint
RUN echo "OK" > /usr/share/nginx/html/health

# Enable Docker's built-in healthcheck
HEALTHCHECK --interval=5s --timeout=3s --retries=3 \
  CMD wget -q --spider http://localhost/health || exit 1

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
</file>

<file path="dev/manager/test-container/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Test haloy</title>
</head>
<body>
<pre>
	Hello!
</pre>
</body>
</html>
</file>

<file path="dev/manager/test-container/run.sh">
DEPLOYMENT_ID=$(date +%Y%m%d%H%M%S)
DEPLOYMENT_ID_STATIC=20250318152205

docker run --name my-nginx-container-two-${DEPLOYMENT_ID} \
  --network haloy-public \
  -l "haloy.appName=my-nginx-container" \
  -l "haloy.deployment-id=${DEPLOYMENT_ID}" \
  -l "haloy.acme.email=test@haloy.dev" \
  -l "haloy.domain.0=domain.com" \
  -l "haloy.domain.0.alias.0=www.domain.com" \
  -l "haloy.health-check-path=/health" \
  -l "haloy.port=80" \
  -l "haloy.role=app" \
  -e "NODE_ENV=production" \
  my-nginx
</file>

<file path="dev/manager/build.sh">
#!/bin/bash
cd $(git rev-parse --show-toplevel)
docker build -t haloy-manager-dev -f ./dev/manager/Dockerfile.dev .
</file>

<file path="internal/cli/commands/completion.go">
package commands

import (
	"fmt"
	"os"

	"github.com/spf13/cobra"
)

// NewCompletionCmd creates a new completion command
func CompletionCmd() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "completion [bash|zsh|fish|powershell]",
		Short: "Generate completion script",
		Args:  cobra.ExactArgs(1),
		Long: `To load completions:

Bash:
  $ source <(haloy completion bash)
  # Permanently:
  $ haloy completion bash > /etc/bash_completion.d/haloy  # Linux
  $ haloy completion bash > /usr/local/etc/bash_completion.d/haloy  # macOS

Zsh:
  $ echo "autoload -U compinit; compinit" >> ~/.zshrc
  $ source <(haloy completion zsh)

Fish:
  $ haloy completion fish | source

Powershell:
  PS> haloy completion powershell | Out-String | Invoke-Expression
`,
		RunE: func(cmd *cobra.Command, args []string) error {
			switch args[0] {
			case "bash":
				return cmd.Root().GenBashCompletion(os.Stdout)
			case "zsh":
				return cmd.Root().GenZshCompletion(os.Stdout)
			case "fish":
				return cmd.Root().GenFishCompletion(os.Stdout, true)
			case "powershell":
				return cmd.Root().GenPowerShellCompletionWithDesc(os.Stdout)
			default:
				return fmt.Errorf("unsupported shell type: %s", args[0])
			}
		},
	}

	return cmd
}
</file>

<file path="internal/cli/commands/list.go">
package commands

import (
	"fmt"

	"github.com/ameistad/haloy/internal/config"
	"github.com/spf13/cobra"
)

func ListAppsCmd() *cobra.Command {
	listAppsCmd := &cobra.Command{
		Use:   "list",
		Short: "List all apps from config",
		RunE: func(cmd *cobra.Command, args []string) error {
			confFilePath, err := config.ConfigFilePath()
			if err != nil {
				return err
			}

			confFile, err := config.LoadAndValidateConfig(confFilePath)
			if err != nil {
				return fmt.Errorf("configuration error: %w", err)
			}

			fmt.Println("Apps in config:")
			for _, app := range confFile.Apps {
				fmt.Printf(" - %s\n", app.Name)
			}
			return nil
		},
	}
	return listAppsCmd
}
</file>

<file path="internal/cli/commands/stop.go">
package commands

import (
	"context"
	"fmt"
	"time"

	"github.com/ameistad/haloy/internal/docker"
	"github.com/ameistad/haloy/internal/ui"
	"github.com/spf13/cobra"
)

const (
	stopAppTimeout = 2 * time.Minute // Timeout for stop operations
)

func StopAppCmd() *cobra.Command {
	var removeContainersFlag bool

	cmd := &cobra.Command{
		Use:   "stop <app-name>",
		Short: "Stop an application's running containers",
		Long: `Stops all running containers associated with the specified application.
Containers are identified by the 'haloy.app=<app-name>' label.
Optionally, it can also remove the containers after stopping them.`,
		Args: func(cmd *cobra.Command, args []string) error {
			if len(args) != 1 {
				return fmt.Errorf("stop command requires exactly one argument: the app name (e.g., 'haloy stop my-app')")
			}
			return nil
		},
		Run: func(cmd *cobra.Command, args []string) {
			appName := args[0]

			ctx, cancel := context.WithTimeout(context.Background(), stopAppTimeout)
			defer cancel()

			dockerClient, err := docker.NewClient(ctx)
			if err != nil {
				ui.Error("Failed to create Docker client: %v\n", err)
				return
			}
			defer dockerClient.Close()

			stoppedIDs, err := docker.StopContainers(ctx, dockerClient, appName, "")
			if err != nil {
				ui.Error("Error while stopping containers for app %q: %v\n", appName, err)
				// If stopping failed and nothing was stopped, it's probably best to return.
				if len(stoppedIDs) == 0 {
					return
				}
			}

			if len(stoppedIDs) > 0 {
				ui.Success("Successfully stopped %d container(s) for app %q.\n", len(stoppedIDs), appName)
			} else {
				ui.Info("No running containers found for app %q to stop.\n", appName)
			}

			if removeContainersFlag {
				ui.Info("Attempting to remove containers for app %q...\n", appName)
				removeParams := docker.RemoveContainersParams{
					Context:             ctx,
					DockerClient:        dockerClient,
					AppName:             appName,
					IgnoreDeploymentID:  "", // No specific deployment to ignore
					MaxContainersToKeep: 0,  // Remove all, don't keep any
				}
				removedIDs, removeErr := docker.RemoveContainers(removeParams)
				if removeErr != nil {
					ui.Error("Error while removing containers for app %q: %v\n", appName, removeErr)
				}

				if len(removedIDs) > 0 {
					ui.Success("Successfully removed %d container(s) for app %q.\n", len(removedIDs), appName)
				} else {
					if removeErr == nil { // No error, but no containers removed
						ui.Info("No containers found for app %q to remove.\n", appName)
					}
					// If removeErr != nil, the error message was already printed.
				}
			}
		},
	}

	cmd.Flags().BoolVarP(&removeContainersFlag, "remove-containers", "r", false, "Remove the containers after stopping them")
	return cmd
}
</file>

<file path="internal/cli/commands/version.go">
package commands

import (
	"fmt"

	"github.com/ameistad/haloy/internal/version"
	"github.com/spf13/cobra"
)

// VersionCmd creates a new version command
func VersionCmd() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "version",
		Short: "Print the current version of haloy",
		Run: func(cmd *cobra.Command, args []string) {
			fmt.Printf("haloy %s\n", version.GetVersion())
		},
	}

	return cmd
}
</file>

<file path="internal/config/helpers.go">
package config

import "fmt"

func AppConfigByName(appName string) (*AppConfig, error) {
	configFilePath, err := ConfigFilePath()
	if err != nil {
		return nil, err
	}

	configFile, err := LoadAndValidateConfig(configFilePath)
	if err != nil {
		return nil, fmt.Errorf("configuration error: %w", err)
	}

	var appConfig *AppConfig
	for i := range configFile.Apps {
		if configFile.Apps[i].Name == appName {
			// Create a copy of the app config
			app := configFile.Apps[i]
			appConfig = &app
			break
		}
	}
	if appConfig == nil {
		return nil, fmt.Errorf("app '%s' not found in config", appName)
	}

	return appConfig, nil
}
</file>

<file path="internal/config/source_test.go">
package config

import (
	"os"
	"path/filepath"
	"testing"
	// ...existing code...
)

func TestSource_Validate(t *testing.T) {
	// Create a temporary directory for the test
	tempDir := t.TempDir()
	dockerfilePath := filepath.Join(tempDir, "Dockerfile")
	buildContextPath := tempDir // Use the same temp dir as build context

	// Create a dummy Dockerfile
	if err := os.WriteFile(dockerfilePath, []byte("FROM scratch"), 0644); err != nil {
		t.Fatalf("Failed to create temp Dockerfile: %v", err)
	}

	tests := []struct {
		name    string
		source  Source
		wantErr bool
	}{
		{
			name: "valid dockerfile source",
			source: Source{
				// Use the paths to the temporary files/dirs created above
				Dockerfile: &DockerfileSource{Path: dockerfilePath, BuildContext: buildContextPath},
			},
			wantErr: false, // Now expects false as the file exists
		},
		// ...existing code...
		{
			name: "invalid dockerfile missing path",
			source: Source{
				// Use a non-existent path within the temp dir for a controlled failure
				Dockerfile: &DockerfileSource{BuildContext: buildContextPath, Path: filepath.Join(tempDir, "nonexistent")},
			},
			wantErr: true, // This should still fail validation (file not found)
		},
		{
			name: "invalid dockerfile missing buildContext",
			source: Source{
				// Use a non-existent path for build context
				Dockerfile: &DockerfileSource{Path: dockerfilePath, BuildContext: filepath.Join(tempDir, "nonexistent_context")},
			},
			wantErr: true, // This should fail validation (build context not found)
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// We are now actually checking file existence via the temp files
			err := tt.source.Validate()
			if (err != nil) != tt.wantErr {
				// Provide more context on failure
				t.Errorf("Source.Validate() error = %v, wantErr %v", err, tt.wantErr)
			}
		})
	}
}
</file>

<file path="internal/config/validate_test.go">
package config

import (
	"testing"
)

func TestValidateDomain(t *testing.T) {
	tests := []struct {
		name    string
		domain  string
		wantErr bool
	}{
		{"valid domain", "example.com", false},
		{"valid domain with subdomain", "sub.example.co.uk", false},
		{"valid domain with hyphen", "example-domain.com", false},
		{"invalid TLD too short", "example.c", true},
		{"invalid no TLD", "example", true},
		{"invalid starting with hyphen", "-example.com", true},
		{"invalid domain with space", "example domain.com", true},
		{"empty domain", "", true},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if err := ValidateDomain(tt.domain); (err != nil) != tt.wantErr {
				t.Errorf("ValidateDomain() error = %v, wantErr %v", err, tt.wantErr)
			}
		})
	}
}

func TestValidateHealthCheckPath(t *testing.T) {
	tests := []struct {
		name    string
		path    string
		wantErr bool
	}{
		{"valid root path", "/", false},
		{"valid sub path", "/healthz", false},
		{"valid path with hyphen", "/health-check", false},
		{"valid path with numbers", "/status/123", false},
		{"invalid no leading slash", "health", true},
		{"invalid empty path", "", true},
		{"invalid path with query", "/health?check=true", false}, // Assuming query params are allowed
		{"invalid path fragment", "/health#status", false},       // Assuming fragments are allowed
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if err := ValidateHealthCheckPath(tt.path); (err != nil) != tt.wantErr {
				t.Errorf("ValidateHealthCheckPath() error = %v, wantErr %v", err, tt.wantErr)
			}
		})
	}
}

// TODO: Add tests for AppConfig.Validate() focusing on interactions between fields.
// TODO: Add tests for Config.Validate().
</file>

<file path="internal/docker/client.go">
package docker

import (
	"context"
	"errors"
	"fmt"
	"time"

	"github.com/docker/docker/client"
)

func NewClient(ctx context.Context) (*client.Client, error) {
	pingCtx, cancel := context.WithTimeout(ctx, 5*time.Second)
	defer cancel()

	dockerClient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())
	if err != nil {
		return nil, fmt.Errorf("failed to create Docker client: %w", err)
	}

	_, err = dockerClient.Ping(pingCtx)
	if err != nil {
		_ = dockerClient.Close() // Best effort close, ignore error
		if errors.Is(err, context.DeadlineExceeded) {
			return nil, fmt.Errorf("failed to connect to Docker daemon (timeout during ping): %w", err)
		}
		return nil, fmt.Errorf("failed to connect to Docker daemon (ping failed): %w", err)
	}
	return dockerClient, nil
}
</file>

<file path="internal/embed/init/containers/error-pages/404.html">
HTTP/1.0 404 Not Found
Cache-Control: no-cache
Connection: close
Content-Type: text/html

<!DOCTYPE html>
<html>
<head>
    <title>404 - page not found</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            background-color: #f8f9fa;
            color: #343a40;
            margin: 0;
            padding: 40px;
            line-height: 1.6;
        }
        .container {
            max-width: 600px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 5px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #0062cc;
            margin-top: 0;
        }
        p {
            margin-bottom: 20px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>404 - Not found</h1>
        <p>The page is not found.</p>
    </div>
</body>
</html>
</file>

<file path="internal/embed/init/test-app/Dockerfile">
FROM nginx:alpine

# Copy website files
COPY . /usr/share/nginx/html/

# Expose port 80
EXPOSE 80

# Default command
CMD ["nginx", "-g", "daemon off;"]
</file>

<file path="internal/embed/init/test-app/haloy-curlew.svg">
<svg xmlns="http://www.w3.org/2000/svg" width="53" height="38" fill="none">
    <path
    fill="#474747"
    d="M47.259 21.251c-1.366-.478-3.27-.801-4.942-1.003-1.77-.214-3.486-1.51-5.244-1.808-.576-.098-1.166-.116-1.759-.116-1.472 0-1.677.163-2.086.407-.456.272-1.145 1.057-1.8 1.748-.654.69-1.104.975-1.513 1.097-.41.122-1.555.081-2.21 0-.523-.065-1.008-.433-1.185-.61-.106-.087-.39-.353-.86-.847-.257-.27-.57-.608-.94-1.022-1.31-1.463-1.596-4.878-1.964-6.097-.368-1.22-3.559-2.886-5.4-4.024-1.84-1.138-2.658-1.585-4.581-2.601C10.852 5.359 4.43 1.782 1.894.522.092-.373-.037.003.127.748c.137.622-.26 1.336-.08 1.947.043.146.114.299.202.452.433.752.922 2.096 1.405 2.816.308.46.465 1.392.767 1.856.082.126.187.254.308.384.642.697 1.257 1.91 1.985 2.517.083.07.169.138.256.206.714.557 1.308 1.515 2.04 2.049.202.147.432.308.678.492.367.273.62.661.904 1.02.374.475.59.233 1.36 1.318.982 1.381.491.447.9 1.788.41 1.341 1.841 4.837 3.846 7.966 2.004 3.13.981 2.886-.573 4.715-.687.808-2.46 1.835-4.29 2.745-2.133 1.061-3.562 5.213-1.192 4.978 1.228-.122 2.987-1.382 4.337-2.195 1.35-.813 2.25-.854 3.722-.975 1.473-.122 2.904-.244 6.259-.854 3.354-.61 3.845-1.341 5.645-2.195 1.282-.608 3.46-2.401 4.702-3.494.275-.243.518-.517.745-.806.471-.6 1.431-1.78 3.102-3.748 2.864-3.373 5.932-2.235 8.918-1.788 2.986.447 4.868 1.829 5.972 2.317 1.105.488 1.227.203.49-.488-.735-.691-2.372-1.504-5.276-2.52Z"
    />
</svg>
</file>

<file path="internal/embed/init/test-app/health.html">
OK
</file>

<file path="internal/embed/templates/apps-with-test-app.yml">
apps:
  - name: "test-app"
    source:
      dockerfile:
        path: "{{ .ConfigDirPath }}/test-app/Dockerfile"
        buildContext: "{{ .ConfigDirPath }}/test-app"
    domains:
      - domain: "{{ .Domain }}"
        aliases:
          - "{{ .Alias }}"
    acmeEmail: "{{ .AcmeEmail }}"
    maxContainersToKeep: 3
    healthCheckPath: "/health.html"
</file>

<file path="internal/helpers/networkutils.go">
package helpers

import (
	"fmt"
	"io"
	"net"
	"net/http"
	"strings"
)

// GetARecord returns the first A record (IPv4 address) for the provided host.
// It returns an error if no A record is found.
func GetARecord(host string) (net.IP, error) {
	ips, err := net.LookupIP(host)
	if err != nil {
		return nil, err
	}
	for _, ip := range ips {
		if ip4 := ip.To4(); ip4 != nil {
			return ip4, nil
		}
	}
	return nil, fmt.Errorf("no A record found for host: %s", host)
}

// GetExternalIP queries a public service for this machine's external IPv4.
// It returns the IP or an error.
func GetExternalIP() (net.IP, error) {
	resp, err := http.Get("https://api.ipify.org?format=text")
	if err != nil {
		return nil, fmt.Errorf("failed to query external IP service: %w", err)
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read external IP response: %w", err)
	}

	ipStr := strings.TrimSpace(string(body))
	ip := net.ParseIP(ipStr).To4()
	if ip == nil {
		return nil, fmt.Errorf("invalid IPv4 address returned: %s", ipStr)
	}
	return ip, nil
}
</file>

<file path="internal/helpers/sanitize_test.go">
package helpers

import "testing"

func TestSanitizeString(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  string
	}{
		{"empty string", "", ""},
		{"alphanumeric", "abc123XYZ", "abc123XYZ"},
		{"with hyphens", "my-app-name", "my-app-name"},
		{"with underscores", "my_app_name", "my_app_name"},
		{"with dots", "my.app.name", "my_app_name"},
		{"with spaces", "my app name", "my_app_name"},
		{"mixed disallowed", "my!app@name#$", "my_app_name_"},
		{"leading/trailing disallowed", ".test-app.", "_test-app_"},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if got := SanitizeString(tt.input); got != tt.want {
				t.Errorf("SanitizeString() = %v, want %v", got, tt.want)
			}
		})
	}
}

func TestSafeIDPrefix(t *testing.T) {
	tests := []struct {
		name string
		id   string
		want string
	}{
		{"long id", "abcdef1234567890", "abcdef123456"},
		{"exact length id", "abcdef123456", "abcdef123456"},
		{"short id", "abcde", "abcde"},
		{"empty id", "", ""},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if got := SafeIDPrefix(tt.id); got != tt.want {
				t.Errorf("SafeIDPrefix() = %v, want %v", got, tt.want)
			}
		})
	}
}

func TestSanitizeFilename(t *testing.T) {
	tests := []struct {
		name string
		in   string
		want string
	}{
		{"valid filename", "my-file.txt", "my-file.txt"},
		{"with spaces", "my file.txt", "my_file.txt"},
		{"with slashes", "my/file.txt", "my_file.txt"},
		{"with colons", "my:file.txt", "my_file.txt"},
		{"with question mark", "my?file.txt", "my_file.txt"},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if got := SanitizeFilename(tt.in); got != tt.want {
				t.Errorf("SanitizeFilename() = %v, want %v", got, tt.want)
			}
		})
	}
}
</file>

<file path="internal/helpers/validators.go">
package helpers

import "regexp"

func IsValidEmail(email string) bool {
	emailRegex := regexp.MustCompile(`^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$`)
	return emailRegex.MatchString(email)
}
</file>

<file path="scripts/del-tags.sh">
git fetch --tags && for tag in $(git tag); do
  git push --delete origin "$tag"
done
</file>

<file path="scripts/local-build-cli.sh">
#!/bin/bash
 BINARY_NAME=haloy
 go build -o $BINARY_NAME ../cmd/cli
</file>

<file path="scripts/remove-haloy.sh">
#!/bin/bash
set -e

# Ensure an argument is provided
if [ -z "$1" ]; then
    echo "Usage: $0 <hostname>"
    exit 1
fi

HOSTNAME=$1

# Use the current username from the shell
USERNAME=$(whoami)

# Remote command to delete the haloy binary and config directory
echo "Cleaning up haloy components on ${USERNAME}@${HOSTNAME}"
ssh ${USERNAME}@"$HOSTNAME" << 'EOF'
  echo "Attempting to remove haloy binary..."
  if rm -f $HOME/haloy; then
    echo "Binary removed successfully."
  else
    echo "Failed to remove binary or binary does not exist."
  fi

  echo "Attempting to remove haloy config directory..."
  if rm -rf $HOME/.config/haloy; then
    echo "Config directory removed successfully."
  else
    echo "Failed to remove config directory or directory does not exist."
  fi
EOF
</file>

<file path=".gitignore">
# Binaries for programs and plugins
*.exe
*.exe~
*.dll
*.so
*.dylib

# Test binary, built with `go test -c`
*.test

# Output of the go coverage tool, specifically when used with LiteIDE
*.out

# Dependency directories (remove the comment below to include it)
# vendor/

# Go workspace file
go.work
go.work.sum

# env file
.env

.DS_Store

tmp/
</file>

<file path="LICENSE">
Copyright (c) 2025 Andreas Meistad

Permission is hereby granted, free of charge, to any person obtaining
a copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be
included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
</file>

<file path="cmd/manager/main.go">
package main

import (
	"flag"
	"os"

	"github.com/ameistad/haloy/internal/manager"
)

func main() {
	// Parse command line flags
	dryRunFlag := flag.Bool("dry-run", false, "Run in dry-run mode (don't actually send commands to HAProxy)")
	flag.Parse()

	dryRunEnv := os.Getenv("DRY_RUN") == "true"
	dryRun := *dryRunFlag || dryRunEnv

	manager.RunManager(dryRun)
}
</file>

<file path="dev/manager/Dockerfile.dev">
# Build the development image
# docker build -t haloy-manager-dev -f ./dev/manager/Dockerfile.dev .

FROM golang:1.24-alpine

# Install development tools and runtime dependencies
RUN apk add --no-cache \
    git \
    curl \
    bash \
    && go install github.com/air-verse/air@latest

# Set environment variables
ENV HOME=/root
ENV DRY_RUN=true

# Debug: Print environment variable to verify it's set
RUN echo "DRY_RUN environment variable is: $DRY_RUN"

# Expose any ports that might be needed
EXPOSE 9999

# Create air config file without trying to pass command line arguments
RUN echo '[build]' > /root/.air.toml && \
    echo 'cmd = "go build -o ./tmp/app ./cmd/manager"' >> /root/.air.toml && \
    echo 'bin = "./tmp/app"' >> /root/.air.toml && \
    echo 'include_ext = ["go", "yaml"]' >> /root/.air.toml && \
    echo 'exclude_dir = ["tmp", "vendor"]' >> /root/.air.toml

# Working directory will be mounted from host
WORKDIR /src

LABEL haloy.role=manager

# Use air for hot reload during development
CMD ["air", "-c", "/root/.air.toml"]
</file>

<file path="dev/manager/run.sh">
#!/bin/bash
DOCKER_NETWORK=haloy-public

cd $(git rev-parse --show-toplevel)

CERT_STORAGE=$(mktemp -d /tmp/haloy-cert-storage-XXXXXX)
LOGS=$(mktemp -d /tmp/haloy-logs-XXXXXX)

cleanup() {
  rm -rf "$CERT_STORAGE" "$LOGS"
}
trap cleanup EXIT

if ! docker network inspect "$DOCKER_NETWORK" >/dev/null 2>&1; then
  echo "Creating Docker network: $DOCKER_NETWORK"
  docker network create "$DOCKER_NETWORK"
else
  echo "Docker network $DOCKER_NETWORK already exists"
fi

docker run -it --rm \
  --name haloy-manager-dev \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v "$CERT_STORAGE":/cert-storage:rw \
  -v "$LOGS":/logs:rw \
  -v $(pwd):/src \
  --network haloy-public \
  -e DRY_RUN=true \
  haloy-manager-dev
</file>

<file path="internal/cli/commands/start.go">
package commands

import (
	"context"
	"time"

	"github.com/ameistad/haloy/internal/docker"
	"github.com/ameistad/haloy/internal/ui"
	"github.com/spf13/cobra"
)

const (
	startTimeout = 5 * time.Minute
)

func StartCmd() *cobra.Command {

	cmd := &cobra.Command{
		Use:   "start",
		Short: "Start the haloy services",
		Long:  "Start the haloy services, including HAProxy and haloy-manager.",
		Args:  cobra.NoArgs,
		Run: func(cmd *cobra.Command, args []string) {
			ctx, cancel := context.WithTimeout(context.Background(), startTimeout)
			defer cancel()
			dockerClient, err := docker.NewClient(ctx)
			if err != nil {
				ui.Error("%v", err)
				return
			}
			defer dockerClient.Close()

			// Start the haloy services
			status, err := docker.EnsureServicesIsRunning(dockerClient, ctx)
			if err != nil {
				ui.Error("Failed to start services: %v", err)
				return
			}

			if status.State == docker.ServiceStateRunning {
				ui.Success("Haloy services are already running\n")
			} else {
				ui.Success("Haloy services started successfully\n")
			}
		},
	}
	return cmd
}
</file>

<file path="internal/cli/commands/validate.go">
package commands

import (
	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/ui"
	"github.com/spf13/cobra"
)

// NewValidateCmd creates a new validate command
func ValidateCmd() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "validate",
		Short: "Validate the config file",
		Run: func(cmd *cobra.Command, args []string) {
			confFilePath, err := config.ConfigFilePath()
			if err != nil {
				ui.Error("couldn't determine config file path: %v", err)
				return
			}

			_, err = config.LoadAndValidateConfig(confFilePath)
			if err != nil {
				ui.Error("Config file found at '%s' is not valid: %v", confFilePath, err)
				return
			}

			ui.Success("Config file '%s' is valid!\n", confFilePath)
		},
	}

	return cmd
}
</file>

<file path="internal/config/source.go">
package config

import (
	"fmt"
	"os"
	"reflect"
	"strings"

	"gopkg.in/yaml.v3"
)

type Source struct {
	// Use pointers to ensure only one is provided.
	Dockerfile *DockerfileSource `yaml:"dockerfile,omitempty"`
	Image      *ImageSource      `yaml:"image,omitempty"`
}

type DockerfileSource struct {
	Path         string            `yaml:"path"`
	BuildContext string            `yaml:"buildContext"`
	BuildArgs    map[string]string `yaml:"buildArgs,omitempty"`
}

// TODO: implement this
type ImageSource struct {
	Repository string `yaml:"repository"`
	Tag        string `yaml:"tag, omitempty"`
}

func (s *Source) UnmarshalYAML(value *yaml.Node) error {
	// Get expected field names
	expectedFields := ExtractYAMLFieldNames(reflect.TypeOf(*s))

	// Check for unknown fields
	if err := CheckUnknownFields(value, expectedFields, "source: "); err != nil {
		return err
	}

	// Use type alias to avoid infinite recursion
	type SourceAlias Source
	var alias SourceAlias

	// Unmarshal to the alias type
	if err := value.Decode(&alias); err != nil {
		return err
	}

	// Copy data back to original struct
	*s = Source(alias)

	return nil
}

func (s *Source) Validate() error {
	sourceIsDefined := false

	// Check Dockerfile Source
	if s.Dockerfile != nil {
		sourceIsDefined = true
		dfSource := s.Dockerfile
		if dfSource.Path == "" {
			return fmt.Errorf("source.dockerfile.path is required")
		}
		if dfSource.BuildContext == "" {
			return fmt.Errorf("source.dockerfile.buildContext is required")
		}

		// Check Dockerfile Path existence and type (should be a file)
		// Consider making paths absolute before checking, or resolving relative to config file?
		// For now, assuming paths are relative to where the app runs or absolute.
		fileInfo, err := os.Stat(dfSource.Path)
		if os.IsNotExist(err) {
			return fmt.Errorf("source.dockerfile.path '%s' does not exist", dfSource.Path)
		} else if err != nil {
			return fmt.Errorf("unable to check source.dockerfile.path '%s': %w", dfSource.Path, err)
		}
		if fileInfo.IsDir() {
			return fmt.Errorf("source.dockerfile.path '%s' is a directory, not a file", dfSource.Path)
		}

		// Check BuildContext existence and type (should be a directory)
		ctxInfo, err := os.Stat(dfSource.BuildContext)
		if os.IsNotExist(err) {
			return fmt.Errorf("source.dockerfile.buildContext '%s' does not exist", dfSource.BuildContext)
		} else if err != nil {
			return fmt.Errorf("unable to check source.dockerfile.buildContext '%s': %w", dfSource.BuildContext, err)
		}
		if !ctxInfo.IsDir() {
			return fmt.Errorf("source.dockerfile.buildContext '%s' is not a directory", dfSource.BuildContext)
		}
	}

	// Check Image Source
	if s.Image != nil {
		// Check if Dockerfile source was *also* defined (mutual exclusivity)
		if sourceIsDefined {
			return fmt.Errorf("cannot define both source.dockerfile and source.image")
		}
		sourceIsDefined = true
		imgSource := s.Image
		// Validate Image source fields
		if imgSource.Repository == "" {
			return fmt.Errorf("source.image.repository is required")
		}
		// Optional: Add regex validation for imgSource.Repository and imgSource.Tag if needed.
		// Example simple check: prevent whitespace
		if strings.ContainsAny(imgSource.Repository, " \t\n\r") {
			return fmt.Errorf("source.image.repository '%s' contains whitespace", imgSource.Repository)
		}
		if strings.ContainsAny(imgSource.Tag, " \t\n\r") {
			return fmt.Errorf("source.image.tag '%s' contains whitespace", imgSource.Tag)
		}
	}

	// Check if *at least one* source type was defined
	if !sourceIsDefined {
		return fmt.Errorf("source must contain either 'dockerfile' or 'image'")
	}
	return nil
}
</file>

<file path="internal/deploy/types.go">
package deploy

// ContainerInfo represents essential information about a deployed container.
// It is used to track deployments and for operations like rolling back.
type ContainerInfo struct {
	ID           string
	DeploymentID string
}
</file>

<file path="internal/docker/services.go">
package docker

import (
	"context"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"strings"

	"github.com/ameistad/haloy/internal/config"
	"github.com/docker/docker/api/types/container"
	"github.com/docker/docker/client"
)

// ServiceState represents the current state of the haloy services
type ServiceState string

const (
	// ServiceStateRunning indicates services were already running and healthy
	ServiceStateRunning ServiceState = "running"
	// ServiceStateStarted indicates services needed to be started
	ServiceStateStarted ServiceState = "started"
	// ServiceStatePartial indicates some services are running but not all
	ServiceStatePartial ServiceState = "partial"
)

// ServiceStatus contains detailed information about the services
type ServiceStatus struct {
	State           ServiceState
	RunningServices map[string]bool
	Details         string
}

func EnsureServicesIsRunning(dockerClient *client.Client, ctx context.Context) (ServiceStatus, error) {
	requiredRoles := []string{config.HAProxyLabelRole, config.ManagerLabelRole}
	status := ServiceStatus{
		RunningServices: make(map[string]bool),
	}

	// Check if containers are running
	containers, err := dockerClient.ContainerList(ctx, container.ListOptions{All: true})
	if err != nil {
		return status, fmt.Errorf("failed to list containers: %w", err)
	}

	// Track which services are running and healthy
	for _, container := range containers {
		// Check if this container has a haloy role label
		if roleValue, hasRole := container.Labels[config.LabelRole]; hasRole {
			// Check if the container's role is one we're looking for
			for _, requiredRole := range requiredRoles {
				if roleValue == requiredRole {
					// Check if the container is running
					isRunning := container.State == "running"

					// Check if the container is healthy (if it has health check)
					isHealthy := true
					if container.Status != "" && strings.Contains(container.Status, "unhealthy") {
						isHealthy = false
					}

					status.RunningServices[roleValue] = isRunning && isHealthy
				}
			}
		}
	}

	// If all required roles are running and healthy, return early
	allRunning := true
	runningCount := 0
	for _, role := range requiredRoles {
		if status.RunningServices[role] {
			runningCount++
		} else {
			allRunning = false
		}
	}

	if allRunning {
		status.State = ServiceStateRunning
		status.Details = "All services are already running and healthy"
		return status, nil
	}

	if runningCount > 0 {
		status.State = ServiceStatePartial
		status.Details = fmt.Sprintf("%d of %d services are running", runningCount, len(requiredRoles))
	}

	// Rest of the function remains the same
	// Get docker-compose file path
	dockerComposeFilePath, err := config.ServicesDockerComposeFilePath()
	if err != nil {
		return status, fmt.Errorf("failed to get docker-compose file path: %w", err)
	}

	// Check if the docker-compose file exists
	if _, err := os.Stat(dockerComposeFilePath); os.IsNotExist(err) {
		return status, fmt.Errorf("docker-compose file not found at %s: %w", dockerComposeFilePath, err)
	}

	// Get the directory of the docker-compose file
	composeDir := filepath.Dir(dockerComposeFilePath)

	// Start the containers according to docker-compose
	cmd := exec.CommandContext(ctx, "docker", "compose", "-f", dockerComposeFilePath, "up", "-d")
	cmd.Dir = composeDir

	output, err := cmd.CombinedOutput()
	if err != nil {
		status.Details = string(output)
		return status, fmt.Errorf("failed to start services: %w", err)
	}

	status.State = ServiceStateStarted
	status.Details = "Services started successfully"
	return status, nil
}
</file>

<file path="internal/embed/init/containers/entrypoint.sh">
#!/bin/sh

CERT_DIR="/usr/local/etc/haproxy/certs"
DUMMY_CERT="${CERT_DIR}/dummy.pem"

if [ ! -f "$DUMMY_CERT" ]; then
    echo "No certificate found. Generating a self-signed dummy certificate..."
    mkdir -p "$CERT_DIR"
    openssl req -x509 -nodes -days 1 -newkey rsa:2048 \
      -keyout "$DUMMY_CERT" -out "$DUMMY_CERT" \
      -subj "/CN=localhost"
    echo "Dummy certificate generated."
fi

echo "Starting HAProxy..."
exec haproxy -f /usr/local/etc/haproxy/config/haproxy.cfg "$@"
</file>

<file path="internal/embed/init/test-app/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Haloy: It Works!</title>
    <link rel="icon" href="haloy-curlew.svg" type="image/svg+xml">
    <style>
        body {
            font-family: sans-serif;
            line-height: 1.6;
            color: #474747;
            padding: 2em;
            background-color: #f4fcff;
        }
        .container {
            max-width: 600px;
            margin: 2em auto;
            padding: 2em;
            text-align: center;
        }
        .logo {
            max-width: 100px;
            height: auto;
            margin-bottom: 1em;
        }
        h1 {
            color: #333;
            margin-top: 0;
        }
        ul {
            list-style: none;
            padding: 0;
            margin-top: 1.5em;
            text-align: left;
            display: inline-block; /* Center the list block */
        }
        li {
            margin-bottom: 0.8em;
            position: relative;
            padding-left: 1.5em;
        }
        li::before {
            content: '✅'; /* Simple checkmark */
            position: absolute;
            left: 0;
            color: #28a745;
        }
        .footer {
            margin-top: 2em;
            font-size: 0.9em;
            color: #777;
        }
        .footer a {
            color: #007bff;
            text-decoration: none;
        }
        .footer a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <img src="haloy-curlew.svg" alt="Haloy Logo" class="logo">

        <h1>It works!</h1>

        <p>If you're reading this, it most likely means everything is working.</p>

        <div class="footer">
            <p>Thanks for using <a href="https://github.com/ameistad/haloy">Haloy</a></p>
        </div>
    </div>
</body>
</html>
</file>

<file path="internal/embed/embed.go">
package embed

import "embed"

const (
	HAProxyConfigFileTemplate = "haproxy.cfg"
	ConfigFileTemplate        = "apps.yml"
	ConfigFileTemplateTest    = "apps-with-test-app.yml"
)

//go:embed init/*
var InitFS embed.FS

//go:embed templates/*
var TemplatesFS embed.FS
</file>

<file path="internal/embed/types.go">
package embed

// HAProxyTemplateData holds all values your HAProxy template expects.
type HAProxyTemplateData struct {
	HTTPFrontend            string
	HTTPSFrontend           string
	HTTPSFrontendUseBackend string
	Backends                string
}

type ConfigFileWithTestAppTemplateData struct {
	ConfigDirPath string
	Domain        string
	Alias         string
	AcmeEmail     string
}
</file>

<file path="internal/helpers/debouncer.go">
package helpers

import (
	"sync"
	"time"
)

// DebounceFunc defines the type for the function to be executed after debouncing.
type DebounceFunc func()

// Debouncer manages debouncing calls for different keys.
type Debouncer struct {
	mu     sync.Mutex
	timers map[string]*time.Timer // key: identifier for the debounced action
	delay  time.Duration
}

// NewDebouncer creates a new Debouncer.
func NewDebouncer(delay time.Duration) *Debouncer {
	return &Debouncer{
		timers: make(map[string]*time.Timer),
		delay:  delay,
	}
}

// Debounce schedules or resets the timer for a given key.
// When the delay expires without subsequent calls for the same key, the action function is executed.
func (d *Debouncer) Debounce(key string, action DebounceFunc) {
	d.mu.Lock()
	defer d.mu.Unlock()

	// If a timer already exists for this key, stop and reset it.
	if timer, ok := d.timers[key]; ok {
		timer.Stop()
	}

	// Create a new timer.
	d.timers[key] = time.AfterFunc(d.delay, func() {
		// This function runs after the delay has passed without new calls for this key.

		// Remove the timer entry *before* executing the action.
		d.mu.Lock()
		delete(d.timers, key)
		d.mu.Unlock()

		// Execute the provided action function.
		action()
	})
}

// Stop cancels all pending debounced actions.
func (d *Debouncer) Stop() {
	d.mu.Lock()
	defer d.mu.Unlock()
	for key, timer := range d.timers {
		timer.Stop()
		delete(d.timers, key)
	}
}
</file>

<file path="internal/cli/commands/deploy.go">
package commands

import (
	"fmt"

	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/deploy"
	"github.com/ameistad/haloy/internal/ui"
	"github.com/spf13/cobra"
)

func DeployAppCmd() *cobra.Command {
	deployAppCmd := &cobra.Command{
		Use:   "deploy <app-name>",
		Short: "Deploy an application",
		Long:  `Deploy a single application by name`,
		Args: func(cmd *cobra.Command, args []string) error {
			if len(args) != 1 {
				return fmt.Errorf("app deploy requires exactly one argument: the app name (e.g., 'haloy app deploy my-app')")
			}
			return nil
		},
		Run: func(cmd *cobra.Command, args []string) {
			appName := args[0]
			appConfig, err := config.AppConfigByName(appName)
			if err != nil {
				ui.Error("Failed to get configuration for %q: %v\n", appName, err)
				return
			}

			if err := deploy.DeployApp(appConfig); err != nil {
				ui.Error("Failed to deploy %q: %v\n", appName, err)
			}
		},
	}
	return deployAppCmd
}

func DeployAllCmd() *cobra.Command {
	deployAllCmd := &cobra.Command{
		Use:   "deploy-all",
		Short: "Deploy all applications",
		Long:  `Deploy all applications defined in the configuration file.`,
		Args:  cobra.NoArgs,
		Run: func(cmd *cobra.Command, args []string) {
			configFilePath, err := config.ConfigFilePath()
			if err != nil {
				ui.Error("Failed to determine config file path: %v\n", err)
				return
			}
			configFile, err := config.LoadAndValidateConfig(configFilePath)
			if err != nil {
				ui.Error("Failed to load configuration file: %v\n", err)
				return
			}

			for i := range configFile.Apps {
				app := configFile.Apps[i]
				appConfig := &app
				if err := deploy.DeployApp(appConfig); err != nil {
					ui.Error("Failed to deploy %q: %v\n", app.Name, err)
				}
			}
		},
	}
	return deployAllCmd
}
</file>

<file path="internal/cli/commands/secrets.go">
package commands

import (
	"bytes"
	"crypto/md5"
	"encoding/base64"
	"encoding/hex"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"strings"
	"time"

	"filippo.io/age"
	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/ui"
	"github.com/spf13/cobra"
)

// SecretsInitCommand creates a command to initialize the secrets system by generating the age identity.
func SecretsInitCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "init",
		Short: "Initialize secrets by generating an age identity",
		RunE: func(cmd *cobra.Command, args []string) error {
			identity, err := age.GenerateX25519Identity()
			if err != nil {
				return fmt.Errorf("failed to generate age identity: %w", err)
			}
			configDir, err := config.ConfigDirPath()
			if err != nil {
				return fmt.Errorf("failed to get config directory: %w", err)
			}
			identityPath := filepath.Join(configDir, config.IdentityFileName)
			// Create the identity file with restricted permissions (0600 - read/write for owner only)
			f, err := os.OpenFile(identityPath, os.O_RDWR|os.O_CREATE|os.O_TRUNC, 0600)
			if err != nil {
				return fmt.Errorf("failed to create identity file: %w", err)
			}
			defer f.Close()
			if _, err := f.WriteString(identity.String()); err != nil {
				return fmt.Errorf("failed to write identity to file: %w", err)
			}
			publicKey := identity.Recipient().String()
			fmt.Printf("Age identity generated and saved to %s\n", identityPath)
			fmt.Printf("Public key: %s\n", publicKey)
			return nil
		},
	}
	return cmd
}

// SecretsSetCommand encrypts a plain-text value and stores it under the provided key.
func SecretsSetCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "set <key> <value>",
		Short: "Encrypt a plain-text value and store it under <key>",
		Args:  cobra.MinimumNArgs(2),
		RunE: func(cmd *cobra.Command, args []string) error {
			key := args[0]
			value := strings.Join(args[1:], " ")

			recipient, err := config.GetAgeRecipient()
			if err != nil {
				return err
			}

			// Encrypt the value.
			var rawBuffer bytes.Buffer
			encryptWriter, err := age.Encrypt(&rawBuffer, recipient)
			if err != nil {
				return fmt.Errorf("failed to initialize encryption: %w", err)
			}
			if _, err = io.WriteString(encryptWriter, value); err != nil {
				return fmt.Errorf("failed to write plain-text to encryption writer: %w", err)
			}
			if err := encryptWriter.Close(); err != nil {
				return fmt.Errorf("failed to close encryption writer: %w", err)
			}
			fullEncrypted := base64.StdEncoding.EncodeToString(rawBuffer.Bytes())

			newSecretRecord := config.SecretRecord{
				Encrypted: fullEncrypted,
				Date:      time.Now().Format(time.RFC3339),
			}

			secretRecords, err := config.LoadSecrets()
			if err != nil {
				return err
			}
			secretRecords[key] = newSecretRecord
			if err := config.SaveSecrets(secretRecords); err != nil {
				return err
			}
			fmt.Printf("Secret stored with key: %s\n", key)
			return nil
		},
	}
	return cmd
}

// SecretsListCommand lists all stored secrets in a table.
func SecretsListCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "list",
		Short: "List all stored secrets",
		RunE: func(cmd *cobra.Command, args []string) error {
			secrets, err := config.LoadSecrets()
			if err != nil {
				return err
			}
			if len(secrets) == 0 {
				fmt.Println("No secrets stored.")
				return nil
			}

			headers := []string{"NAME", "DIGEST", "DATE"}
			rows := make([][]string, 0, len(secrets))
			for key, rec := range secrets {
				// Compute the digest from the encrypted value using MD5.
				digest := md5.Sum([]byte(rec.Encrypted))
				digestStr := hex.EncodeToString(digest[:])
				rows = append(rows, []string{key, digestStr, rec.Date})
			}

			ui.Table(headers, rows)
			return nil
		},
	}
	return cmd
}

func SecretsDeleteCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "delete <key>",
		Short: "Delete a stored secret by key",
		Args:  cobra.ExactArgs(1),
		RunE: func(cmd *cobra.Command, args []string) error {
			key := args[0]

			secretRecords, err := config.LoadSecrets()
			if err != nil {
				return err
			}
			if _, exists := secretRecords[key]; !exists {
				fmt.Printf("No secret found with key: %s\n", key)
				return nil
			}
			delete(secretRecords, key)
			if err := config.SaveSecrets(secretRecords); err != nil {
				return err
			}
			fmt.Printf("Secret deleted with key: %s\n", key)
			return nil
		},
	}
	return cmd
}

// SecretsCommand creates the parent secrets command with its subcommands.
func SecretsCommand() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "secrets",
		Short: "Manage secrets using age encryption",
	}
	cmd.AddCommand(SecretsInitCommand())
	cmd.AddCommand(SecretsSetCommand())
	cmd.AddCommand(SecretsListCommand())
	cmd.AddCommand(SecretsDeleteCommand())
	return cmd
}
</file>

<file path="internal/config/env.go">
package config

import (
	"errors"
	"fmt"
	"reflect"

	"gopkg.in/yaml.v3"
)

// EnvVar represents an environment variable that can either have a plaintext value or be backed by a secret.
type EnvVar struct {
	Name string `yaml:"name"`

	// Use pointers to ensure only one is provided.
	Value      *string `yaml:"value,omitempty"`
	SecretName *string `yaml:"secretName,omitempty"`
	// Internal field to hold the decrypted value after processing.
	decryptedValue *string `yaml:"-"`
}

func (e *EnvVar) UnmarshalYAML(value *yaml.Node) error {
	// Get expected field names
	expectedFields := ExtractYAMLFieldNames(reflect.TypeOf(*e))

	// Check for unknown fields
	if err := CheckUnknownFields(value, expectedFields, "env var: "); err != nil {
		return err
	}

	// Use type alias to avoid infinite recursion
	type EnvVarAlias EnvVar
	var alias EnvVarAlias

	// Unmarshal to the alias type
	if err := value.Decode(&alias); err != nil {
		return err
	}

	// Copy data back to original struct
	*e = EnvVar(alias)

	return nil
}

// DecryptEnvVars iterates over the provided environment variables and, when a SecretName is set,
// looks up the corresponding encrypted secret, decrypts it using the age identity, and updates the variable.
func DecryptEnvVars(initialEnvVars []EnvVar) ([]EnvVar, error) {
	// If not secrets are provided, return the original env vars without initializing secrets.
	// We do this because the age ideetity might not be available in the current context and we can't load them.<
	hasSecrets := false
	for _, ev := range initialEnvVars {
		if ev.SecretName != nil {
			hasSecrets = true
			break
		}
	}
	if !hasSecrets {
		return initialEnvVars, nil
	}

	secrets, err := LoadSecrets()
	if err != nil {
		return nil, fmt.Errorf("failed to load secrets: %w", err)
	}

	// Load the full age identity (private key) — needed for decryption.
	identity, err := GetAgeIdentity()
	if err != nil {
		return nil, fmt.Errorf("failed to get age identity: %w", err)
	}

	envVars := make([]EnvVar, len(initialEnvVars))
	copy(envVars, initialEnvVars)
	for i, ev := range envVars {
		if ev.SecretName != nil {
			record, exists := secrets[*ev.SecretName]
			if !exists {
				continue
			}
			// DecryptSecret will use the full identity to decrypt the stored encrypted value.
			decrypted, err := DecryptSecret(record.Encrypted, identity)
			if err != nil {
				return nil, fmt.Errorf("failed to decrypt value for '%s': %w", ev.Name, err)
			}
			// Write back to the underlying slice element.
			envVars[i].decryptedValue = &decrypted
		}
	}
	return envVars, nil
}

// GetValue returns the final value of the environment variable. It returns the decrypted value if available;
// otherwise it returns the plaintext value. If neither is set, it returns an error.
func (ev *EnvVar) GetValue() (string, error) {
	if ev.decryptedValue != nil {
		return *ev.decryptedValue, nil
	}
	if ev.Value != nil {
		return *ev.Value, nil
	}
	// Failsafe: Should not happen if validation runs first.
	return "", fmt.Errorf("environment variable '%s' has neither a plaintext nor a decrypted value", ev.Name)
}

// Validate ensures the EnvVar is correctly configured.
func (ev *EnvVar) Validate() error {
	if ev.Name == "" {
		return errors.New("environment variable name cannot be empty")
	}
	if ev.Value != nil && ev.SecretName != nil {
		return fmt.Errorf("environment variable '%s': cannot provide both 'value' and 'secretName'", ev.Name)
	}
	if ev.Value == nil && ev.SecretName == nil {
		// Assuming that one must be provided...
		return fmt.Errorf("environment variable '%s': must provide either 'value' or 'secretName'", ev.Name)
	}

	if ev.SecretName != nil {
		secrets, err := LoadSecrets()
		if err != nil {
			return fmt.Errorf("failed to load secrets: %w", err)
		}
		if _, exists := secrets[*ev.SecretName]; !exists {
			return fmt.Errorf("secret '%s' not found in secrets store", *ev.SecretName)
		}
	}
	return nil
}
</file>

<file path="internal/config/secrets.go">
package config

import (
	"bytes"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"strings"

	"filippo.io/age"
)

const (
	IdentityFileName = "age_identity.txt"
	SecretsFileName  = "secrets.json"
)

type SecretRecord struct {
	Encrypted string `json:"encrypted"`
	Date      string `json:"date"`
}

// GetAgeRecipient reads the age identity file and returns the corresponding recipient.
func GetAgeRecipient() (age.Recipient, error) {
	configDir, err := ConfigDirPath()
	if err != nil {
		return nil, fmt.Errorf("failed to get config directory: %w", err)
	}
	identityPath := filepath.Join(configDir, IdentityFileName)
	data, err := os.ReadFile(identityPath)
	if err != nil {
		if os.IsNotExist(err) {
			return nil, fmt.Errorf("age identity file not found at %s - please run 'secrets init' first", identityPath)
		}
		return nil, fmt.Errorf("failed to read age identity file: %w", err)
	}
	identityStr := strings.TrimSpace(string(data))
	identity, err := age.ParseX25519Identity(identityStr)
	if err != nil {
		return nil, fmt.Errorf("failed to parse age identity from file: %w", err)
	}
	return identity.Recipient(), nil
}

func GetAgeIdentity() (age.Identity, error) {
	configDir, err := ConfigDirPath()
	if err != nil {
		return nil, fmt.Errorf("failed to get config directory: %w", err)
	}
	identityPath := filepath.Join(configDir, IdentityFileName)
	data, err := os.ReadFile(identityPath)
	if err != nil {
		if os.IsNotExist(err) {
			return nil, fmt.Errorf("age identity file not found at %s - please run 'secrets init' first", identityPath)
		}
		return nil, fmt.Errorf("failed to read age identity file: %w", err)
	}
	identityStr := strings.TrimSpace(string(data))
	identity, err := age.ParseX25519Identity(identityStr)
	if err != nil {
		return nil, fmt.Errorf("failed to parse age identity from file: %w", err)
	}
	return identity, nil
}

// LoadSecrets loads the secrets map from secrets.json (or returns an empty map if not found).
func LoadSecrets() (map[string]SecretRecord, error) {
	configDir, err := ConfigDirPath()
	if err != nil {
		return nil, err
	}
	secretsPath := filepath.Join(configDir, SecretsFileName)
	secrets := make(map[string]SecretRecord)
	data, err := os.ReadFile(secretsPath)
	if err != nil {
		if os.IsNotExist(err) {
			return secrets, nil
		}
		return nil, fmt.Errorf("failed to read secrets file: %w", err)
	}
	if err := json.Unmarshal(data, &secrets); err != nil {
		return nil, fmt.Errorf("failed to parse secrets file: %w", err)
	}
	return secrets, nil
}

// SaveSecrets writes the secrets map to secrets.json.
func SaveSecrets(secrets map[string]SecretRecord) error {
	configDir, err := ConfigDirPath()
	if err != nil {
		return err
	}
	secretsPath := filepath.Join(configDir, SecretsFileName)
	data, err := json.MarshalIndent(secrets, "", "  ")
	if err != nil {
		return fmt.Errorf("failed to encode secrets as JSON: %w", err)
	}
	// Write with restricted permissions (0600 - read/write for owner only)
	if err := os.WriteFile(secretsPath, data, 0600); err != nil {
		return fmt.Errorf("failed to write secrets file: %w", err)
	}
	return nil
}

// DecryptSecret decrypts a base64-encoded secret using the provided age identity.
// It returns the decrypted secret as a string.
func DecryptSecret(secret string, identity age.Identity) (string, error) {
	// Decode the stored encrypted value from its base64 representation.
	encryptedBytes, err := base64.StdEncoding.DecodeString(secret)
	if err != nil {
		return "", fmt.Errorf("failed to decode base64 secret: %w", err)
	}

	decryptReader, err := age.Decrypt(bytes.NewReader(encryptedBytes), identity)
	if err != nil {
		return "", fmt.Errorf("failed to decrypt value: %w", err)
	}

	var decryptedBuf bytes.Buffer
	if _, err := io.Copy(&decryptedBuf, decryptReader); err != nil {
		return "", fmt.Errorf("failed to read decrypted value: %w", err)
	}

	return decryptedBuf.String(), nil
}
</file>

<file path="internal/deploy/constants.go">
package deploy

import "time"

const (
	// DefaultDeployTimeout is the maximum time allowed for a deployment to complete
	// before it's considered failed. This includes time for building images, creating
	// containers, and waiting for health checks to pass.
	DefaultDeployTimeout = 120 * time.Second
)
</file>

<file path="internal/docker/network.go">
package docker

import (
	"context"
	"fmt"

	"github.com/ameistad/haloy/internal/config"
	"github.com/docker/docker/api/types/container"
	"github.com/docker/docker/api/types/network"
	"github.com/docker/docker/client"
)

func CreateNetwork(dockerClient *client.Client, ctx context.Context) error {
	options := network.CreateOptions{
		Driver:     "bridge",
		Attachable: true,
		Labels: map[string]string{
			"created-by": "haloy",
		},
	}
	_, err := dockerClient.NetworkCreate(ctx, config.DockerNetwork, options)
	if err != nil {
		return fmt.Errorf("failed to create Docker network: %w", err)
	}
	return nil
}

func EnsureNetwork(dockerClient *client.Client, ctx context.Context) error {
	networks, err := dockerClient.NetworkList(ctx, network.ListOptions{})
	if err != nil {
		return fmt.Errorf("failed to list Docker networks: %w", err)
	}

	defaultNetworkExists := false
	for _, network := range networks {
		if network.Name == config.DockerNetwork {
			defaultNetworkExists = true
			break
		}
	}

	if !defaultNetworkExists {
		if err := CreateNetwork(dockerClient, ctx); err != nil {
			return fmt.Errorf("failed to create Docker network: %w", err)
		}
	}
	return nil
}

// ContainerNetworkInfo extracts the container's IP address
func ContainerNetworkIP(container container.InspectResponse, networkName string) (string, error) {
	if _, exists := container.NetworkSettings.Networks[networkName]; !exists {
		return "", fmt.Errorf("specified network not found: %s", networkName)
	}
	if container.State == nil || !container.State.Running {
		return "", fmt.Errorf("container is not running")
	}
	ipAddress := container.NetworkSettings.Networks[networkName].IPAddress
	if ipAddress == "" {
		return "", fmt.Errorf("container has no IP address on the specified network: %s", networkName)
	}

	return ipAddress, nil
}
</file>

<file path="internal/helpers/sanitize.go">
package helpers

import "strings"

// SanitizeString takes a string and replaces characters unsuitable for HAProxy
// identifiers (like backend names, ACL names) with underscores.
// Allows alphanumeric characters, hyphen, and underscore. Consecutive disallowed
// characters are replaced by a single underscore.
func SanitizeString(input string) string {
	if input == "" {
		return ""
	}
	var result strings.Builder
	result.Grow(len(input))        // Pre-allocate roughly the right size
	lastCharWasUnderscore := false // Track if the last added char was a replacement underscore

	for _, r := range input {
		// Whitelist allowed characters
		if (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') || (r >= '0' && r <= '9') || r == '-' || r == '_' {
			result.WriteRune(r)
			lastCharWasUnderscore = false // Reset flag if allowed char is added
		} else {
			// Only add an underscore if the previous char wasn't already a replacement underscore
			if !lastCharWasUnderscore {
				result.WriteRune('_')
				lastCharWasUnderscore = true // Set flag as we added a replacement underscore
			}
			// If lastCharWasUnderscore is true, we skip adding another underscore
		}
	}
	return result.String()
}

// SanitizeFilename takes a string (originally intended for email, but should be generic filename)
// and replaces characters potentially unsafe for filenames with underscores.
// Allows alphanumeric, hyphen, and dot. Consecutive disallowed characters are replaced by a single underscore.
func SanitizeFilename(filename string) string {
	if filename == "" {
		return ""
	}
	var result strings.Builder
	result.Grow(len(filename))
	lastCharWasUnderscore := false

	for _, r := range filename {
		// Whitelist allowed characters for filenames (alphanumeric, hyphen, dot)
		if (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') || (r >= '0' && r <= '9') || r == '-' || r == '.' {
			result.WriteRune(r)
			lastCharWasUnderscore = false
		} else {
			if !lastCharWasUnderscore {
				result.WriteRune('_')
				lastCharWasUnderscore = true
			}
		}
	}
	return result.String()
}

func SafeIDPrefix(id string) string {
	if len(id) > 12 {
		return id[:12]
	}
	return id
}
</file>

<file path="internal/manager/certificates.go">
package manager

import (
	"bytes"
	"context"
	"crypto"
	"crypto/ecdsa"
	"crypto/elliptic"
	"crypto/rand"
	"crypto/x509"
	"encoding/pem"
	"fmt"
	"maps"
	"os"
	"path/filepath"
	"reflect"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/ameistad/haloy/internal/helpers"
	"github.com/ameistad/haloy/internal/logging"
	"github.com/go-acme/lego/v4/certificate"
	"github.com/go-acme/lego/v4/challenge/http01"
	"github.com/go-acme/lego/v4/lego"
	"github.com/go-acme/lego/v4/registration"
)

const (
	// Define a key for the certificate refresh debounce action
	refreshDebounceKey = "certificate_refresh"
	// Define the debounce delay for certificate refreshes
	refreshDebounceDelay = 5 * time.Second
	accountsDirName      = "accounts"
)

type CertificatesUser struct {
	Email        string
	Registration *registration.Resource
	privateKey   crypto.PrivateKey
}

func (u *CertificatesUser) GetEmail() string {
	return u.Email
}
func (u *CertificatesUser) GetRegistration() *registration.Resource {
	return u.Registration
}
func (u *CertificatesUser) GetPrivateKey() crypto.PrivateKey {
	return u.privateKey
}

type CertificatesClientManager struct {
	tlsStaging         bool
	keyManager         *CertificatesKeyManager
	clients            map[string]*lego.Client
	clientsMutex       sync.RWMutex
	sharedHTTPProvider *http01.ProviderServer
}

func NewCertificatesClientManager(certDir string, tlsStaging bool, httpProviderPort string) (*CertificatesClientManager, error) {
	keyDir := filepath.Join(certDir, accountsDirName)
	// Ensure the key directory exists
	if err := os.MkdirAll(keyDir, 0755); err != nil {
		return nil, fmt.Errorf("failed to create key directory '%s': %w", keyDir, err)
	}
	keyManager, err := NewCertificatesKeyManager(keyDir)
	if err != nil {
		return nil, fmt.Errorf("failed to create key manager: %w", err)
	}

	httpProvider := http01.NewProviderServer("", httpProviderPort)

	return &CertificatesClientManager{
		tlsStaging:         tlsStaging,
		clients:            make(map[string]*lego.Client),
		keyManager:         keyManager,
		sharedHTTPProvider: httpProvider,
	}, nil
}

func (cm *CertificatesClientManager) LoadOrRegisterClient(email string) (*lego.Client, error) {

	// Return client early if it exists
	cm.clientsMutex.RLock()
	client, ok := cm.clients[email]
	cm.clientsMutex.RUnlock()

	if ok {
		return client, nil
	}

	// Client doesn't exist, acquire write lock for creation
	cm.clientsMutex.Lock()
	defer cm.clientsMutex.Unlock()

	// Check again in case another goroutine created it while we were waiting
	if client, ok := cm.clients[email]; ok {
		return client, nil
	}

	privateKey, err := cm.keyManager.LoadOrCreateKey(email)
	if err != nil {
		return nil, fmt.Errorf("failed to load/create user key: %w", err)
	}

	user := &CertificatesUser{
		Email:      email,
		privateKey: privateKey,
	}

	legoConfig := lego.NewConfig(user)
	if cm.tlsStaging {
		legoConfig.CADirURL = lego.LEDirectoryStaging
	} else {
		legoConfig.CADirURL = lego.LEDirectoryProduction
	}

	client, err = lego.NewClient(legoConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to create lego client: %w", err)
	}

	// Configure HTTP challenge provider using a server that listens on port 8080
	// HAProxy is configured to forward /.well-known/acme-challenge/* requests to this server
	err = client.Challenge.SetHTTP01Provider(cm.sharedHTTPProvider)
	if err != nil {
		return nil, fmt.Errorf("failed to set HTTP challenge provider: %w", err)
	}

	// Register the user with the ACME server
	reg, err := client.Registration.Register(registration.RegisterOptions{TermsOfServiceAgreed: true})
	if err != nil {
		return nil, fmt.Errorf("failed to register user: %w", err)
	}
	user.Registration = reg

	cm.clients[email] = client

	return client, nil
}

type CertificatesManagerConfig struct {
	CertDir          string
	HTTPProviderPort string
	TlsStaging       bool
}

type CertificatesDomain struct {
	Canonical string
	Aliases   []string
	Email     string
}

type CertificatesManager struct {
	config        CertificatesManagerConfig
	domains       map[string]CertificatesDomain
	domainMutex   sync.RWMutex
	checkMutex    sync.Mutex
	ctx           context.Context
	cancel        context.CancelFunc
	clientManager *CertificatesClientManager
	updateSignal  chan<- string // Channel to signal successful updates
	debouncer     *helpers.Debouncer
}

func NewCertificatesManager(config CertificatesManagerConfig, updateSignal chan<- string) (*CertificatesManager, error) {
	// Create directories if they don't exist
	if err := os.MkdirAll(config.CertDir, 0755); err != nil {
		return nil, fmt.Errorf("failed to create certificate directory: %w", err)
	}

	ctx, cancel := context.WithCancel(context.Background())

	clientManager, err := NewCertificatesClientManager(config.CertDir, config.TlsStaging, config.HTTPProviderPort)
	if err != nil {
		cancel()
		return nil, fmt.Errorf("failed to create client manager: %w", err)
	}

	m := &CertificatesManager{
		config:        config,
		domains:       make(map[string]CertificatesDomain),
		ctx:           ctx,
		cancel:        cancel,
		clientManager: clientManager,
		updateSignal:  updateSignal, // Store the channel
		debouncer:     helpers.NewDebouncer(refreshDebounceDelay),
	}

	return m, nil
}

func (m *CertificatesManager) Start(logger *logging.Logger) {
	// Initial check might still be direct if desired on immediate startup
	// go m.checkRenewals() // Or use Refresh() if debounce on startup is ok
	go m.renewalLoop(logger)
	go m.cleanupLoop(logger)
}

func (m *CertificatesManager) Stop() {
	m.cancel()
	m.debouncer.Stop() // Stop the debouncer to clean up any pending timers
}

// AddDomains updates the set of domains managed by the certificate manager.
// It adds new domains, updates existing ones if aliases or email change,
// and removes domains that are no longer present in the input list.
func (m *CertificatesManager) AddDomains(managedDomains []CertificatesDomain, logger *logging.Logger) {
	m.domainMutex.Lock()
	defer m.domainMutex.Unlock()

	added := 0
	updated := 0
	removed := 0
	currentManaged := make(map[string]struct{}, len(managedDomains)) // Track for removal check

	for _, md := range managedDomains {
		if md.Canonical == "" {
			continue
		} // Skip invalid entries (no canonical domain)
		currentManaged[md.Canonical] = struct{}{} // Mark as required

		existing, exists := m.domains[md.Canonical]
		if !exists {
			// Ensure Aliases slice is initialized even if empty
			if md.Aliases == nil {
				md.Aliases = []string{}
			}
			m.domains[md.Canonical] = md // Add new domain
			added++
		} else {
			// Ensure new Aliases slice is initialized if needed
			if md.Aliases == nil {
				md.Aliases = []string{}
			}
			// Check if update needed (compare email and sorted aliases)
			sort.Strings(existing.Aliases)
			sort.Strings(md.Aliases)
			if existing.Email != md.Email || !reflect.DeepEqual(existing.Aliases, md.Aliases) {
				logger.Debug(fmt.Sprintf("Updating managed domain for (Email or Aliases changd) %s", md.Canonical))
				m.domains[md.Canonical] = md // Update entry
				updated++
			}
			// If no change, do nothing
		}
	}

	// Remove domains from m.domains that are no longer in the input list
	for domain := range m.domains {
		if _, ok := currentManaged[domain]; !ok {
			delete(m.domains, domain)
			logger.Debug(fmt.Sprintf("%s is no longer managed, removing", domain))
			removed++
		}
	}

	// Log summary of changes
	if added > 0 || updated > 0 || removed > 0 {
		// Trigger a refresh check - Refresh itself is non-blocking
		m.Refresh(logger)
	}
}

func (m *CertificatesManager) Refresh(logger *logging.Logger) {
	logger.Debug("Refresh requested for certificate manager, using debouncer.")

	// Define the action to perform after the debounce delay
	refreshAction := func() {
		m.checkRenewals(logger)
	}

	// Use the generic debouncer with a specific key for certificate refreshes
	m.debouncer.Debounce(refreshDebounceKey, refreshAction)
}

// renewalLoop periodically checks for certificates that need renewal
func (m *CertificatesManager) renewalLoop(logger *logging.Logger) {
	ticker := time.NewTicker(24 * time.Hour)
	defer ticker.Stop()

	// Do an initial check after a short delay to allow startup/discovery
	time.Sleep(30 * time.Second) // Delay initial check slightly
	m.checkRenewals(logger)

	for {
		select {
		case <-ticker.C:
			m.checkRenewals(logger)
		case <-m.ctx.Done():
			return
		}
	}
}

// cleanupLoop periodically checks for and removes expired certificates
func (m *CertificatesManager) cleanupLoop(logger *logging.Logger) {
	ticker := time.NewTicker(24 * time.Hour) // Check once a day
	defer ticker.Stop()

	time.Sleep(60 * time.Second) // Delay initial check slightly
	m.cleanupExpiredCertificates(logger)

	for {
		select {
		case <-ticker.C:
			m.cleanupExpiredCertificates(logger)
		case <-m.ctx.Done():
			return
		}
	}
}

func (m *CertificatesManager) checkRenewals(logger *logging.Logger) {
	m.checkMutex.Lock()
	defer func() {
		m.checkMutex.Unlock()
	}()

	m.domainMutex.RLock()
	domainsToCheck := make(map[string]CertificatesDomain, len(m.domains))
	if len(m.domains) > 0 {
		maps.Copy(domainsToCheck, m.domains)
	}
	m.domainMutex.RUnlock()
	if len(domainsToCheck) == 0 {
		return
	}

	for domain, managedDomainInfo := range domainsToCheck {
		// File paths always use the canonical domain name (map key)
		certFilePath := filepath.Join(m.config.CertDir, domain+".crt")
		// Combined file used by HAProxy
		combinedCertKeyPath := filepath.Join(m.config.CertDir, domain+".crt.key")

		_, err := os.Stat(combinedCertKeyPath) // Check for the combined file HAProxy needs
		needsObtain := os.IsNotExist(err)
		needsRenewalDueToExpiry := false
		sanMismatch := false // Flag for SAN list mismatch

		if !needsObtain {
			// Load the .crt file to check expiry and SANs
			certData, err := os.ReadFile(certFilePath)
			if err != nil {
				logger.Error(fmt.Sprintf("%s: Failed to read certificate file", domain))
				// If we can't read the .crt file, we can't check expiry/SANs.
				needsObtain = true // Treat read error as needing obtainment
			} else {
				// Use the local parseCertificate helper
				parsedCert, err := parseCertificate(certData)
				if err != nil {
					logger.Error(fmt.Sprintf("%s: Failed to parse certificate", domain))
					// Treat parse error as needing obtainment
					needsObtain = true
				} else {
					// Check expiry
					if time.Until(parsedCert.NotAfter) < 30*24*time.Hour {
						logger.Info(fmt.Sprintf("%s: Certficate expires soon and needs renewal...", domain))
						needsRenewalDueToExpiry = true
					}

					requiredDomains := append([]string{managedDomainInfo.Canonical}, managedDomainInfo.Aliases...)
					// Ensure requiredDomains is not nil if Aliases was nil
					if requiredDomains == nil {
						requiredDomains = []string{managedDomainInfo.Canonical}
					}

					currentDomains := parsedCert.DNSNames // Get SANs from loaded cert
					if currentDomains == nil {
						currentDomains = []string{}
					} // Ensure not nil for comparison

					// Sort both slices for consistent comparison
					sort.Strings(requiredDomains)
					sort.Strings(currentDomains)

					if !reflect.DeepEqual(requiredDomains, currentDomains) {
						sanMismatch = true
					}
				}
			}
		} // end if !needsObtain (checking existing cert)

		// Trigger obtain if file doesn't exist OR expiry nearing OR SAN list mismatch
		if needsObtain || needsRenewalDueToExpiry || sanMismatch {
			// Pass the full info needed for the request
			m.obtainCertificate(managedDomainInfo, logger)
		} else if !os.IsNotExist(err) { // Only log skipping if we actually checked a cert file
			logger.Info(fmt.Sprintf("Certificates for %s and aliases %s are valid.", domain, strings.Join(managedDomainInfo.Aliases, ", ")))
		}
	}
}

// obtainCertificate requests a certificate from ACME provider for the canonical domain and its aliases.
func (m *CertificatesManager) obtainCertificate(managedDomain CertificatesDomain, logger *logging.Logger) {
	canonicalDomain := managedDomain.Canonical
	email := managedDomain.Email
	// Ensure Aliases is not nil before appending
	aliases := managedDomain.Aliases
	if aliases == nil {
		aliases = []string{}
	}
	allDomains := append([]string{canonicalDomain}, aliases...) // Combine canonical + aliases

	logger.Debug("Starting certificate obtainment/renewal")

	client, err := m.clientManager.LoadOrRegisterClient(email)
	if err != nil {
		logger.Error(fmt.Sprintf("%s: Failed to load or register ACME client", canonicalDomain), err)
		return
	}
	logger.Debug(fmt.Sprintf("Using ACME client for %s with email %s", canonicalDomain, email))

	request := certificate.ObtainRequest{
		Domains: allDomains, // Request cert for canonical + aliases
		Bundle:  true,       // Bundle intermediate certs
	}

	logger.Debug(fmt.Sprintf("%s: Requesting certificate from ACME provider", canonicalDomain))

	logger.Info(fmt.Sprintf("Requesting new certificate for %s and aliases %s", canonicalDomain, strings.Join(aliases, ", ")))
	certificates, err := client.Certificate.Obtain(request)
	if err != nil {
		logger.Error(fmt.Sprintf("Failed to obtain certificate for domains, %s", strings.Join(allDomains, ", ")), err)
		return
	}
	logger.Debug(fmt.Sprintf("Obtained certificate for %s and aliases %s", canonicalDomain, strings.Join(aliases, ", ")))
	err = m.saveCertificate(canonicalDomain, certificates, logger)
	if err != nil {
		logger.Error(fmt.Sprintf("%s: Failed to save certificate", canonicalDomain), err)
		return
	}
	logger.Info(fmt.Sprintf("Successfully fetched certificate for %s and aliases %s", canonicalDomain, strings.Join(aliases, ", ")))

	// Send signal to HAProxy manager to update config
	logger.Debug(fmt.Sprintf("%s: Signaling for HAProxy config update after obtaining certificate", canonicalDomain))
	// Send canonical domain name non-blockingly (in case channel buffer full or receiver slow)
	select {
	case m.updateSignal <- canonicalDomain:
		logger.Debug(fmt.Sprintf("%s: Successfully signaled update", canonicalDomain))
	default:
		logger.Warn(fmt.Sprintf("%s: Update signal channel full or closed, skipping signal", canonicalDomain))
	}

}

func (m *CertificatesManager) saveCertificate(domain string, cert *certificate.Resource, logger *logging.Logger) error {
	// Save certificate (.crt)
	certPath := filepath.Join(m.config.CertDir, domain+".crt")
	if err := os.WriteFile(certPath, cert.Certificate, 0644); err != nil {
		return fmt.Errorf("failed to save certificate: %w", err)
	}

	// Save private key (.key)
	keyPath := filepath.Join(m.config.CertDir, domain+".key")
	if err := os.WriteFile(keyPath, cert.PrivateKey, 0600); err != nil {
		// Attempt cleanup of .crt file if key saving fails
		os.Remove(certPath)
		return fmt.Errorf("failed to save private key: %w", err)
	}

	// Create combined file for HAProxy (.crt.key) - some CAs might include key in Certificate field, others separate
	combinedPath := filepath.Join(m.config.CertDir, domain+".crt.key")
	// Ensure correct order: Cert first, then Key
	pemContent := bytes.Buffer{}
	pemContent.Write(cert.Certificate)
	// Add newline separator if cert doesn't end with one
	if len(cert.Certificate) > 0 && cert.Certificate[len(cert.Certificate)-1] != '\n' {
		pemContent.WriteByte('\n')
	}
	pemContent.Write(cert.PrivateKey)

	if err := os.WriteFile(combinedPath, pemContent.Bytes(), 0600); err != nil {
		// Attempt cleanup of .crt and .key files if combined saving fails
		os.Remove(certPath)
		os.Remove(keyPath)
		return fmt.Errorf("failed to save combined certificate/key: %w", err)
	}

	logger.Debug(fmt.Sprintf("%s: Saved certificate files", domain))
	return nil
}

func (m *CertificatesManager) cleanupExpiredCertificates(logger *logging.Logger) {
	logger.Debug("Starting certificate cleanup check")

	files, err := os.ReadDir(m.config.CertDir)
	if err != nil {
		logger.Error(fmt.Sprintf("Failed to read certificates directory: %s", m.config.CertDir), err)
		return
	}

	deleted := 0

	m.domainMutex.RLock()
	managedDomainsMap := make(map[string]struct{}, len(m.domains))
	for domain := range m.domains { // Keys are canonical domains
		managedDomainsMap[domain] = struct{}{}
	}
	m.domainMutex.RUnlock()

	for _, file := range files {
		// Look for the combined file HAProxy uses
		if !file.IsDir() && strings.HasSuffix(file.Name(), ".crt.key") {
			domain := strings.TrimSuffix(file.Name(), ".crt.key")
			_, isManaged := managedDomainsMap[domain]

			// Define paths for all related files
			combinedPath := filepath.Join(m.config.CertDir, file.Name())
			certPath := filepath.Join(m.config.CertDir, domain+".crt")
			keyPath := filepath.Join(m.config.CertDir, domain+".key")

			// Check expiry using the .crt file
			certData, err := os.ReadFile(certPath)
			if err != nil {
				// If .crt is missing but .crt.key exists, log and potentially clean up if unmanaged
				if os.IsNotExist(err) && !isManaged {
					logger.Warn(fmt.Sprintf("%s: Found orphaned combined file for unmanaged domain (.crt missing). Deleting.", domain))
					os.Remove(combinedPath)
					os.Remove(keyPath) // Try removing .key too if it exists
					deleted++
				} else if !os.IsNotExist(err) {
					// Log other read errors
					logger.Warn(fmt.Sprintf("Failed to read certificate file during cleanup: %s", certPath), err)
				}
				continue // Skip if we can't read the cert
			}

			// Use the local parseCertificate helper
			parsedCert, err := parseCertificate(certData)
			if err != nil {
				logger.Warn(fmt.Sprintf("Failed to parse certificate during cleanup: %s", certPath))
				continue // Skip if parsing fails
			}

			// Delete if expired AND unmanaged
			if time.Now().After(parsedCert.NotAfter) && !isManaged {
				logger.Debug(fmt.Sprintf("%s: Deleting expired certificate files for unmanaged domain", domain))
				os.Remove(combinedPath)
				os.Remove(certPath)
				os.Remove(keyPath)
				deleted++
			}
		}
	} // end loop over files

	logger.Debug("Certificate cleanup complete. Deleted expired/orphaned certificate sets for unmanaged domains")
}

// parseCertificate takes PEM encoded certificate data and returns the parsed x509.Certificate
// Kept unexported as it's only used internally by checkRenewals and cleanup.
func parseCertificate(certData []byte) (*x509.Certificate, error) {
	block, _ := pem.Decode(certData)
	if block == nil || block.Type != "CERTIFICATE" {
		return nil, fmt.Errorf("failed to decode PEM block containing certificate")
	}
	cert, err := x509.ParseCertificate(block.Bytes)
	if err != nil {
		return nil, fmt.Errorf("failed to parse certificate: %w", err)
	}
	return cert, nil
}

// CertificatesKeyManager handles private key operations for the ACME client
type CertificatesKeyManager struct {
	keyDir string
}

// NewCertificatesKeyManager creates a new key manager
func NewCertificatesKeyManager(keyDir string) (*CertificatesKeyManager, error) {
	stat, err := os.Stat(keyDir)
	if err != nil {
		if os.IsNotExist(err) {
			return nil, fmt.Errorf("key directory '%s' does not exist; ensure init process has created it", keyDir)
		}
		return nil, fmt.Errorf("failed to stat key directory '%s': %w", keyDir, err)
	}
	if !stat.IsDir() {
		return nil, fmt.Errorf("key directory path '%s' is not a directory", keyDir)
	}

	return &CertificatesKeyManager{
		keyDir: keyDir,
	}, nil
}

// LoadOrCreateKey loads an existing account key or creates a new one
func (km *CertificatesKeyManager) LoadOrCreateKey(email string) (crypto.PrivateKey, error) {
	// Sanitize email for filename
	filename := helpers.SanitizeFilename(email) + ".key"
	keyPath := filepath.Join(km.keyDir, filename)

	// Check if key already exists
	if _, err := os.Stat(keyPath); err == nil {
		// Key exists, load it
		return km.loadKey(keyPath)
	}

	// Key doesn't exist, create a new one
	return km.createKey(keyPath)
}

// loadKey loads a private key from disk
func (km *CertificatesKeyManager) loadKey(path string) (crypto.PrivateKey, error) {
	// Read key file
	keyBytes, err := os.ReadFile(path)
	if err != nil {
		return nil, fmt.Errorf("failed to read key file: %w", err)
	}

	// Decode PEM
	keyBlock, _ := pem.Decode(keyBytes)
	if keyBlock == nil {
		return nil, fmt.Errorf("failed to decode PEM block")
	}

	// Parse private key
	switch keyBlock.Type {
	case "EC PRIVATE KEY":
		return x509.ParseECPrivateKey(keyBlock.Bytes)
	default:
		return nil, fmt.Errorf("unsupported key type: %s", keyBlock.Type)
	}
}

// createKey creates a new ECDSA private key and saves it to disk
func (km *CertificatesKeyManager) createKey(path string) (crypto.PrivateKey, error) {
	// Generate new ECDSA key (P-256 for good balance of security and performance)
	privateKey, err := ecdsa.GenerateKey(elliptic.P256(), rand.Reader)
	if err != nil {
		return nil, fmt.Errorf("failed to generate private key: %w", err)
	}

	// Encode private key to PEM
	keyBytes, err := x509.MarshalECPrivateKey(privateKey)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal private key: %w", err)
	}

	// Create PEM block
	pemBlock := &pem.Block{
		Type:  "EC PRIVATE KEY",
		Bytes: keyBytes,
	}

	// Write key to file
	keyFile, err := os.OpenFile(path, os.O_WRONLY|os.O_CREATE|os.O_TRUNC, 0600)
	if err != nil {
		return nil, fmt.Errorf("failed to create key file: %w", err)
	}
	defer keyFile.Close()

	if err := pem.Encode(keyFile, pemBlock); err != nil {
		return nil, fmt.Errorf("failed to write key file: %w", err)
	}

	return privateKey, nil
}
</file>

<file path="scripts/build-upload-cli-manager.sh">
#!/bin/bash
set -e

# Ensure an argument is provided
if [ -z "$1" ]; then
    echo "Usage: $0 <hostname>"
    exit 1
fi

BINARY_NAME=haloy
HOSTNAME=$1

# Use the current username from the shell
USERNAME=$(whoami)

# If haloy-cli exists, remove it
if [ -f haloy-cli ]; then
    rm haloy-cli
fi

# Extract the version from internal/version/version.go (assumes format: var Version = "v0.1.9")
version=$(grep 'var Version' ../internal/version/version.go | sed 's/.*"\(.*\)".*/\1/')
echo "Building version: $version"

# Build the CLI binary from cmd/cli using the extracted version
GOOS=linux GOARCH=amd64 go build -ldflags="-X 'github.com/ameistad/haloy/cmd.version=$version'" -o $BINARY_NAME ../cmd/cli

# Ensure remote bin dir exists
ssh "${USERNAME}@${HOSTNAME}" "mkdir -p /home/${USERNAME}/.local/bin"

# Upload the binary via scp using the current username
scp $BINARY_NAME ${USERNAME}@"$HOSTNAME":/home/${USERNAME}/.local/bin/$BINARY_NAME

# Remove binary after copying
if [ -f $BINARY_NAME ]; then
    rm $BINARY_NAME
fi

# Build the Docker image from Dockerfile
docker build --platform linux/amd64 -t haloy-manager -t haloy-manager:latest -t ghcr.io/ameistad/haloy-manager:latest -f ../build/manager/Dockerfile ../

# Save the image to a tarball
docker save -o haloy-manager.tar haloy-manager

# Upload the Docker image tarball via scp to the server's /tmp directory
scp haloy-manager.tar ${USERNAME}@"$HOSTNAME":/tmp/haloy-manager.tar

# Load the Docker image on the remote server and remove the tarball
echo "Loading Docker image on remote server..."
if ssh ${USERNAME}@"$HOSTNAME" "docker load -i /tmp/haloy-manager.tar && rm /tmp/haloy-manager.tar"; then
    echo "Successfully loaded Docker image and removed tarball on remote server."
else
    echo "Warning: There was an issue with loading the Docker image or removing the tarball on the remote server."
    echo "You may need to manually check and clean up /tmp/haloy-manager.tar on ${HOSTNAME}"
fi

# Remove the local tarball
rm haloy-manager.tar
</file>

<file path="scripts/build-upload-cli.sh">
#!/bin/bash
set -e

# Ensure an argument is provided
if [ -z "$1" ]; then
    echo "Usage: $0 <hostname>"
    exit 1
fi

BINARY_NAME=haloy
HOSTNAME=$1

# Use the current username from the shell
USERNAME=$(whoami)

# If haloy-cli exists, remove it
if [ -f haloy-cli ]; then
    rm haloy-cli
fi

# Extract the version from internal/version/version.go (assumes format: var Version = "v0.1.9")
version=$(grep 'var Version' ../internal/version/version.go | sed 's/.*"\(.*\)".*/\1/')
echo "Building version: $version"

# Build the CLI binary from cmd/cli using the extracted version
GOOS=linux GOARCH=amd64 go build -ldflags="-X 'github.com/ameistad/haloy/cmd.version=$version'" -o $BINARY_NAME ../cmd/cli

# Ensure remote bin dir exists
ssh "${USERNAME}@${HOSTNAME}" "mkdir -p /home/${USERNAME}/.local/bin"

# Upload the binary via scp using the current username
scp $BINARY_NAME ${USERNAME}@"$HOSTNAME":/home/${USERNAME}/.local/bin/$BINARY_NAME

# Remove binary after copying
if [ -f $BINARY_NAME ]; then
    rm $BINARY_NAME
fi
</file>

<file path="internal/cli/commands/rollback.go">
package commands

import (
	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/deploy"
	"github.com/ameistad/haloy/internal/ui"
	"github.com/spf13/cobra"
)

func RollbackAppCmd() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "rollback <app-name>",
		Short: "Rollback an application",
		Long:  `Rollback an application to a previous deployment`,
		Args:  cobra.ExactArgs(1),
		Run: func(cmd *cobra.Command, args []string) {
			appName := args[0]
			appConfig, err := config.AppConfigByName(appName)
			if err != nil {
				ui.Error("Failed to get configuration for %q: %v\n", appName, err)
				return
			}

			// Retrieve container flag if provided.
			deploymentIDFlag, _ := cmd.Flags().GetString("deployment")
			var targetDeploymentID string
			if deploymentIDFlag != "" {
				targetDeploymentID = deploymentIDFlag
			}

			if err := deploy.RollbackApp(appConfig, targetDeploymentID); err != nil {
				ui.Error("Failed to rollback %q: %v\n", appName, err)
			} else {
				ui.Success("Rollback of %s completed successfully.\n", appName)
			}
		},
	}

	cmd.Flags().StringP("deployment", "d", "", "Specify deployment ID to use for rollback")
	return cmd
}
</file>

<file path="internal/config/paths.go">
package config

import (
	"fmt"
	"os"
	"path/filepath"
)

// Defaults to ~/.config/haloy
// If HALOY_CONFIG_PATH is set, it will use that instead.
func ConfigDirPath() (string, error) {
	// allow overriding via env
	if envPath, ok := os.LookupEnv("HALOY_CONFIG_PATH"); ok && envPath != "" {
		return envPath, nil
	}

	home, err := os.UserHomeDir()
	if err != nil {
		return "", err
	}
	return filepath.Join(home, ".config", "haloy"), nil
}

// EnsureConfigDir makes sure the config dir exists (creates it if needed)
func EnsureConfigDir() (string, error) {
	path, err := ConfigDirPath()
	if err != nil {
		return "", err
	}
	if err := os.MkdirAll(path, 0755); err != nil {
		return "", fmt.Errorf("failed to create config directory '%s': %w", path, err)
	}
	return path, nil
}

// ConfigFilePath returns "~/.config/haloy/apps.yml".
func ConfigFilePath() (string, error) {
	configDirPath, err := ConfigDirPath()
	if err != nil {
		return "", err
	}
	return filepath.Join(configDirPath, ConfigFileName), nil
}

func ConfigContainersPath() (string, error) {
	configDirPath, err := ConfigDirPath()
	if err != nil {
		return "", err
	}
	return filepath.Join(configDirPath, "containers"), nil
}

func ServicesDockerComposeFilePath() (string, error) {
	containersPath, err := ConfigContainersPath()
	if err != nil {
		return "", err
	}
	return filepath.Join(containersPath, "docker-compose.yml"), nil
}

func HAProxyConfigFilePath() (string, error) {
	containersPath, err := ConfigContainersPath()
	if err != nil {
		return "", err
	}
	return filepath.Join(containersPath, "haproxy-config", HAProxyConfigFileName), nil
}

func LogsPath() (string, error) {
	containersPath, err := ConfigContainersPath()
	if err != nil {
		return "", err
	}
	return filepath.Join(containersPath, "logs"), nil
}
</file>

<file path="internal/deploy/rollback.go">
package deploy

import (
	"context"
	"fmt"
	"sort"
	"time"

	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/docker"
	"github.com/ameistad/haloy/internal/helpers"
	"github.com/ameistad/haloy/internal/ui"
	"github.com/docker/docker/api/types/container"
	"github.com/docker/docker/api/types/filters"
	"github.com/docker/docker/client"
)

func RollbackApp(appConfig *config.AppConfig, targetDeploymentID string) error {
	ctx, cancel := context.WithTimeout(context.Background(), DefaultDeployTimeout)
	defer cancel()
	dockerClient, err := docker.NewClient(ctx)
	if err != nil {
		return fmt.Errorf("failed to create Docker client: %w", err)
	}
	defer dockerClient.Close()

	deployments, err := deployments(ctx, dockerClient, appConfig.Name)
	if err != nil {
		return err
	}

	if len(deployments.oldDeployments) == 0 {
		return fmt.Errorf("there are no older deployments to rollback to for %s", appConfig.Name)
	}

	targetDeployment := deployments.oldDeployments[0]

	// If a target deployment ID is provided, find the corresponding deployment
	if targetDeploymentID != "" {
		found := false
		for _, deployment := range deployments.oldDeployments {
			if deployment.ID == targetDeploymentID {
				targetDeployment = deployment
				found = true
				break
			}
		}
		if !found {
			return fmt.Errorf("target deployment %s not found in previous deployments", targetDeploymentID)
		}
	}

	if targetDeployment.ID == deployments.currentDeployment.ID {
		return fmt.Errorf("target deployment %s is already the current deployment", targetDeployment.ID)
	}

	if err := RollbackToDeployment(ctx, dockerClient, appConfig.Name, deployments.currentDeployment, targetDeployment); err != nil {
		return fmt.Errorf("failed to rollback to deployment %s: %w", targetDeployment.ID, err)
	}

	return nil
}

func RollbackToDeployment(ctx context.Context, dockerClient *client.Client, appName string, currentDeployment, targetDeployment deploymentInfo) error {
	// Track which containers we've started so we can clean them up on failure
	startError := false
	healthCheckError := false

	for _, targetContainerID := range targetDeployment.containerIDs {
		ui.Info("Starting target container %s...", helpers.SafeIDPrefix(targetContainerID))
		if err := dockerClient.ContainerStart(ctx, targetContainerID, container.StartOptions{}); err != nil {
			startError = true
			ui.Error("failed to start target container %s: %v", helpers.SafeIDPrefix(targetContainerID), err)
			break
		}
	}

	if !startError {
		for _, targetContainerID := range targetDeployment.containerIDs {
			ui.Info("Checking health of target container %s...", helpers.SafeIDPrefix(targetContainerID))
			// Give the container some time to start
			healtCheckDelay := 3 * time.Second
			if err := docker.HealthCheckContainer(ctx, dockerClient, targetContainerID, healtCheckDelay); err != nil {
				healthCheckError = true
				ui.Error("target container %s is not healthy: %v", helpers.SafeIDPrefix(targetContainerID), err)
				break
			}
		}

	}

	ignoreDeploymentID := targetDeployment.ID
	if startError || healthCheckError {
		ui.Error("Rollback failed, cleaning up started containers...")
		ignoreDeploymentID = currentDeployment.ID
	}
	if _, err := docker.StopContainers(ctx, dockerClient, appName, ignoreDeploymentID); err != nil {
		return fmt.Errorf("failed to stop containers for app %s: %w", appName, err)
	}

	if startError {
		return fmt.Errorf("failed to start target containers for app %s", appName)
	}
	if healthCheckError {
		return fmt.Errorf("target containers for app %s are not healthy", appName)
	}

	return nil
}

type deploymentInfo struct {
	ID           string
	containerIDs []string
}

type deploymentsResult struct {
	currentDeployment deploymentInfo
	oldDeployments    []deploymentInfo
}

// deployments retrieves and sorts all deployments for an app by timestamp (newest first)
func deployments(ctx context.Context, dockerClient *client.Client, appName string) (deploymentsResult, error) {

	result := deploymentsResult{}

	filtersArgs := filters.NewArgs()
	filtersArgs.Add("label", fmt.Sprintf("%s=%s", config.LabelRole, config.AppLabelRole))
	filtersArgs.Add("label", fmt.Sprintf("%s=%s", config.LabelAppName, appName))

	// Get all containers, including stopped ones
	containers, err := dockerClient.ContainerList(ctx, container.ListOptions{
		Filters: filtersArgs,
		All:     true, // Include stopped containers for rollback purposes
	})
	if err != nil {
		return result, fmt.Errorf("failed to list containers: %w", err)
	}

	if len(containers) == 0 {
		return result, fmt.Errorf("no containers found for app %s", appName)
	}

	runningDeployment := deploymentInfo{
		ID:           "",
		containerIDs: []string{},
	}
	deployments := make(map[string][]string)
	for _, container := range containers {
		deploymentID := container.Labels[config.LabelDeploymentID]
		if deploymentID == "" {
			continue
		}

		// Add ALL containers to deployments map regardless of state
		deployments[deploymentID] = append(deployments[deploymentID], container.ID)

		// Only update running deployment for running containers
		if container.State == "running" {
			runningDeployment = deploymentInfo{
				ID:           deploymentID,
				containerIDs: append(runningDeployment.containerIDs, container.ID),
			}
		}
	}

	if len(deployments) == 0 {
		return result, fmt.Errorf("no valid deployment information found for app %s", appName)
	}

	oldDeployments := make([]deploymentInfo, 0, len(deployments))
	for deploymentID, containerIDs := range deployments {
		if deploymentID == runningDeployment.ID {
			continue
		}

		// Skip deployment that are newer than the running deployment.
		if deploymentID > runningDeployment.ID {
			continue
		}
		oldDeployments = append(oldDeployments, deploymentInfo{
			ID:           deploymentID,
			containerIDs: containerIDs,
		})
	}

	// Sort deployments by timestamp (newest first)
	sort.Slice(oldDeployments, func(i, j int) bool {
		// Reliable timestamp sorting (ISO format or Unix timestamp strings)
		return oldDeployments[i].ID > oldDeployments[j].ID
	})

	result = deploymentsResult{
		currentDeployment: runningDeployment,
		oldDeployments:    oldDeployments,
	}
	return result, nil
}
</file>

<file path="internal/embed/templates/apps.yml">
apps:
</file>

<file path="internal/embed/templates/haproxy.cfg">
global
    master-worker
    log stdout format raw local0

    # Increase the SSL cache to improve performance
    tune.ssl.cachesize 20000
    ssl-default-bind-options no-sslv3 no-tlsv10 no-tlsv11 no-tls-tickets
    ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256
    ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384

defaults
    mode http
    timeout connect 5000ms
    timeout client  50000ms
    timeout server  50000ms
    log global
    option httplog


frontend http-in
    bind *:80
    mode http

    # Add ACME HTTP-01 challenge path exception
    acl is_acme_challenge path_beg /.well-known/acme-challenge/

    # Dynamically generated code by haloy
{{ .HTTPFrontend }}
    # End of dynamically generated code by haloy

    use_backend acme_challenge if is_acme_challenge

frontend https-in
    bind *:443 ssl crt /usr/local/etc/haproxy/certs/ alpn h2,http/1.1
    mode http

    # Add ACME HTTP-01 challenge path exception for HTTPS
    acl is_acme_challenge path_beg /.well-known/acme-challenge/

    # Dynamically generated code by haloy (Host-based routing)
{{ .HTTPSFrontend }}
{{ .HTTPSFrontendUseBackend}}
    # End of dynamically generated code by haloy

    use_backend acme_challenge if is_acme_challenge

    # Fallback for unmatched requests
    default_backend default_backend


# Dynamically generated code by haloy
{{ .Backends }}
# End of dynamically generated code by haloy

backend acme_challenge
    mode http
    # Forward to the manager container which will handle the ACME challenge
    http-request set-header X-Forwarded-For %[src]
    http-request set-header X-Forwarded-Proto http
    http-request set-header X-Forwarded-Port %[dst_port]
    http-request set-header Host %[req.hdr(host)]

    # ACME Challenge backend with no-check because it's only available for a short time.
    server haloy-manager haloy-manager:8080 no-check

    # Return custom HTML page if backend is unreachable
    errorfile 503 /usr/local/etc/haproxy/errors/404.html


# Default backend for unmatched requests
backend default_backend
    http-request deny deny_status 404
</file>

<file path="internal/config/labels.go">
package config

import (
	"fmt"
	"sort"
	"strings"
	"text/tabwriter"

	"github.com/ameistad/haloy/internal/helpers"
	"github.com/fatih/color"
)

const (
	LabelAppName         = "haloy.appName"
	LabelDeploymentID    = "haloy.deployment-id"
	LabelHealthCheckPath = "haloy.health-check-path" // optional default to "/"
	LabelACMEEmail       = "haloy.acme.email"
	LabelPort            = "haloy.port" // optional

	// Format strings for indexed canonical domains and aliases.
	// Use fmt.Sprintf(LabelDomainCanonical, index) to get "haloy.domain.<index>"
	LabelDomainCanonical = "haloy.domain.%d"
	// Use fmt.Sprintf(LabelDomainAlias, domainIndex, aliasIndex) to get "haloy.domain.<domainIndex>.alias.<aliasIndex>"
	LabelDomainAlias = "haloy.domain.%d.alias.%d"

	// Used to identify the role of the container (e.g., "haproxy", "manager", etc.)
	LabelRole = "haloy.role"
)

const (
	HAProxyLabelRole = "haproxy"
	ManagerLabelRole = "manager"
	AppLabelRole     = "app"
)

type ContainerLabels struct {
	AppName         string
	DeploymentID    string
	HealthCheckPath string
	ACMEEmail       string
	Port            string
	Domains         []Domain
	Role            string
}

// Parse from docker labels to ContainerLabels struct.
func ParseContainerLabels(labels map[string]string) (*ContainerLabels, error) {
	cl := &ContainerLabels{
		AppName:      labels[LabelAppName],
		DeploymentID: labels[LabelDeploymentID],
		ACMEEmail:    labels[LabelACMEEmail],
		Role:         labels[LabelRole],
	}

	if v, ok := labels[LabelPort]; ok {
		cl.Port = v
	} else {
		cl.Port = DefaultContainerPort
	}

	// Set HealthCheckPath with default value.
	if v, ok := labels[LabelHealthCheckPath]; ok {
		cl.HealthCheckPath = v
	} else {
		cl.HealthCheckPath = DefaultHealthCheckPath
	}

	// Parse domains
	domainMap := make(map[int]*Domain)

	// Process domain and alias labels.
	for key, value := range labels {
		if !strings.HasPrefix(key, "haloy.domain.") {
			continue
		}
		if strings.Contains(key, ".alias.") {
			// Parse alias key: "haloy.domain.<domainIdx>.alias.<aliasIdx>"
			var domainIdx, aliasIdx int
			if _, err := fmt.Sscanf(key, LabelDomainAlias, &domainIdx, &aliasIdx); err != nil {
				// Skip keys that don't conform.
				continue
			}
			domain := getOrCreateDomain(domainMap, domainIdx)
			domain.Aliases = append(domain.Aliases, value)
		} else {
			// Parse canonical domain key: "haloy.domain.<domainIdx>"
			var domainIdx int
			if _, err := fmt.Sscanf(key, LabelDomainCanonical, &domainIdx); err != nil {
				continue
			}
			domain := getOrCreateDomain(domainMap, domainIdx)
			domain.Canonical = value
		}
	}

	// Build the sorted slice of domains.
	var indices []int
	for i := range domainMap {
		indices = append(indices, i)
	}
	sort.Ints(indices)
	for _, i := range indices {
		cl.Domains = append(cl.Domains, *domainMap[i])
	}

	// Optional: validate the parsed labels.
	if err := cl.IsValid(); err != nil {
		return nil, err
	}

	return cl, nil
}

// getOrCreateDomain returns an existing *config.Domain from domainMap or creates a new one.
func getOrCreateDomain(domainMap map[int]*Domain, idx int) *Domain {
	if domain, exists := domainMap[idx]; exists {
		return domain
	}
	domainMap[idx] = &Domain{}
	return domainMap[idx]
}

// ToLabels converts the ContainerLabels struct back to a map[string]string.
func (cl *ContainerLabels) ToLabels() map[string]string {
	labels := map[string]string{
		LabelAppName:         cl.AppName,
		LabelDeploymentID:    cl.DeploymentID,
		LabelHealthCheckPath: cl.HealthCheckPath,
		LabelPort:            cl.Port,
		LabelACMEEmail:       cl.ACMEEmail,
		LabelRole:            cl.Role,
	}

	// Iterate through the domains slice.
	for i, domain := range cl.Domains {
		// Set canonical domain.
		canonicalKey := fmt.Sprintf(LabelDomainCanonical, i)
		labels[canonicalKey] = domain.Canonical

		// Set aliases.
		for j, alias := range domain.Aliases {
			aliasKey := fmt.Sprintf(LabelDomainAlias, i, j)
			labels[aliasKey] = alias
		}
	}

	return labels
}

// We assume that all labels need to be present for the labels to be valid.
func (cl *ContainerLabels) IsValid() error {
	if cl.AppName == "" {
		return fmt.Errorf("appName is required")
	}
	if cl.DeploymentID == "" {
		return fmt.Errorf("deploymentID is required")
	}

	if cl.ACMEEmail == "" {
		return fmt.Errorf("ACME email is required")
	}

	if !helpers.IsValidEmail(cl.ACMEEmail) {
		return fmt.Errorf("ACME email is not valid")
	}

	if cl.Port == "" {
		return fmt.Errorf("port is required")
	}

	if len(cl.Domains) == 0 {
		return fmt.Errorf("at least one domain is required")
	}

	if cl.Role != AppLabelRole {
		return fmt.Errorf("role must be '%s'", AppLabelRole)
	}

	return nil
}

func (cl *ContainerLabels) String() string {
	bold := color.New(color.Bold).SprintFunc()
	yellow := color.New(color.FgHiYellow).SprintFunc()
	cyan := color.New(color.FgHiCyan).SprintFunc()

	var builder strings.Builder
	// Create a tabwriter with padding settings.
	w := tabwriter.NewWriter(&builder, 0, 0, 2, ' ', 0)

	// TODO: use ui package for colors
	fmt.Fprintf(w, "%s:\t%s\n", yellow("App Name"), cyan(cl.AppName))
	fmt.Fprintf(w, "%s:\t%s\n", yellow("Deployment ID"), cyan(cl.DeploymentID))
	fmt.Fprintf(w, "%s:\t%s\n", yellow("Health Check Path"), cyan(cl.HealthCheckPath))
	fmt.Fprintf(w, "%s:\t%s\n", yellow("ACME Email"), cyan(cl.ACMEEmail))
	fmt.Fprintf(w, "%s:\t%s\n", yellow("Port"), cyan(cl.Port))

	fmt.Fprintln(w, yellow("Domains:"))
	for i, domain := range cl.Domains {
		fmt.Fprintf(w, "\t%s\t%s\n", bold(fmt.Sprintf("Domain %d", i+1)), cyan(domain.Canonical))
		if len(domain.Aliases) > 0 {
			fmt.Fprintf(w, "\t%s\t%s\n", yellow("Aliases"), cyan(strings.Join(domain.Aliases, ", ")))
		}
	}
	w.Flush()

	return builder.String()
}
</file>

<file path="internal/embed/init/containers/docker-compose.yml">
services:

  haloy-manager:
    image: ghcr.io/ameistad/haloy-manager:latest
    labels:
      - "haloy.role=manager"
    container_name: haloy-manager
    # Add group_add to allow Docker socket access for the golang docker client.
    group_add:
      - "${DOCKER_GID:-999}"  # Default to 999 if not set
    volumes:
      - ./haproxy-config:/haproxy-config:rw
      - ./cert-storage:/cert-storage:rw
      - ./logs/:/logs:rw
      # Enable Docker socket access for the golang docker client
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      # Certificates server
      - "127.0.0.1:8080:8080"
      # Logserver
      - "127.0.0.1:9000:9000"
    user: root
    restart: unless-stopped
    networks:
      - haloy-network
  haloy-haproxy:
    image: haproxy:3.1.5
    labels:
      - "haloy.role=haproxy"
    # This need to be set to haloy-haproxy so the manager can find it.
    container_name: haloy-haproxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./haproxy-config:/usr/local/etc/haproxy/config:ro
      - ./cert-storage:/usr/local/etc/haproxy/certs:rw
      - ./entrypoint.sh:/entrypoint.sh:ro
      - ./error-pages:/usr/local/etc/haproxy/errors:ro
    entrypoint: ["/bin/sh", "/entrypoint.sh"]
    user: root
    restart: unless-stopped
    networks:
      - haloy-network
    depends_on:
      - haloy-manager

networks:
  haloy-network:
    name: haloy-public
    external: true
</file>

<file path="internal/version/version.go">
package version

var Version = "v0.1.3"

func GetVersion() string {
	return Version
}
</file>

<file path="internal/config/validate.go">
package config

import (
	"errors"
	"fmt"
	"path/filepath"
	"reflect"
	"regexp"
	"strings"

	"github.com/ameistad/haloy/internal/helpers"
	"gopkg.in/yaml.v3"
)

// ValidateDomain checks that a domain string is not empty and has a basic valid structure.
func ValidateDomain(domain string) error {
	if domain == "" {
		return errors.New("domain cannot be empty")
	}
	// This regex enforces that labels start/end with alphanumeric chars
	// and contain only alphanumeric chars or hyphens internally.
	pattern := `^([a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?\.)+[a-zA-Z]{2,}$`
	matched, err := regexp.MatchString(pattern, domain)
	if err != nil {
		// Consider logging the regex compilation error if it happens
		return fmt.Errorf("domain regex error: %w", err)
	}
	if !matched {
		// Add checks for other potential issues not covered by regex, if needed
		if strings.HasPrefix(domain, "-") || strings.Contains(domain, ".-") {
			return fmt.Errorf("invalid domain format: labels cannot start with a hyphen: %s", domain)
		}
		// Add more specific checks if the regex fails for unexpected reasons
		return fmt.Errorf("invalid domain format: %s", domain)
	}
	return nil
}

// ValidateHealthCheckPath checks that a health check path is a valid URL path.
func ValidateHealthCheckPath(path string) error {
	if path == "" {
		return errors.New("health check path cannot be empty")
	}
	if path[0] != '/' {
		return errors.New("health check path must start with a slash")
	}
	return nil
}

func (ac *AppConfig) Validate() error {
	if ac.Name == "" {
		return errors.New("name cannot be empty")
	}

	if err := ac.Source.Validate(); err != nil {
		return fmt.Errorf("invalid source: %w", err)
	}
	if len(ac.Domains) == 0 {
		return fmt.Errorf("no domains defined")
	}
	for _, domain := range ac.Domains {
		if err := ValidateDomain(domain.Canonical); err != nil {
			return err
		}
		for _, alias := range domain.Aliases {
			if err := ValidateDomain(alias); err != nil {
				return fmt.Errorf("alias '%s': %w", alias, err)
			}
		}
	}

	// Validate ACME email.
	if ac.ACMEEmail == "" {
		return fmt.Errorf("missing ACME email used to get TLS certificates")
	}
	if !helpers.IsValidEmail(ac.ACMEEmail) {
		return fmt.Errorf("invalid ACME email '%s'", ac.ACMEEmail)
	}

	// Validate environment variables.
	for j, envVar := range ac.Env {
		if err := envVar.Validate(); err != nil {
			return fmt.Errorf("env[%d]: %w", j, err)
		}
	}

	// Validate volumes.
	for _, volume := range ac.Volumes {
		// Expected format: /host/path:/container/path[:options]
		parts := strings.Split(volume, ":")
		if len(parts) < 2 || len(parts) > 3 {
			return fmt.Errorf("invalid volume mapping '%s'; expected '/host/path:/container/path[:options]'", volume)
		}
		// Validate host path.
		if !filepath.IsAbs(parts[0]) {
			return fmt.Errorf("volume host path '%s' in '%s' is not an absolute path", parts[0], volume)
		}
		// Validate container path.
		if !filepath.IsAbs(parts[1]) {
			return fmt.Errorf("volume container path '%s' in '%s' is not an absolute path", parts[1], volume)
		}
	}

	// Validate health check path.
	if err := ValidateHealthCheckPath(ac.HealthCheckPath); err != nil {
		return err
	}

	// Validate replicas.
	if ac.Replicas == nil {
		return errors.New("replicas cannot be nil")
	}
	if *ac.Replicas < 1 {
		return errors.New("replicas must be at least 1")
	}

	return nil
}

// ValidateConfigFile checks that the Config is well-formed.
func (c *Config) Validate() error {
	// Check that at least one app is defined.
	if len(c.Apps) == 0 {
		return errors.New("config: no apps defined")
	}
	for _, app := range c.Apps {
		err := app.Validate()
		if err != nil {
			return fmt.Errorf("app '%s': %w", app.Name, err)
		}
	}
	return nil
}

// This is used in unmarshalling to check for unknown fields in the YAML file.
// extractYAMLFieldNames returns a map of field names from YAML struct tags
func ExtractYAMLFieldNames(t reflect.Type) map[string]bool {
	fields := make(map[string]bool)
	for i := 0; i < t.NumField(); i++ {
		field := t.Field(i)
		tag := field.Tag.Get("yaml")
		if tag == "" || tag == "-" {
			continue
		}

		// Split on comma to handle tags like `yaml:"name,omitempty"`
		parts := strings.Split(tag, ",")
		fields[parts[0]] = true
	}
	return fields
}

// checkUnknownFields verifies no unknown fields exist in the YAML node
func CheckUnknownFields(node *yaml.Node, expectedFields map[string]bool, context string) error {
	if node.Kind != yaml.MappingNode {
		return nil
	}

	// YAML mapping nodes have key-value pairs in sequence
	for i := 0; i < len(node.Content); i += 2 {
		// Skip if we somehow have an odd number of items
		if i+1 >= len(node.Content) {
			continue
		}

		// Get the key name
		key := node.Content[i].Value

		// Check if it's a known field
		if !expectedFields[key] {
			return fmt.Errorf("%sunknown field: %s", context, key)
		}
	}

	return nil
}
</file>

<file path="internal/logging/logging.go">
package logging

import (
	"fmt"
	"io"
	"os"
	"path/filepath"
	"sync"
)

type Logger struct {
	writer io.Writer
	Level  LogLevel
	mutex  sync.Mutex
}

type LogLevel int

const (
	DEBUG LogLevel = iota
	INFO
	WARN
	ERROR
	FATAL
)

func NewLogger(level LogLevel) (*Logger, error) {
	writer := os.Stdout
	return &Logger{writer: writer, Level: level}, nil
}

func (l *Logger) Debug(msg string) {
	if l.Level <= DEBUG {
		l.mutex.Lock()
		defer l.mutex.Unlock()
		fmt.Fprintf(l.writer, "[DEBUG] %s\n", msg)
	}
}
func (l *Logger) Info(msg string) {
	if l.Level <= INFO {
		l.mutex.Lock()
		defer l.mutex.Unlock()
		fmt.Fprintf(l.writer, "[INFO] %s\n", msg)
	}
}
func (l *Logger) Warn(msg string, err ...error) {
	if l.Level <= WARN {
		l.mutex.Lock()
		defer l.mutex.Unlock()
		if len(err) > 0 && err[0] != nil {
			fmt.Fprintf(l.writer, "[WARN] %s: %v\n", msg, err[0])
		} else {
			fmt.Fprintf(l.writer, "[WARN] %s\n", msg)
		}
	}
}
func (l *Logger) Error(msg string, err ...error) {
	if l.Level <= ERROR {
		l.mutex.Lock()
		defer l.mutex.Unlock()
		if len(err) > 0 && err[0] != nil {
			fmt.Fprintf(l.writer, "[ERROR] %s: %v\n", msg, err[0])
		} else {
			fmt.Fprintf(l.writer, "[ERROR] %s\n", msg)
		}
	}
}
func (l *Logger) Fatal(msg string, err ...error) {
	l.mutex.Lock()
	defer l.mutex.Unlock()
	if len(err) > 0 && err[0] != nil {
		fmt.Fprintf(l.writer, "[FATAL] %s: %v\n", msg, err[0])
	} else {
		fmt.Fprintf(l.writer, "[FATAL] %s\n", msg)
	}
	if f, ok := l.writer.(*os.File); ok {
		f.Sync()
		f.Close()
	}
	os.Exit(1)
}
func (l *Logger) SetDeploymentIDFileWriter(logsPath, deploymentID string) error {
	l.mutex.Lock()
	defer l.mutex.Unlock()
	if deploymentID == "" {
		return fmt.Errorf("deployment ID cannot be empty")
	}
	logFilePath := filepath.Join(logsPath, deploymentID+".log")
	file, err := os.OpenFile(logFilePath, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		return err
	}
	l.writer = file
	return nil
}

func (l *Logger) CloseLog() error {
	l.mutex.Lock()
	defer l.mutex.Unlock()
	if l.writer != os.Stdout {
		fmt.Fprintln(l.writer, "[LOG END]")
		if f, ok := l.writer.(*os.File); ok {
			return f.Close()
		}
	}
	return nil
}
</file>

<file path="internal/cli/commands/root.go">
package commands

import (
	"github.com/spf13/cobra"
)

// NewRootCmd creates the root command
func NewRootCmd() *cobra.Command {
	cmd := &cobra.Command{
		Use:           "haloy",
		Short:         "haloy builds and runs Docker containers based on a YAML config",
		SilenceErrors: true, // Don't print errors automatically
		SilenceUsage:  true, // Don't show usage on error
	}

	// Add all subcommands
	cmd.AddCommand(
		CompletionCmd(),
		DeployAppCmd(),
		DeployAllCmd(),
		InitCmd(),
		ListAppsCmd(),
		RollbackAppCmd(),
		StartCmd(),
		StopAppCmd(),
		StatusAppCmd(),
		ValidateCmd(),
		VersionCmd(),
		SecretsCommand(),
	)

	return cmd
}
</file>

<file path="internal/config/config.go">
package config

import (
	"fmt"
	"os"
	"reflect"

	"gopkg.in/yaml.v3"
)

const (
	DockerNetwork = "haloy-public"
	DefaultMaxContainersToKeep = 3
	DefaultHealthCheckPath = "/"
	DefaultContainerPort = "80"
	DefaultReplicas = 1
	ConfigFileName = "apps.yml"
	HAProxyConfigFileName = "haproxy.cfg"
)

type Config struct {
	Apps []AppConfig `yaml:"apps"`
}

func (c *Config) UnmarshalYAML(value *yaml.Node) error {
	// Get expected field names from struct tags
	expectedFields := ExtractYAMLFieldNames(reflect.TypeOf(*c))

	// Check for unknown fields
	if err := CheckUnknownFields(value, expectedFields, ""); err != nil {
		return err
	}

	// Use type alias to avoid infinite recursion
	type ConfigAlias Config
	var alias ConfigAlias

	// Unmarshal to the alias type
	if err := value.Decode(&alias); err != nil {
		return err
	}

	// Copy data back to original struct
	*c = Config(alias)

	return nil
}

type AppConfig struct {
	Name      string   `yaml:"name"`
	Source    Source   `yaml:"source"`
	Domains   []Domain `yaml:"domains"`
	ACMEEmail string   `yaml:"acmeEmail"`
	Env       []EnvVar `yaml:"env,omitempty"`
	// Using pointer to allow nil value
	MaxContainersToKeep *int     `yaml:"maxContainersToKeep,omitempty"`
	Volumes             []string `yaml:"volumes,omitempty"`
	HealthCheckPath     string   `yaml:"healthCheckPath,omitempty"`
	Port                string   `yaml:"port,omitempty"`
	Replicas            *int     `yaml:"replicas,omitempty"`
}

func (a *AppConfig) UnmarshalYAML(value *yaml.Node) error {
	// Get expected field names
	expectedFields := ExtractYAMLFieldNames(reflect.TypeOf(*a))

	// Find the app name for better error messages
	var appName string
	for i := 0; i < len(value.Content); i += 2 {
		if i+1 >= len(value.Content) {
			continue
		}
		if value.Content[i].Value == "name" {
			appName = value.Content[i+1].Value
			break
		}
	}

	// Set default context
	context := "app: "
	if appName != "" {
		context = fmt.Sprintf("app '%s': ", appName)
	}

	// Check for unknown fields
	if err := CheckUnknownFields(value, expectedFields, context); err != nil {
		return err
	}

	// Use type alias to avoid infinite recursion
	type AppConfigAlias AppConfig
	var alias AppConfigAlias

	// Unmarshal to the alias type
	if err := value.Decode(&alias); err != nil {
		return err
	}

	// Copy data back to original struct
	*a = AppConfig(alias)

	return nil
}

type Domain struct {
	Canonical string   `yaml:"domain"`
	Aliases   []string `yaml:"aliases,omitempty"`
}

func (d *Domain) ToSlice() []string {
	return append([]string{d.Canonical}, d.Aliases...)
}
func (d *Domain) UnmarshalYAML(value *yaml.Node) error {
	// If the YAML node is a scalar, treat it as a simple canonical domain.
	if value.Kind == yaml.ScalarNode {
		d.Canonical = value.Value
		d.Aliases = []string{}
		return nil
	}

	// If the node is a mapping, check for unknown fields
	if value.Kind == yaml.MappingNode {
		expectedFields := ExtractYAMLFieldNames(reflect.TypeOf(*d))

		if err := CheckUnknownFields(value, expectedFields, "domain: "); err != nil {
			return err
		}

		// Use type alias to avoid infinite recursion
		type DomainAlias Domain
		var alias DomainAlias

		// Unmarshal to the alias type
		if err := value.Decode(&alias); err != nil {
			return err
		}

		// Copy data back to original struct
		*d = Domain(alias)

		// Ensure Aliases is not nil
		if d.Aliases == nil {
			d.Aliases = []string{}
		}

		return nil
	}

	return fmt.Errorf("unexpected YAML node kind %d for Domain", value.Kind)
}

// NormalizeConfig sets default values for the loaded configuration.
func NormalizeConfig(conf *Config) *Config {
	normalized := *conf
	normalized.Apps = make([]AppConfig, len(conf.Apps))
	for i, app := range conf.Apps {
		normalized.Apps[i] = app

		// Default MaxContainersToKeep to 3 if not set.
		if app.MaxContainersToKeep == nil {
			defaultMax := DefaultMaxContainersToKeep
			normalized.Apps[i].MaxContainersToKeep = &defaultMax
		}

		if app.HealthCheckPath == "" {
			normalized.Apps[i].HealthCheckPath = DefaultHealthCheckPath
		}

		if app.Port == "" {
			normalized.Apps[i].Port = DefaultContainerPort
		}

		if app.Replicas == nil {
			defaultReplicas := DefaultReplicas
			normalized.Apps[i].Replicas = &defaultReplicas
		}
	}
	return &normalized
}

func LoadConfig(path string) (*Config, error) {
	data, err := os.ReadFile(path)
	if err != nil {
		return nil, fmt.Errorf("failed to read config file '%s': %w", path, err)
	}
	var config Config
	if err := yaml.Unmarshal(data, &config); err != nil {
		return nil, fmt.Errorf("failed to unmarshal config: %w", err)
	}
	return &config, nil
}

// LoadAndValidateConfig loads the configuration from a file, normalizes it, and validates it.
func LoadAndValidateConfig(path string) (*Config, error) {
	config, err := LoadConfig(path)
	if err != nil {
		return nil, err
	}
	normalizedConfig := NormalizeConfig(config)

	if err := normalizedConfig.Validate(); err != nil {
		return nil, err
	}
	return normalizedConfig, nil
}
</file>

<file path="internal/manager/deployments.go">
package manager

import (
	"context"
	"fmt"
	"sync"

	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/docker"
	"github.com/ameistad/haloy/internal/logging"
	"github.com/docker/docker/api/types/container"
	"github.com/docker/docker/api/types/filters"
	"github.com/docker/docker/client"
)

type DeploymentInstance struct {
	ContainerID string
	IP          string
	Port        string
}

type Deployment struct {
	Labels    *config.ContainerLabels
	Instances []DeploymentInstance
}

type DeploymentManager struct {
	Context      context.Context
	DockerClient *client.Client
	// deployments is a map of appName to Deployment, key is the app name.
	deployments      map[string]Deployment
	logger           *logging.Logger
	compareResult    compareResult
	deploymentsMutex sync.RWMutex
}

func NewDeploymentManager(ctx context.Context, dockerClient *client.Client, logger *logging.Logger) *DeploymentManager {
	return &DeploymentManager{
		Context:      ctx,
		DockerClient: dockerClient,
		deployments:  make(map[string]Deployment),
		logger:       logger,
	}
}

// BuildDeployments scans all running Docker containers with the app label and builds a map of
// current deployments in the system. It compares the new deployment state with the previous state
// to determine if any changes have occurred (additions, removals, or updates to deployments).
// Returns true if the deployment state has changed, along with any error encountered.
func (dm *DeploymentManager) BuildDeployments(ctx context.Context) (bool, error) {
	newDeployments := make(map[string]Deployment)

	// Filter for containers with the app label
	filtersArgs := filters.NewArgs()
	filtersArgs.Add("label", fmt.Sprintf("%s=%s", config.LabelRole, config.AppLabelRole))
	containers, err := dm.DockerClient.ContainerList(ctx, container.ListOptions{
		Filters: filtersArgs,
		All:     false, // Only running containers
	})
	if err != nil {
		return false, fmt.Errorf("failed to get containers: %w", err)
	}

	for _, containerSummary := range containers {
		container, err := dm.DockerClient.ContainerInspect(ctx, containerSummary.ID)
		if err != nil {
			dm.logger.Error(fmt.Sprintf("Failed to inspect container %s", containerSummary.ID), err)
			continue
		}

		if !IsAppContainer(container) {
			dm.logger.Info(fmt.Sprintf("Container %s is not eligible for haloy management", containerSummary.ID))
			continue
		}

		labels, err := config.ParseContainerLabels(container.Config.Labels)
		if err != nil {
			dm.logger.Error(fmt.Sprintf("Error parsing labels for container %s", containerSummary.ID), err)
			continue
		}

		ip, err := docker.ContainerNetworkIP(container, config.DockerNetwork)
		if err != nil {
			dm.logger.Error(fmt.Sprintf("Error getting IP for container %s", container.ID), err)
			continue
		}

		var port string
		if labels.Port != "" {
			port = labels.Port
		} else {
			port = config.DefaultContainerPort
		}

		instance := DeploymentInstance{ContainerID: container.ID, IP: ip, Port: port}

		if deployment, exists := newDeployments[labels.AppName]; exists {
			// There is a appName match, check if the deployment ID matches.
			if deployment.Labels.DeploymentID == labels.DeploymentID {
				deployment.Instances = append(deployment.Instances, instance)
				newDeployments[labels.AppName] = deployment
			} else {
				// Replace the deployment if the new one has a higher deployment ID
				if deployment.Labels.DeploymentID < labels.DeploymentID {
					newDeployments[labels.AppName] = Deployment{Labels: labels, Instances: []DeploymentInstance{instance}}
				}
			}
		} else {
			newDeployments[labels.AppName] = Deployment{Labels: labels, Instances: []DeploymentInstance{instance}}
		}
	}

	dm.deploymentsMutex.Lock()
	defer dm.deploymentsMutex.Unlock()

	oldDeployments := dm.deployments
	dm.deployments = newDeployments

	compareResult := compareDeployments(oldDeployments, newDeployments)
	hasChanged := len(compareResult.AddedDeployments) > 0 ||
		len(compareResult.RemovedDeployments) > 0 ||
		len(compareResult.UpdatedDeployments) > 0

	dm.compareResult = compareResult
	return hasChanged, nil
}

func (dm *DeploymentManager) HealthCheckNewContainers() error {
	deploymentsToCheck := []Deployment{}

	for _, deployment := range dm.compareResult.AddedDeployments {
		deploymentsToCheck = append(deploymentsToCheck, deployment)
	}

	for _, deployment := range dm.compareResult.UpdatedDeployments {
		deploymentsToCheck = append(deploymentsToCheck, deployment)
	}

	for _, deployment := range deploymentsToCheck {
		for _, instance := range deployment.Instances {
			if err := docker.HealthCheckContainer(dm.Context, dm.DockerClient, instance.ContainerID); err != nil {
				return fmt.Errorf("health check failed for container %s: %w", instance.ContainerID, err)
			}
		}
	}
	return nil
}

func (dm *DeploymentManager) Deployments() map[string]Deployment {
	dm.deploymentsMutex.RLock()
	defer dm.deploymentsMutex.RUnlock()

	// Return a copy to prevent external modification after unlock
	deploymentsCopy := make(map[string]Deployment, len(dm.deployments))
	for appName, deployment := range dm.deployments {
		deploymentsCopy[appName] = deployment
	}
	return deploymentsCopy
}

// GetCertificateDomains collects all canonical domains and their aliases for certificate management.
func (dm *DeploymentManager) GetCertificateDomains() []CertificatesDomain {
	dm.deploymentsMutex.RLock()
	defer dm.deploymentsMutex.RUnlock()

	managedDomains := make([]CertificatesDomain, 0, len(dm.deployments)) // Pre-allocate roughly

	for _, deployment := range dm.deployments {
		if deployment.Labels == nil {
			continue // Skip if labels somehow nil
		}
		for _, domain := range deployment.Labels.Domains {
			// Only process if canonical domain is set and not empty
			if domain.Canonical != "" {
				// Ensure Aliases slice is not nil before passing
				aliases := domain.Aliases
				if aliases == nil {
					aliases = []string{}
				}
				managedDomains = append(managedDomains, CertificatesDomain{
					Canonical: domain.Canonical,
					Aliases:   aliases, // Include aliases
					Email:     deployment.Labels.ACMEEmail,
				})
			}
		}
	}
	return managedDomains
}

type compareResult struct {
	UpdatedDeployments map[string]Deployment
	RemovedDeployments map[string]Deployment
	AddedDeployments   map[string]Deployment
}

// compareDeployments analyzes differences between the previous and current deployment states.
// It identifies three types of changes:
// 1. Updated deployments - same app name but different deployment ID or instance configuration
// 2. Removed deployments - deployments that existed before but are no longer present
// 3. Added deployments - new deployments that didn't exist in the previous state
// This comparison is critical for determining when HAProxy configuration should be updated.
func compareDeployments(oldDeployments, newDeployments map[string]Deployment) compareResult {

	updatedDeployments := make(map[string]Deployment)
	removedDeployments := make(map[string]Deployment)
	addedDeployments := make(map[string]Deployment)

	// Find removed and updated deployments by comparing previous to current
	for appName, prevDeployment := range oldDeployments {
		// Check if this deployment still exists
		if currentDeployment, exists := newDeployments[appName]; exists {
			// Deployment exists - check if it's been updated
			if prevDeployment.Labels.DeploymentID != currentDeployment.Labels.DeploymentID {
				// DeploymentID changed - it's an update
				updatedDeployments[appName] = currentDeployment
			} else {
				// Check if instances changed (added or removed instances)
				if !instancesEqual(prevDeployment.Instances, currentDeployment.Instances) {
					updatedDeployments[appName] = currentDeployment
				}
			}
		} else {
			// Deployment no longer exists - it was removed
			removedDeployments[appName] = prevDeployment
		}
	}

	// Find added deployments by comparing current to previous
	for appName, currentDeployment := range newDeployments {
		if _, exists := oldDeployments[appName]; !exists {
			// This is a new deployment
			addedDeployments[appName] = currentDeployment
		}
	}

	result := compareResult{
		UpdatedDeployments: updatedDeployments,
		RemovedDeployments: removedDeployments,
		AddedDeployments:   addedDeployments,
	}

	return result
}

// Helper function to check if two instance lists are equal
func instancesEqual(a, b []DeploymentInstance) bool {
	if len(a) != len(b) {
		return false
	}

	// Create maps of container IDs for easy comparison
	mapA := make(map[string]bool)
	for _, instance := range a {
		mapA[instance.ContainerID] = true
	}

	// Check if all instances in b exist in a
	for _, instance := range b {
		if !mapA[instance.ContainerID] {
			return false
		}
	}

	return true
}
</file>

<file path="go.mod">
module github.com/ameistad/haloy

go 1.24

require (
	filippo.io/age v1.2.1
	github.com/charmbracelet/lipgloss v1.1.0
	github.com/docker/docker v28.0.4+incompatible
	github.com/fatih/color v1.18.0
	github.com/go-acme/lego/v4 v4.22.2
	github.com/olekukonko/tablewriter v0.0.5
	github.com/pterm/pterm v0.12.80
	github.com/spf13/cobra v1.9.1
	gopkg.in/yaml.v3 v3.0.1
)

require (
	atomicgo.dev/cursor v0.2.0 // indirect
	atomicgo.dev/keyboard v0.2.9 // indirect
	atomicgo.dev/schedule v0.1.0 // indirect
	github.com/AdaLogics/go-fuzz-headers v0.0.0-20240806141605-e8a1dd7889d6 // indirect
	github.com/Azure/go-ansiterm v0.0.0-20210617225240-d185dfc1b5a1 // indirect
	github.com/Microsoft/go-winio v0.6.2 // indirect
	github.com/aymanbagabas/go-osc52/v2 v2.0.1 // indirect
	github.com/cenkalti/backoff/v4 v4.3.0 // indirect
	github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc // indirect
	github.com/charmbracelet/x/ansi v0.8.0 // indirect
	github.com/charmbracelet/x/cellbuf v0.0.13-0.20250311204145-2c3ea96c31dd // indirect
	github.com/charmbracelet/x/term v0.2.1 // indirect
	github.com/containerd/console v1.0.3 // indirect
	github.com/containerd/log v0.1.0 // indirect
	github.com/distribution/reference v0.6.0 // indirect
	github.com/docker/go-connections v0.5.0 // indirect
	github.com/docker/go-units v0.5.0 // indirect
	github.com/felixge/httpsnoop v1.0.4 // indirect
	github.com/go-jose/go-jose/v4 v4.0.5 // indirect
	github.com/go-logr/logr v1.4.2 // indirect
	github.com/go-logr/stdr v1.2.2 // indirect
	github.com/gogo/protobuf v1.3.2 // indirect
	github.com/gookit/color v1.5.4 // indirect
	github.com/inconshreveable/mousetrap v1.1.0 // indirect
	github.com/klauspost/compress v1.18.0 // indirect
	github.com/lithammer/fuzzysearch v1.1.8 // indirect
	github.com/lucasb-eyer/go-colorful v1.2.0 // indirect
	github.com/mattn/go-colorable v0.1.14 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/mattn/go-runewidth v0.0.16 // indirect
	github.com/miekg/dns v1.1.64 // indirect
	github.com/moby/docker-image-spec v1.3.1 // indirect
	github.com/moby/patternmatcher v0.6.0 // indirect
	github.com/moby/sys/sequential v0.6.0 // indirect
	github.com/moby/sys/user v0.3.0 // indirect
	github.com/moby/sys/userns v0.1.0 // indirect
	github.com/moby/term v0.5.0 // indirect
	github.com/morikuni/aec v1.0.0 // indirect
	github.com/muesli/termenv v0.16.0 // indirect
	github.com/opencontainers/go-digest v1.0.0 // indirect
	github.com/opencontainers/image-spec v1.1.1 // indirect
	github.com/pkg/errors v0.9.1 // indirect
	github.com/rivo/uniseg v0.4.7 // indirect
	github.com/sirupsen/logrus v1.9.3 // indirect
	github.com/spf13/pflag v1.0.6 // indirect
	github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e // indirect
	go.opentelemetry.io/auto/sdk v1.1.0 // indirect
	go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.60.0 // indirect
	go.opentelemetry.io/otel v1.35.0 // indirect
	go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp v1.35.0 // indirect
	go.opentelemetry.io/otel/metric v1.35.0 // indirect
	go.opentelemetry.io/otel/trace v1.35.0 // indirect
	golang.org/x/crypto v0.36.0 // indirect
	golang.org/x/mod v0.24.0 // indirect
	golang.org/x/net v0.37.0 // indirect
	golang.org/x/sync v0.12.0 // indirect
	golang.org/x/sys v0.31.0 // indirect
	golang.org/x/term v0.30.0 // indirect
	golang.org/x/text v0.23.0 // indirect
	golang.org/x/tools v0.31.0 // indirect
	gotest.tools/v3 v3.5.1 // indirect
)
</file>

<file path="internal/cli/commands/status.go">
package commands

import (
	"context"
	"fmt"
	"strings"
	"time"

	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/docker"
	"github.com/ameistad/haloy/internal/helpers"
	"github.com/ameistad/haloy/internal/ui"
	"github.com/charmbracelet/lipgloss"
	"github.com/docker/docker/api/types/container"
	"github.com/docker/docker/api/types/filters"
	"github.com/docker/docker/client"
	"github.com/spf13/cobra"
)

func StatusAppCmd() *cobra.Command {
	statusAppCmd := &cobra.Command{
		Use:   "status [app-name]",
		Short: "Show status for all apps or detailed status for a specific app",
		Long: `Show status for all applications if no app name is provided.
If an app name is given, show detailed status including DNS configuration.`,
		Args: cobra.MaximumNArgs(1),
		Run: func(cmd *cobra.Command, args []string) {
			context, cancel := context.WithTimeout(context.Background(), showStatusTimeout)
			defer cancel()
			dockerClient, err := docker.NewClient(context)
			if err != nil {
				ui.Error("Error: %s", err)
				return
			}
			defer dockerClient.Close()

			if len(args) == 0 {
				// Show status for all apps
				configFilePath, err := config.ConfigFilePath()
				if err != nil {
					ui.Error("Error: %s", err)
					return
				}
				configFile, err := config.LoadAndValidateConfig(configFilePath)
				if err != nil {
					ui.Error("Error: %s", err)
					return
				}
				for i := range configFile.Apps {
					if err := showAppStatus(dockerClient, context, &configFile.Apps[i]); err != nil {
						ui.Error("Error: %s", err)
					}
				}
				return
			}

			appName := args[0]
			appConfig, err := config.AppConfigByName(appName)
			if err != nil {
				ui.Error("Error: %s", err)
				return
			}

			if err := showAppStatusDetailed(dockerClient, context, appConfig); err != nil {
				ui.Error("Error: %s", err)
			}

		},
	}
	return statusAppCmd
}

const (
	showStatusTimeout = 5 * time.Second
)

type initialStatus struct {
	state            string
	containerIDs     []string
	domains          []config.Domain
	formattedDomains []string
	envVars          []config.EnvVar
	formattedEnvVars []string
}

func getInitialStatus(dockerClient *client.Client, context context.Context, appConfig *config.AppConfig) (initialStatus, error) {
	status := initialStatus{}
	filtersArgs := filters.NewArgs()
	filtersArgs.Add("label", fmt.Sprintf("%s=%s", config.LabelRole, config.AppLabelRole))
	filtersArgs.Add("label", fmt.Sprintf("%s=%s", config.LabelAppName, appConfig.Name))

	containers, err := dockerClient.ContainerList(context, container.ListOptions{
		Filters: filtersArgs,
		All:     false,
	})
	if err != nil {
		return status, fmt.Errorf("failed to get container for app %s: %w", appConfig.Name, err)
	}

	state := "Not running"
	containerIDs := make([]string, len(containers))
	if len(containers) > 0 {
		for i, c := range containers {
			if c.State == "running" || c.State == "restarting" {
				containerIDs[i] = helpers.SafeIDPrefix(c.ID)
			}
		}

		for _, c := range containers {
			switch c.State {
			case "running":
				state = "Running"
			case "restarting":
				state = "Restarting"
			case "exited":
				state = "Exited"
			}
		}
	}

	formattedDomains := make([]string, 0, len(appConfig.Domains))
	for _, d := range appConfig.Domains {
		if len(d.Aliases) == 0 {
			formattedDomains = append(formattedDomains, fmt.Sprintf("  %s", d.Canonical))
		} else {
			aliases := strings.Join(d.Aliases, ", ")
			aliasStyle := lipgloss.NewStyle().Foreground(ui.LightGray).Italic(true)
			styledAliases := aliasStyle.Render(fmt.Sprintf("<- %s", aliases))
			formattedDomains = append(formattedDomains, fmt.Sprintf("  %s %s", d.Canonical, styledAliases))
		}
	}

	// Build environment variables output.
	var formattedEnvVars []string
	for _, ev := range appConfig.Env {
		var val string
		if ev.Value != nil {
			val = *ev.Value
		} else if ev.SecretName != nil {
			// If there's a secret reference, simulate the "SECRET:" prefix.
			val = "SECRET:" + *ev.SecretName
		}
		if strings.HasPrefix(val, "SECRET:") {
			secretName := strings.TrimPrefix(val, "SECRET:")
			formattedEnvVars = append(formattedEnvVars, fmt.Sprintf("  %s: loaded from secret (%s)", ev.Name, secretName))
		} else {
			formattedEnvVars = append(formattedEnvVars, fmt.Sprintf("  %s: %s", ev.Name, val))
		}
	}

	status = initialStatus{
		state:            state,
		containerIDs:     containerIDs,
		domains:          appConfig.Domains,
		formattedDomains: formattedDomains,
		envVars:          appConfig.Env,
		formattedEnvVars: formattedEnvVars,
	}

	return status, nil
}

func showAppStatusDetailed(dockerClient *client.Client, context context.Context, appConfig *config.AppConfig) error {

	status, err := getInitialStatus(dockerClient, context, appConfig)
	if err != nil {
		return err
	}

	output := []string{
		fmt.Sprintf("State: %s", status.state),
		fmt.Sprintf("Domains:\n%s", strings.Join(status.formattedDomains, "\n")),
		fmt.Sprintf("Container IDs: %s", strings.Join(status.containerIDs, ", ")),
	}

	if len(status.formattedEnvVars) > 0 {
		output = append(output, fmt.Sprintf("Environment Variables:\n%s", strings.Join(status.formattedEnvVars, "\n")))
	}
	// Create section
	ui.Section(fmt.Sprintf("%s (detailed status)", appConfig.Name), output)

	return nil
}

func showAppStatus(dockerClient *client.Client, context context.Context, appConfig *config.AppConfig) error {
	status, err := getInitialStatus(dockerClient, context, appConfig)
	if err != nil {
		return err
	}

	output := []string{
		fmt.Sprintf("State: %s", status.state),
		fmt.Sprintf("Domains:\n%s", strings.Join(status.formattedDomains, "\n")),
		fmt.Sprintf("Container IDs: %s", strings.Join(status.containerIDs, ", ")),
	}

	if len(status.formattedEnvVars) > 0 {
		output = append(output, fmt.Sprintf("Environment Variables:\n%s", strings.Join(status.formattedEnvVars, "\n")))
	}
	// Create section
	ui.Section(appConfig.Name, output)

	return nil
}
</file>

<file path="internal/manager/updater.go">
package manager

import (
	"context"
	"fmt"

	"github.com/ameistad/haloy/internal/logging"
)

type Updater struct {
	deploymentManager *DeploymentManager
	certManager       *CertificatesManager
	haproxyManager    *HAProxyManager
}

type UpdaterConfig struct {
	DeploymentManager *DeploymentManager
	CertManager       *CertificatesManager
	HAProxyManager    *HAProxyManager
}

func NewUpdater(config UpdaterConfig) *Updater {
	return &Updater{
		deploymentManager: config.DeploymentManager,
		certManager:       config.CertManager,
		haproxyManager:    config.HAProxyManager,
	}
}

func (u *Updater) Update(ctx context.Context, reason string, logger *logging.Logger) error {

	logger.Debug(fmt.Sprintf("Updater: Starting update process for reason: %s", reason))

	// Build Deployments and check if anything has changed (Thread-safe)
	deploymentsHasChanged, err := u.deploymentManager.BuildDeployments(ctx)
	if err != nil {
		return fmt.Errorf("updater: failed to build deployments (%s): %w", reason, err)
	}
	if !deploymentsHasChanged {
		logger.Debug(fmt.Sprintf("Updater: No changes detected in deployments for reason: %s", reason))
		return nil // Nothing changed, successful exit
	}

	if err := u.deploymentManager.HealthCheckNewContainers(); err != nil {
		return fmt.Errorf("deployment aborted: failed to perform health check on new containers (%s): %w", reason, err)
	} else {
		logger.Info("Health check completed successfully")
	}

	logger.Debug(fmt.Sprintf("Updater: Deployment changes detected for reason: %s. Triggering cert and HAProxy updates.", reason))

	// Get domains AFTER checking HasChanged to reflect the latest state
	certDomains := u.deploymentManager.GetCertificateDomains()
	u.certManager.AddDomains(certDomains, logger) // Let CertManager handle duplicates/warnings
	u.certManager.Refresh(logger)                 // Refresh is debounced internally

	// Get deployments AFTER checking HasChanged
	deployments := u.deploymentManager.Deployments() // Gets a safe copy

	// Delegate the entire HAProxy update process (lock, generate, write, signal)
	if err := u.haproxyManager.ApplyConfig(ctx, deployments); err != nil {
		return fmt.Errorf("failed to apply HAProxy config (%s): %w", reason, err)
	} else {
		logger.Info("HAProxy configuration updated successfully")
	}
	return nil
}
</file>

<file path="internal/ui/ui.go">
package ui

import (
	"fmt"

	"github.com/charmbracelet/lipgloss"
	"github.com/charmbracelet/lipgloss/table"
	"github.com/pterm/pterm"
)

// Colors
var (
	White     = lipgloss.Color("#FAFAFA")
	Gray      = lipgloss.Color("245")
	LightGray = lipgloss.Color("241")
)

// Styles
var titleStyle = lipgloss.NewStyle().
	Bold(true).
	Foreground(White)

func Success(format string, a ...any) {
	pterm.Success.Println(fmt.Sprintf(format, a...))
}
func Info(format string, a ...any) {
	pterm.Info.Println(fmt.Sprintf(format, a...))
}

func Debug(format string, a ...any) {
	pterm.Debug.Println(fmt.Sprintf(format, a...))
}

func Warn(format string, a ...any) {
	pterm.Warning.Println(fmt.Sprintf(format, a...))
}

func Error(format string, a ...any) {
	pterm.Error.Println(fmt.Sprintf(format, a...))
}

func BoldText(format string, a ...any) string {
	return pterm.Bold.Sprint(fmt.Sprintf(format, a...))
}

var lineStyle = lipgloss.NewStyle().
	Foreground(White).
	TabWidth(5)

func Section(title string, textLines []string) {

	fmt.Println(titleStyle.BorderStyle(lipgloss.NormalBorder()).BorderForeground(White).BorderBottom(true).Render(title))
	for _, line := range textLines {
		fmt.Println(lineStyle.Render(line))
	}
}

func Table(headers []string, rows [][]string) {

	cellStyle := lipgloss.NewStyle().Padding(0, 1)

	t := table.New().
		Border(lipgloss.NormalBorder()).
		BorderStyle(lipgloss.NewStyle().Foreground(Gray)).
		StyleFunc(func(row, col int) lipgloss.Style {
			switch {
			case row == table.HeaderRow:
				return titleStyle.Align(lipgloss.Center)
			case row%2 == 0:
				return cellStyle.Foreground(LightGray)
			default:
				return cellStyle.Foreground(Gray)
			}
		}).
		Headers(headers...).
		Rows(rows...)
	fmt.Println(t)
}
</file>

<file path="internal/cli/commands/init.go">
package commands

import (
	"bytes"
	"context"
	"fmt"
	"io/fs"
	"os"
	"path/filepath"
	"text/template"
	"time"

	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/docker"
	"github.com/ameistad/haloy/internal/embed"
	"github.com/ameistad/haloy/internal/helpers"
	"github.com/ameistad/haloy/internal/ui"
	"github.com/spf13/cobra"
)

const (
	initTimeout = 5 * time.Minute
)

func InitCmd() *cobra.Command {
	var skipServices bool
	var withTestApp bool
	var overrideExistingConfig bool

	cmd := &cobra.Command{
		Use:   "init",
		Short: "Initialize configuration files and prepare HAProxy for production",
		Run: func(cmd *cobra.Command, args []string) {
			configDirPath, err := config.ConfigDirPath()
			if err != nil {
				ui.Error("Failed to determine config directory: %v\n", err)
				return
			}

			// Check the status of the configuration directory
			fileInfo, statErr := os.Stat(configDirPath)
			if statErr == nil { // Directory exists
				if !fileInfo.IsDir() {
					ui.Error("Configuration path '%s' exists but is not a directory. Please remove it or use a different path.", configDirPath)
					return
				}
				// Directory exists
				if !overrideExistingConfig {
					ui.Error("Configuration directory '%s' already exists. Use --override-existing-config to overwrite.", configDirPath)
					return
				}
				// overrideExistingConfig is true, so remove the existing directory
				ui.Info("Removing existing configuration directory: %s\n", configDirPath)
				if err := os.RemoveAll(configDirPath); err != nil {
					ui.Error("Failed to remove existing config directory: %v\n", err)
					return
				}
				// Proceed to create the directory below
			} else if !os.IsNotExist(statErr) {
				// An error other than "does not exist" occurred with os.Stat
				ui.Error("Failed to check status of config directory '%s': %v\n", configDirPath, statErr)
				return
			}
			// At this point, the directory either did not exist,
			// or it existed, was a directory, and has been removed (if overrideExistingConfig was true).
			// Now, (re)create the configuration directory.
			ui.Info("Creating configuration directory: %s\n", configDirPath)
			if err := os.MkdirAll(configDirPath, 0755); err != nil {
				ui.Error("Failed to create config directory '%s': %v\n", configDirPath, err)
				return
			}

			ctx, cancel := context.WithTimeout(context.Background(), initTimeout)
			defer cancel()

			dockerClient, err := docker.NewClient(ctx)
			if err != nil {
				ui.Error("%v", err)
				return
			}
			defer dockerClient.Close()

			var emptyDirs = []string{
				"containers/cert-storage",
				"containers/cert-storage/accounts",
				"containers/haproxy-config",
			}
			if err := copyConfigFiles(configDirPath, emptyDirs); err != nil {
				ui.Error("Failed to create configuration files: %v\n", err)
				return
			}

			// Prompt the user for email and update apps.yml.
			if err := copyConfigTemplateFiles(withTestApp); err != nil {
				ui.Error("Failed to update configuration files: %v\n", err)
				return
			}

			// Ensure default Docker network exists.
			if err := docker.EnsureNetwork(dockerClient, ctx); err != nil {
				ui.Warn("Failed to ensure Docker network exists: %v\n", err)
				ui.Warn("You can manually create it with:\n")
				ui.Warn("docker network create --driver bridge %s", config.DockerNetwork)
			}

			if !skipServices {
				if _, err := docker.EnsureServicesIsRunning(dockerClient, ctx); err != nil {
					ui.Error("Failed to to start haproxy and haloy-manager: %v\n", err)
					return
				}

			}

			successMsg := fmt.Sprintf("Configuration files created successfully in %s\n", configDirPath)
			if !skipServices {
				successMsg += "HAProxy and haloy-manager started successfully.\n"
			}
			successMsg += "You can now add your applications to apps.yml and run:\n"
			successMsg += "haloy deploy <app-name>"
			ui.Success("%s", successMsg)
		},
	}

	cmd.Flags().BoolVar(&skipServices, "no-services", false, "Initialize configuration files without starting Docker services")
	cmd.Flags().BoolVar(&withTestApp, "with-test-app", false, "Add an initial test app to apps.yml")
	cmd.Flags().BoolVar(&overrideExistingConfig, "override-existing-config", false, "Override existing configuration directory if it already exists")
	return cmd
}

func copyConfigFiles(dst string, emptyDirs []string) error {
	for _, dir := range emptyDirs {
		dirPath := filepath.Join(dst, dir)
		if err := os.MkdirAll(dirPath, 0755); err != nil {
			return fmt.Errorf("failed to create empty directory %s: %w", dirPath, err)
		}
	}

	// Walk the embedded filesystem starting at the init directory.
	return fs.WalkDir(embed.InitFS, "init", func(path string, d fs.DirEntry, err error) error {
		if err != nil {
			return fmt.Errorf("error walking embedded filesystem: %w", err)
		}

		// Compute the relative path based on the init directory.
		relPath, err := filepath.Rel("init", path)
		if err != nil {
			return fmt.Errorf("failed to determine relative path: %w", err)
		}

		targetPath := filepath.Join(dst, relPath)
		if d.IsDir() {
			return os.MkdirAll(targetPath, 0755)
		}

		// Read the file from the embed FS.
		data, err := embed.InitFS.ReadFile(path)
		if err != nil {
			return fmt.Errorf("failed to read embedded file %s: %w", path, err)
		}

		// Determine the file mode - make shell scripts executable
		fileMode := fs.FileMode(0644)
		if filepath.Ext(targetPath) == ".sh" {
			fileMode = 0755
		}

		if err := os.WriteFile(targetPath, data, fileMode); err != nil {
			return fmt.Errorf("failed to write file %s: %w", targetPath, err)
		}

		return nil
	})
}

func copyConfigTemplateFiles(withTestApp bool) error {
	configDirPath, err := config.ConfigDirPath()
	if err != nil {
		return fmt.Errorf("failed to write updated config file: %w", err)
	}

	configFile := bytes.Buffer{}

	if withTestApp {
		testAppData, err := getTestAppData()
		if err != nil {
			return fmt.Errorf("failed to get test app data: %w", err)
		}
		configFileTemplateData := embed.ConfigFileWithTestAppTemplateData{
			ConfigDirPath: configDirPath,
			Domain:        testAppData.domain,
			Alias:         testAppData.alias,
			AcmeEmail:     testAppData.acmeEmail,
		}
		configFileWithTestApp, err := renderTemplate(fmt.Sprintf("templates/%s", embed.ConfigFileTemplateTest), configFileTemplateData)
		if err != nil {
			return fmt.Errorf("failed to build template for config with test app: %w", err)
		}
		configFile = configFileWithTestApp
	} else {
		data, err := embed.TemplatesFS.ReadFile(fmt.Sprintf("templates/%s", embed.ConfigFileTemplate))
		if err != nil {
			return fmt.Errorf("failed to read default apps.yml template: %w", err)
		}
		if _, err := configFile.Write(data); err != nil {
			return fmt.Errorf("failed to buffer default apps.yml: %w", err)
		}
	}

	haproxyConfigTemplateData := embed.HAProxyTemplateData{
		HTTPFrontend:            "",
		HTTPSFrontend:           "",
		HTTPSFrontendUseBackend: "",
		Backends:                "",
	}

	haproxyConfigFile, err := renderTemplate(fmt.Sprintf("templates/%s", config.HAProxyConfigFileName), haproxyConfigTemplateData)
	if err != nil {
		return fmt.Errorf("failed to build HAProxy template: %w", err)
	}

	// Get the full path to apps.yml.
	configFilePath, err := config.ConfigFilePath()
	if err != nil {
		return fmt.Errorf("failed to determine config file path: %w", err)
	}

	if err := os.WriteFile(configFilePath, configFile.Bytes(), 0644); err != nil {
		return fmt.Errorf("failed to write updated config file: %w", err)
	}

	haproxyConfigFilePath, err := config.HAProxyConfigFilePath()
	if err != nil {
		return fmt.Errorf("failed to determine HAProxy config file path: %w", err)
	}

	if err := os.WriteFile(haproxyConfigFilePath, haproxyConfigFile.Bytes(), 0644); err != nil {
		return fmt.Errorf("failed to write updated haproxy config file: %w", err)
	}

	return nil
}

func renderTemplate(templateFilePath string, templateData any) (bytes.Buffer, error) {
	var buf bytes.Buffer
	file, err := embed.TemplatesFS.ReadFile(templateFilePath)
	if err != nil {
		return buf, fmt.Errorf("failed to read embedded file: %w", err)
	}

	tmpl, err := template.New(templateFilePath).Parse(string(file))
	if err != nil {
		return buf, fmt.Errorf("failed to parse template: %w", err)
	}

	if err := tmpl.Execute(&buf, templateData); err != nil {
		return buf, fmt.Errorf("failed to execute template: %w", err)
	}
	return buf, nil
}

type TestAppData struct {
	domain    string
	alias     string
	acmeEmail string
}

func getTestAppData() (TestAppData, error) {
	data := TestAppData{}

	// Prompt for email with validation
	var email string
	for {
		fmt.Print("Enter email for TLS certificates for the test-app: ")
		if _, err := fmt.Scanln(&email); err != nil {
			if err.Error() == "unexpected newline" {
				ui.Info("Email cannot be empty")
				continue
			}
			return data, fmt.Errorf("failed to read email input: %w", err)
		}

		if !helpers.IsValidEmail(email) {
			ui.Info("Please enter a valid email address")
			continue
		}
		break
	}

	data.acmeEmail = email

	ip, err := helpers.GetExternalIP()
	if err != nil {
		return data, fmt.Errorf("failed to get external IP: %w", err)
	}

	data.domain = fmt.Sprintf("%s.nip.io", ip.String())
	data.alias = fmt.Sprintf("www.%s.nip.io", ip.String())

	return data, nil
}
</file>

<file path="internal/docker/image.go">
package docker

import (
	"context"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"io/fs"
	"os"
	"os/exec"
	"path/filepath"
	"strings"
	"time"

	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/ui"

	"github.com/docker/docker/api/types"
	"github.com/docker/docker/client"
	"github.com/docker/docker/pkg/archive"
	"github.com/docker/docker/pkg/jsonmessage"
)

type BuildImageParams struct {
	Context      context.Context
	DockerClient *client.Client
	ImageName    string
	Source       *config.DockerfileSource
	EnvVars      []config.EnvVar
}

func BuildImage(params BuildImageParams) error {

	absContext, err := filepath.Abs(params.Source.BuildContext)
	if err != nil {
		return fmt.Errorf("failed to resolve absolute path for build context '%s': %w", params.Source.BuildContext, err)
	}
	absDockerfile, err := filepath.Abs(params.Source.Path)
	if err != nil {
		return fmt.Errorf("failed to resolve absolute path for Dockerfile '%s': %w", params.Source.Path, err)
	}

	// Check if Dockerfile is within the build context.
	isDockerfileInContext := strings.HasPrefix(absDockerfile, absContext+string(filepath.Separator)) || absDockerfile == absContext

	// Calculate Dockerfile path for Docker API.
	var dockerfilePath string
	if isDockerfileInContext {
		relPath, err := filepath.Rel(absContext, absDockerfile)
		if err != nil {
			return fmt.Errorf("failed to calculate relative Dockerfile path: %w", err)
		}
		dockerfilePath = relPath
	} else {
		dockerfilePath = filepath.Base(absDockerfile)
	}

	buildOpts := types.ImageBuildOptions{
		Tags:       []string{params.ImageName},
		Dockerfile: dockerfilePath,
		BuildArgs:  make(map[string]*string),
		Remove:     true,
		Version:    types.BuilderBuildKit,
		// Uncomment this line to disable cache when testing
		// NoCache: true,
	}

	// Add build args from params.Source.
	if len(params.Source.BuildArgs) > 0 {
		for k, v := range params.Source.BuildArgs {
			value := v
			buildOpts.BuildArgs[k] = &value
		}
	}

	// Add environment variables to build args to make them available during build.
	if len(params.EnvVars) > 0 {
		decryptedEnvVars, err := config.DecryptEnvVars(params.EnvVars)
		if err != nil {
			return fmt.Errorf("failed to decrypt environment variables: %w", err)
		}
		for _, envVar := range decryptedEnvVars {
			strValue, err := envVar.GetValue()
			if err != nil {
				return fmt.Errorf("failed to get value for environment variable '%s': %w", envVar.Name, err)
			}
			value := strValue
			buildOpts.BuildArgs[envVar.Name] = &value
		}
	}

	// Get ignore patterns from the original build context.
	ignorePatterns := getDockerIgnorePatterns(absContext)

	var buildContextTar io.ReadCloser
	var cleanupFunc func()

	if isDockerfileInContext {
		buildContextTar, err = archive.TarWithOptions(absContext, &archive.TarOptions{
			ExcludePatterns: ignorePatterns,
		})
		if err != nil {
			return fmt.Errorf("failed to create build context archive from '%s': %w", absContext, err)
		}
	} else {
		tmpDir, err := os.MkdirTemp("", "haloy-docker-build-")
		if err != nil {
			return fmt.Errorf("failed to create temporary build context directory: %w", err)
		}
		cleanupFunc = func() { _ = os.RemoveAll(tmpDir) }

		if err := copyDir(absContext, tmpDir); err != nil {
			cleanupFunc()
			return fmt.Errorf("failed to copy build context from '%s' to '%s': %w", absContext, tmpDir, err)
		}

		dockerfileBaseName := filepath.Base(absDockerfile)
		tmpDockerfilePath := filepath.Join(tmpDir, dockerfileBaseName)
		if err := copyFile(absDockerfile, tmpDockerfilePath); err != nil {
			cleanupFunc()
			return fmt.Errorf("failed to copy Dockerfile from '%s' to '%s': %w", absDockerfile, tmpDockerfilePath, err)
		}

		buildContextTar, err = archive.TarWithOptions(tmpDir, &archive.TarOptions{
			ExcludePatterns: ignorePatterns,
		})
		if err != nil {
			cleanupFunc()
			return fmt.Errorf("failed to create temporary build context archive from '%s': %w", tmpDir, err)
		}
	}
	defer func() {
		if buildContextTar != nil {
			buildContextTar.Close()
		}
		if cleanupFunc != nil {
			cleanupFunc()
		}
	}()

	// Check if image already exists (suggesting cache might be available)
	_, err = params.DockerClient.ImageInspect(params.Context, params.ImageName)
	cacheExists := err == nil

	startMsg := "Starting Docker build..."
	if !cacheExists {
		startMsg += " (this may take a while for first build)"
	}
	ui.Info("%s", startMsg)

	// Periodic progress messages.
	done := make(chan bool)
	go func() {
		ticker := time.NewTicker(15 * time.Second)
		defer ticker.Stop()
		for {
			select {
			case <-ticker.C:
				ui.Info("Build still in progress...")
			case <-done:
				return
			}
		}
	}()

	resp, err := params.DockerClient.ImageBuild(params.Context, buildContextTar, buildOpts)
	if err != nil {
		if errors.Is(params.Context.Err(), context.Canceled) {
			return fmt.Errorf("image build cancelled: %w", params.Context.Err())
		}
		if errors.Is(params.Context.Err(), context.DeadlineExceeded) {
			return fmt.Errorf("image build timed out: %w", params.Context.Err())
		}
		return fmt.Errorf("failed to initiate image build for '%s': %w", params.ImageName, err)
	}
	defer resp.Body.Close()

	decoder := json.NewDecoder(resp.Body)
	var lastError error

	for {
		var jsonMessage jsonmessage.JSONMessage
		if err := decoder.Decode(&jsonMessage); err != nil {
			if err == io.EOF {
				break // End of stream
			}
			close(done) // Close on decode error
			return fmt.Errorf("failed to decode docker build output: %w", err)
		}

		// Process build output
		if jsonMessage.Stream != "" {
			fmt.Print(jsonMessage.Stream)
		} else if jsonMessage.Status != "" {
			// Only print status lines that aren't download/extract progress
			if !strings.Contains(jsonMessage.Status, "Downloading") &&
				!strings.Contains(jsonMessage.Status, "Extracting") {
				fmt.Printf("%s\n", jsonMessage.Status)
			}
		} else if jsonMessage.ErrorMessage != "" {
			lastError = fmt.Errorf("%s", jsonMessage.ErrorMessage)
			fmt.Fprintf(os.Stderr, "ERROR: %s\n", jsonMessage.ErrorMessage)
		}
	}

	// If there was an error in the build, return it
	if lastError != nil {
		close(done) // Close on build error
		return fmt.Errorf("build failed: %w", lastError)
	}
	close(done)
	ui.Success("Build completed successfully!")
	return nil
}

// getDockerIgnorePatterns reads the .dockerignore file and returns a slice of patterns.
func getDockerIgnorePatterns(contextDir string) []string {
	dockerIgnorePath := filepath.Join(contextDir, ".dockerignore")
	patterns := []string{}
	data, err := os.ReadFile(dockerIgnorePath)
	if err != nil {
		if !errors.Is(err, fs.ErrNotExist) {
			ui.Warn("Error reading .dockerignore file at '%s': %v", dockerIgnorePath, err)
		}
		return patterns
	}
	for _, line := range strings.Split(string(data), "\n") {
		line = strings.TrimSpace(line)
		if line != "" && !strings.HasPrefix(line, "#") {
			patterns = append(patterns, line)
		}
	}
	return patterns
}

// copyDir recursively copies the directory tree from src to dst.
func copyDir(src, dst string) error {
	srcInfo, err := os.Stat(src)
	if err != nil {
		return fmt.Errorf("failed to stat source directory '%s': %w", src, err)
	}
	if !srcInfo.IsDir() {
		return fmt.Errorf("source '%s' is not a directory", src)
	}
	if err := os.MkdirAll(dst, srcInfo.Mode()); err != nil {
		return fmt.Errorf("failed to create destination directory '%s': %w", dst, err)
	}
	return filepath.WalkDir(src, func(path string, d fs.DirEntry, err error) error {
		if err != nil {
			return fmt.Errorf("error walking directory at '%s': %w", path, err)
		}
		relPath, err := filepath.Rel(src, path)
		if err != nil {
			return fmt.Errorf("failed to calculate relative path for '%s' from base '%s': %w", path, src, err)
		}
		targetPath := filepath.Join(dst, relPath)
		if d.IsDir() {
			info, statErr := os.Stat(path)
			if statErr != nil {
				return fmt.Errorf("failed to stat source directory '%s': %w", path, statErr)
			}
			if err := os.MkdirAll(targetPath, info.Mode()); err != nil {
				return fmt.Errorf("failed to create directory '%s': %w", targetPath, err)
			}
		} else {
			if err := copyFile(path, targetPath); err != nil {
				return err
			}
		}
		return nil
	})
}

// copyFile copies a single file from src to dst, preserving permissions.
func copyFile(src, dst string) error {
	srcFile, err := os.Open(src)
	if err != nil {
		return fmt.Errorf("failed to open source file '%s': %w", src, err)
	}
	defer srcFile.Close()
	info, err := os.Stat(src)
	if err != nil {
		return fmt.Errorf("failed to stat source file '%s': %w", src, err)
	}
	dstFile, err := os.Create(dst)
	if err != nil {
		return fmt.Errorf("failed to create destination file '%s': %w", dst, err)
	}
	_, err = io.Copy(dstFile, srcFile)
	if err != nil {
		dstFile.Close()
		return fmt.Errorf("failed to copy content from '%s' to '%s': %w", src, dst, err)
	}
	if err := dstFile.Close(); err != nil {
		return fmt.Errorf("failed to close destination file '%s': %w", dst, err)
	}
	if err := os.Chmod(dst, info.Mode()); err != nil {
		ui.Warn("Failed to set permissions on '%s' (mode %s): %v", dst, info.Mode(), err)
		return fmt.Errorf("failed to set permissions on '%s': %w", dst, err)
	}
	return nil
}

type BuildImageCLIParams struct {
	Context   context.Context
	ImageName string
	Source    *config.DockerfileSource
	EnvVars   []config.EnvVar
}

func BuildImageCLI(params BuildImageCLIParams) error {

	absContext, err := filepath.Abs(params.Source.BuildContext)
	if err != nil {
		return fmt.Errorf("failed to resolve absolute path for build context '%s': %w", params.Source.BuildContext, err)
	}
	absDockerfile, err := filepath.Abs(params.Source.Path)
	if err != nil {
		return fmt.Errorf("failed to resolve absolute path for Dockerfile '%s': %w", params.Source.Path, err)
	}

	// Docker CLI expects the Dockerfile path to be relative to the build context if it's inside.
	dockerfilePathForCli := absDockerfile
	if strings.HasPrefix(absDockerfile, absContext+string(filepath.Separator)) {
		relPath, err := filepath.Rel(absContext, absDockerfile)
		if err != nil {
			return fmt.Errorf("failed to calculate relative Dockerfile path for CLI: %w", err)
		}
		dockerfilePathForCli = relPath
	}

	cmdArgs := []string{"build", "-t", params.ImageName, "-f", dockerfilePathForCli}

	// Add build args from params.Source.BuildArgs
	for k, v := range params.Source.BuildArgs {
		cmdArgs = append(cmdArgs, "--build-arg", fmt.Sprintf("%s=%s", k, v))
	}

	// Add environment variables as build args
	if len(params.EnvVars) > 0 {
		decryptedEnvVars, err := config.DecryptEnvVars(params.EnvVars)
		if err != nil {
			return fmt.Errorf("failed to decrypt env vars for build args: %w", err)
		}
		for _, envVar := range decryptedEnvVars {
			strValue, err := envVar.GetValue() // Assuming GetValue returns the decrypted string
			if err != nil {
				return fmt.Errorf("failed to get value for environment variable '%s': %w", envVar.Name, err)
			}
			cmdArgs = append(cmdArgs, "--build-arg", fmt.Sprintf("%s=%s", envVar.Name, strValue))
		}
	}

	// Add the build context path at the end
	cmdArgs = append(cmdArgs, absContext)

	cmd := exec.CommandContext(params.Context, "docker", cmdArgs...)
	cmd.Dir = absContext // Set the working directory for the command

	// For real-time output, you might want to use cmd.StdoutPipe and cmd.StderrPipe
	// and process them in goroutines. For simplicity, CombinedOutput is used here.
	output, err := cmd.CombinedOutput()

	if err != nil {
		// Log the output for debugging
		ui.Debug("Docker build failed. Output:\n%s", string(output))

		if exitErr, ok := err.(*exec.ExitError); ok {
			return fmt.Errorf("docker build command failed with exit code %d: %w. Output: %s", exitErr.ExitCode(), err, string(output))
		}
		return fmt.Errorf("failed to execute docker build command: %w. Output: %s", err, string(output))
	}

	ui.Success("Docker build completed successfully for '%s'.", params.ImageName)

	return nil
}
</file>

<file path="internal/manager/haproxy.go">
package manager

import (
	"bytes"
	"context"
	"fmt"
	"os"
	"path/filepath"
	"strings"
	"sync"
	"text/template"
	"time"

	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/embed"
	"github.com/ameistad/haloy/internal/helpers"
	"github.com/ameistad/haloy/internal/logging"
	"github.com/docker/docker/api/types/container"
	"github.com/docker/docker/api/types/filters"
	"github.com/docker/docker/client"
)

type HAProxyManager struct {
	dockerClient *client.Client
	logger       *logging.Logger
	configDir    string
	dryRun       bool
	updateMutex  sync.Mutex // Mutex protects config writing and reload signaling
}

type HAProxyManagerConfig struct {
	DockerClient *client.Client
	Logger       *logging.Logger
	ConfigDir    string
	DryRun       bool
}

func NewHAProxyManager(config HAProxyManagerConfig) *HAProxyManager {
	return &HAProxyManager{
		dockerClient: config.DockerClient,
		logger:       config.Logger,
		configDir:    config.ConfigDir,
		dryRun:       config.DryRun,
	}
}

// ApplyConfig generates, writes (if not dryRun), and reloads HAProxy config.
// This method is concurrency-safe due to the internal mutex.
func (hpm *HAProxyManager) ApplyConfig(ctx context.Context, deployments map[string]Deployment) error {
	hpm.logger.Info("HAProxyManager: Attempting to apply new configuration...")

	hpm.updateMutex.Lock()
	defer hpm.updateMutex.Unlock()

	// Generate Config (with certificate check)
	hpm.logger.Info("HAProxyManager: Generating new configuration...")
	configBuf, err := hpm.generateConfig(deployments)
	if err != nil {
		return fmt.Errorf("HAProxyManager: failed to generate config: %w", err)
	}

	if hpm.dryRun {
		hpm.logger.Info("HAProxyManager: DryRun - Skipping config write and reload.")
		hpm.logger.Info(configBuf.String())
		return nil
	}

	// Write Config File
	configPath := filepath.Join(hpm.configDir, config.HAProxyConfigFileName)
	hpm.logger.Info("HAProxyManager: Writing config")
	if err := os.WriteFile(configPath, configBuf.Bytes(), 0644); err != nil {
		return fmt.Errorf("HAProxyManager: failed to write config file %s: %w", configPath, err)
	}

	// Get HAProxy Container ID
	haproxyID, err := hpm.getContainerID(ctx)
	if err != nil {
		return fmt.Errorf("HAProxyManager: failed to find HAProxy container: %w", err)
	}
	if haproxyID == "" {
		hpm.logger.Warn("HAProxyManager: No HAProxy container found with label, cannot reload.")
		return nil // Not necessarily an error if HAProxy isn't running
	}

	// 4. Signal HAProxy Reload
	hpm.logger.Debug("HAProxyManager: Sending SIGUSR2 signal to HAProxy container...")
	err = hpm.dockerClient.ContainerKill(ctx, haproxyID, "SIGUSR2")
	if err != nil {
		// Log error but potentially don't fail the whole update if signal fails? Or return error?
		// Let's return error for now.
		return fmt.Errorf("HAProxyManager: failed to send SIGUSR2 to HAProxy container %s: %w", helpers.SafeIDPrefix(haproxyID), err)
	}

	hpm.logger.Debug("HAProxyManager: Successfully signaled HAProxy for reload.")
	return nil
}

// generateConfig creates the HAProxy configuration content based on deployments.
// It checks for certificate existence before adding HTTPS bindings.
func (hpm *HAProxyManager) generateConfig(deployments map[string]Deployment) (bytes.Buffer, error) {
	var buf bytes.Buffer
	var httpFrontend string
	var httpsFrontend string
	var httpsFrontendUseBackend string
	var backends string
	const indent = "    "

	for appName, d := range deployments {
		backendName := appName
		var canonicalACLs []string

		for _, domain := range d.Labels.Domains {
			if domain.Canonical != "" {
				canonicalKey := strings.ReplaceAll(domain.Canonical, ".", "_")
				canonicalACLName := fmt.Sprintf("%s_%s_canonical", backendName, canonicalKey)

				httpsFrontend += fmt.Sprintf("%sacl %s hdr(host) -i %s\n", indent, canonicalACLName, domain.Canonical)
				canonicalACLs = append(canonicalACLs, canonicalACLName)

				httpFrontend += fmt.Sprintf("%sacl %s hdr(host) -i %s\n", indent, canonicalACLName, domain.Canonical)
				// Redirect HTTP to HTTPS for the canonical domain but exclude ACME challenge.
				httpFrontend += fmt.Sprintf("%shttp-request redirect code 301 location https://%s%%[path] if %s !is_acme_challenge\n",
					indent, domain.Canonical, canonicalACLName)

				for _, alias := range domain.Aliases {
					if alias != "" {
						aliasKey := strings.ReplaceAll(alias, ".", "_")
						aliasACLName := fmt.Sprintf("%s_%s_alias", backendName, aliasKey)

						httpsFrontend += fmt.Sprintf("%sacl %s hdr(host) -i %s\n", indent, aliasACLName, alias)
						httpsFrontend += fmt.Sprintf("%shttp-request redirect code 301 location https://%s%%[path] if %s !is_acme_challenge\n",
							indent, domain.Canonical, aliasACLName)

						httpFrontend += fmt.Sprintf("%sacl %s hdr(host) -i %s\n", indent, aliasACLName, alias)
						httpFrontend += fmt.Sprintf("%shttp-request redirect code 301 location https://%s%%[path] if %s !is_acme_challenge\n",
							indent, domain.Canonical, aliasACLName)
					}
				}
			}
		}

		if len(canonicalACLs) > 0 {
			httpsFrontendUseBackend += fmt.Sprintf("%suse_backend %s if %s\n", indent, backendName, strings.Join(canonicalACLs, " or "))
		}
	}

	for _, d := range deployments {
		backendName := d.Labels.AppName
		backends += fmt.Sprintf("backend %s\n", backendName)
		for i, inst := range d.Instances {
			backends += fmt.Sprintf("%sserver app%d %s:%s check\n", indent, i+1, inst.IP, inst.Port)
		}
	}

	data, err := embed.TemplatesFS.ReadFile(fmt.Sprintf("templates/%s", config.HAProxyConfigFileName))
	if err != nil {
		return buf, fmt.Errorf("failed to read embedded file: %w", err)
	}

	tmpl, err := template.New("config").Parse(string(data))
	if err != nil {
		return buf, fmt.Errorf("failed to parse template: %w", err)
	}

	templateData := embed.HAProxyTemplateData{
		HTTPFrontend:            httpFrontend,
		HTTPSFrontend:           httpsFrontend,
		HTTPSFrontendUseBackend: httpsFrontendUseBackend,
		Backends:                backends,
	}

	if err := tmpl.Execute(&buf, templateData); err != nil {
		return buf, fmt.Errorf("failed to execute template: %w", err)
	}

	return buf, nil
}

func (hpm *HAProxyManager) getContainerID(ctx context.Context) (string, error) {
	// Configure retry parameters
	maxRetries := 30
	retryInterval := time.Second

	for retry := 0; retry < maxRetries; retry++ {
		// Check if context was canceled
		if ctx.Err() != nil {
			return "", fmt.Errorf("context canceled while waiting for HAProxy container: %w", ctx.Err())
		}

		// Set up filter for HAProxy container
		filtersArgs := filters.NewArgs()
		filtersArgs.Add("label", fmt.Sprintf("%s=%s", config.LabelRole, config.HAProxyLabelRole))
		filtersArgs.Add("status", "running") // Only consider running containers

		containers, err := hpm.dockerClient.ContainerList(ctx, container.ListOptions{
			Filters: filtersArgs,
			Limit:   1, // We only expect one HAProxy container managed by haloy
		})
		if err != nil {
			return "", fmt.Errorf("failed to list containers with label %s=%s: %w",
				config.LabelRole, config.HAProxyLabelRole, err)
		}

		if len(containers) > 0 {
			// Found a running HAProxy container
			return containers[0].ID, nil
		}

		// No running container found yet - log on first attempt and halfway through
		if retry == 0 || retry == maxRetries/2 {
			hpm.logger.Info(fmt.Sprintf("HAProxyManager: Waiting for HAProxy container to be running. Attempt %d of %d", retry+1, maxRetries))
		}

		// Wait before retrying
		select {
		case <-ctx.Done():
			return "", fmt.Errorf("context canceled while waiting for HAProxy container: %w", ctx.Err())
		case <-time.After(retryInterval):
			// Continue to next retry
		}
	}

	return "", fmt.Errorf("timed out waiting for HAProxy container to be in running state after %d seconds",
		maxRetries)
}
</file>

<file path="README.md">
# Haloy
Haloy is a tool for managing Dockerized applications with zero downtime deployments on your own infrastructure. It uses HAProxy to route traffic to the correct containers based on domain names and provides a configuration file for easy setup.

## Installation

Download the latest release from [GitHub Releases](https://github.com/ameistad/haloy/releases).

```bash
# Linux (AMD64)
curl -L https://github.com/ameistad/haloy/releases/latest/download/haloy-linux-amd64 -o haloy
chmod +x haloy
sudo mv haloy /usr/local/bin/

# macOS (Apple Silicon)
curl -L https://github.com/ameistad/haloy/releases/latest/download/haloy-darwin-arm64 -o haloy
chmod +x haloy
sudo mv haloy /usr/local/bin/
```

## Getting Started

### Prerequisites

- Docker installed and running
- A non-root user added to the docker group: `sudo usermod -aG docker $(whoami)`
- Verify your group membership (you should see “docker” in the output):
  ```bash
  id -nG $(whoami)
  # or
  groups $(whoami)
  ```
- Log out and log back in for group changes to take effect, or run newgrp docker
- Check that you can run Docker commands without sudo:
  ```bash
  docker ps
  ```

### Initialize Haloy

```bash
haloy init
```

This will:
- Set up the directory structure at `~/.config/haloy/`
- Create a sample configuration file at `~/.config/haloy/apps.yml`
- Initialize HAProxy

### Configure Your Apps

Edit the configuration file at `~/.config/haloy/apps.yml`:

Example configuration:
```yaml
apps:
  - name: "example-app"
    source:
      dockerfile:
         path: "/path/to/your/Dockerfile"
         buildContext: "/path/to/your/app"
         buildArgs:
           - "ARG1=value1"
           - "ARG2=value2"
    domains:
      - domain: "example.com"
        aliases:
          - "www.example.com"
      - domain: "api.example.com"
    port: 8080 # Optional: Default is 80
    env: # Optional
      - name: "NODE_ENV"
        value: "production"
      - name: "API_KEY" 
        secretName: "api-key" # Reference to a secret stored with 'haloy secrets set'
   maxContainersToKeep: 5 # Optional: Default is 3
   volumes: # Optional
      - "/host/path:/container/path"
   healthCheckPath: "/health" # Optional: Default is "/"
```

### Deploy Your Apps

```bash
# Deploy a single app
haloy deploy example-app

# Deploy all apps
haloy deploy-all

# Check the status of your deployments
haloy status

# List all deployed containers
haloy list

# Roll back to a previous deployment
haloy rollback example-app
```

## Configuration
### App Configuration

Each app in the `apps` array can have the following properties:

- `name`: Unique name for the app (required)
- `domains`: List of domains for the app (required) 
  - With aliases: `{ domain: "example.com", aliases: ["www.example.com"] }`
- `dockerfile`: Path to your Dockerfile (required)
- `buildContext`: Build context directory for Docker (required)
- `env`: Environment variables for the container
  - Plain values: `{ name: "NODE_ENV", value: "production" }`
  - Secret values: `{ name: "API_KEY", secretName: "api-key" }` (references a stored secret)
- `maxContainersToKeep`: Number of old containers to keep after deployment (default: 3)
- `volumes`: Docker volumes to mount
- `healthCheckPath`: HTTP path for health checks (default: "/")

### Secrets Management

Haloy provides a secure way to manage sensitive environment variables using the `secrets` command:

```bash
# Initialize the secrets system (generates encryption keys)
haloy secrets init

# Store a secret
haloy secrets set api-key "your-secret-api-key"

# List all stored secrets
haloy secrets list

# Delete a secret
haloy secrets delete api-key
```

To use a secret in your app configuration:

1. Store the secret using `haloy secrets set <secret-name> <value>`
2. Reference it in your `apps.yml` file:
   ```yaml
   env:
     - name: "API_KEY"
       secretName: "api-key"  # References the stored secret named "api-key"
   ```

Secrets are securely encrypted at rest using [age encryption](https://age-encryption.org) and are only decrypted when needed for deployments.

## Development

### Building the CLI

```bash
go build -o haloy ./cmd/cli
```

## Releasing

Haloy uses GitHub Actions for automated builds and releases.

### Automated Release Process

1. Make sure all code changes are committed and pushed to main:
   - Any workflow or CI/CD changes should be made separately from the release process
   - Push these changes and wait for the workflow to complete before proceeding

2. Update the version number in relevant files:
   ```bash
   # Edit the version constant in internal/version/version.go
   git add internal/version/version.go
   git commit -m "Bump version to v1.0.0"
   git push origin main
   ```

3. Create an annotated tag for the release:
   ```bash
   git tag -a v1.0.0 -m "Release v1.0.0: Brief description of changes"
   git push origin v1.0.0
   ```

4. GitHub Actions will automatically:
   - Run all tests
   - Build the manager Docker image and push it to GitHub Container Registry with version tags
   - Build the CLI binaries for all supported platforms
   - Create a GitHub Release with the binaries attached

5. Verify the release at:
   ```
   https://github.com/ameistad/haloy/releases
   ```

**Important Note**: The workflow is optimized to:
- Only run tests for pushes to branches and pull requests
- Run the full build process (including release) only when pushing a tag
- This separation prevents duplicate builds and conserves GitHub Actions minutes

### Manual Release Process

If you need to build releases manually:

1. Build the CLI for multiple platforms:
   ```bash
   # Linux (AMD64)
   GOOS=linux GOARCH=amd64 go build -o haloy-linux-amd64 ./cmd/cli
   
   # Linux (ARM64)
   GOOS=linux GOARCH=arm64 go build -o haloy-linux-arm64 ./cmd/cli
   
   # macOS (AMD64)
   GOOS=darwin GOARCH=amd64 go build -o haloy-darwin-amd64 ./cmd/cli
   
   # macOS (ARM64)
   GOOS=darwin GOARCH=arm64 go build -o haloy-darwin-arm64 ./cmd/cli
   
   # Windows
   GOOS=windows GOARCH=amd64 go build -o haloy-windows-amd64.exe ./cmd/cli
   ```

2. Build and push the manager Docker image:
   ```bash
   docker build -t ghcr.io/ameistad/haloy-manager:latest -f build/manager/Dockerfile .
   docker push ghcr.io/ameistad/haloy-manager:latest
   ```


### List of labels that haloy will use to configure HAProxy.

Haloy uses the following Docker container labels to configure HAProxy:
- `haloy.appName` - Identifies the application name
- `haloy.deployment` - Identifies the deployment ID
- `haloy.domains.all` - A comma-separated list of all domains
- `haloy.domain.<index>` - The canonical domain name for the specified index
- `haloy.domain.<index>.alias.<alias_index>` - Domain aliases that should redirect to the canonical domain
- `haloy.health-check-path` - The path to the health check endpoint


## License

[MIT License](LICENSE)
</file>

<file path="internal/docker/container.go">
package docker

import (
	"context"
	"fmt"
	"io"
	"net/http"
	"sort"
	"time"

	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/helpers"
	"github.com/ameistad/haloy/internal/ui"
	"github.com/docker/docker/api/types/container"
	"github.com/docker/docker/api/types/filters"
	"github.com/docker/docker/api/types/network"
	"github.com/docker/docker/client"
)

type ContainerRunResult struct {
	ID           string
	DeploymentID string
	ReplicaID    int
}

func RunContainer(ctx context.Context, dockerClient *client.Client, deploymentID, imageName string, appConfig *config.AppConfig) ([]ContainerRunResult, error) {

	result := make([]ContainerRunResult, 0, *appConfig.Replicas)

	// Convert AppConfig to ContainerLabels
	cl := config.ContainerLabels{
		AppName:         appConfig.Name,
		DeploymentID:    deploymentID,
		ACMEEmail:       appConfig.ACMEEmail,
		Port:            appConfig.Port,
		HealthCheckPath: appConfig.HealthCheckPath,
		Domains:         appConfig.Domains,
		Role:            config.AppLabelRole,
	}
	labels := cl.ToLabels()

	// Process environment variables
	var envVars []string
	decryptedEnvVars, err := config.DecryptEnvVars(appConfig.Env)
	if err != nil {
		return result, fmt.Errorf("failed to decrypt environment variables: %w", err)
	}
	for _, v := range decryptedEnvVars {
		value, err := v.GetValue()
		if err != nil {
			return result, fmt.Errorf("failed to get value for env var '%s': %w", v.Name, err)
		}
		envVars = append(envVars, fmt.Sprintf("%s=%s", v.Name, value))
	}

	// Prepare host configuration - set restart policy and volumes to mount.
	hostConfig := &container.HostConfig{
		RestartPolicy: container.RestartPolicy{Name: "unless-stopped"},
		Binds:         appConfig.Volumes,
	}

	// Attach the container to the predefined network
	networkingConfig := &network.NetworkingConfig{
		EndpointsConfig: map[string]*network.EndpointSettings{
			config.DockerNetwork: {},
		},
	}

	for i := range make([]struct{}, *appConfig.Replicas) {
		envVars := append(envVars, fmt.Sprintf("HALOY_REPLICA_ID=%d", i+1))
		containerConfig := &container.Config{
			Image:  imageName,
			Labels: labels,
			Env:    envVars,
		}
		containerName := fmt.Sprintf("%s-haloy-%s-replica-%d", appConfig.Name, deploymentID, i+1)
		resp, err := dockerClient.ContainerCreate(ctx, containerConfig, hostConfig, networkingConfig, nil, containerName)
		if err != nil {
			return result, fmt.Errorf("failed to create container: %w", err)
		}

		// Ensure the container is removed on error
		// This is important to avoid leaving dangling containers in case of failure.
		// We use a deferred function to ensure cleanup happens even if the function exits early.
		defer func() {
			if err != nil && resp.ID != "" {
				// Try to remove container on error
				removeErr := dockerClient.ContainerRemove(ctx, resp.ID, container.RemoveOptions{Force: true})
				if removeErr != nil {
					fmt.Printf("Failed to clean up container after error: %v\n", removeErr)
				}
			}
		}()

		if err := dockerClient.ContainerStart(ctx, resp.ID, container.StartOptions{}); err != nil {
			return result, fmt.Errorf("failed to start container: %w", err)
		}

		result = append(result, ContainerRunResult{
			ID:           resp.ID,
			DeploymentID: deploymentID,
			ReplicaID:    i + 1,
		})

	}

	return result, nil
}

func StopContainers(ctx context.Context, dockerClient *client.Client, appName, ignoreDeploymentID string) ([]string, error) {
	stoppedIDs := []string{}
	filter := filters.NewArgs()
	filter.Add("label", fmt.Sprintf("%s=%s", config.LabelAppName, appName))

	containerList, err := dockerClient.ContainerList(ctx, container.ListOptions{
		Filters: filter,
		All:     false, // Only running containers
	})

	for _, containerInfo := range containerList {
		deploymentID := containerInfo.Labels[config.LabelDeploymentID]
		if deploymentID == ignoreDeploymentID {
			continue
		}

		timeout := 20
		stopOptions := container.StopOptions{
			Timeout: &timeout,
		}
		err := dockerClient.ContainerStop(ctx, containerInfo.ID, stopOptions)
		if err != nil {
			ui.Warn("Error stopping container %s: %v\n", helpers.SafeIDPrefix(containerInfo.ID), err)
		} else {
			stoppedIDs = append(stoppedIDs, containerInfo.ID)
		}
	}
	if err != nil {
		return stoppedIDs, fmt.Errorf("failed to list containers: %w", err)
	}
	return stoppedIDs, nil
}

type RemoveContainersParams struct {
	Context             context.Context
	DockerClient        *client.Client
	AppName             string
	IgnoreDeploymentID  string
	MaxContainersToKeep int
}

type RemoveContainersResult struct {
	ID           string
	DeploymentID string
}

func RemoveContainers(params RemoveContainersParams) ([]RemoveContainersResult, error) {
	filter := filters.NewArgs()
	filter.Add("label", fmt.Sprintf("%s=%s", config.LabelAppName, params.AppName))

	containerList, err := params.DockerClient.ContainerList(params.Context, container.ListOptions{
		Filters: filter,
		All:     true,
	})
	if err != nil {
		return nil, fmt.Errorf("failed to list containers: %w", err)
	}

	var containers []RemoveContainersResult

	// Filter out the container with IgnoreDeploymentID
	for _, c := range containerList {
		deploymentID := c.Labels[config.LabelDeploymentID]
		if deploymentID == params.IgnoreDeploymentID {
			continue
		}
		containers = append(containers, RemoveContainersResult{
			ID:           c.ID,
			DeploymentID: deploymentID,
		})
	}

	// Sort containers by deploymentID (newest/largest timestamp first)
	sort.Slice(containers, func(i, j int) bool {
		return containers[i].DeploymentID > containers[j].DeploymentID
	})

	// Skip newest containers according to NumberOfContainersToSkip
	removedContainers := []RemoveContainersResult{}
	if params.MaxContainersToKeep == 0 {
		// Remove all containers except the one with IgnoreDeploymentID
		removedContainers = containers
	} else if params.MaxContainersToKeep > 0 && len(containers) > params.MaxContainersToKeep {
		containersToKeep := containers[:params.MaxContainersToKeep]
		removedContainers = containers[params.MaxContainersToKeep:]

		_ = containersToKeep // just to avoid linter error
	}

	// Remove the remaining containers
	for _, c := range removedContainers {
		err := params.DockerClient.ContainerRemove(params.Context, c.ID, container.RemoveOptions{Force: true})
		if err != nil {
			ui.Warn("Error removing container %s: %v\n", helpers.SafeIDPrefix(c.ID), err)
		}
	}

	return removedContainers, nil
}

func HealthCheckContainer(ctx context.Context, dockerClient *client.Client, containerID string, initialWaitTime ...time.Duration) error {
	// Check if container is running - wait up to 30 seconds for it to start
	startCtx, cancel := context.WithTimeout(ctx, 30*time.Second)
	defer cancel()

	var containerInfo container.InspectResponse
	var err error

	// Wait for container to be running
	ui.Info("Waiting for container %s to be running...", helpers.SafeIDPrefix(containerID))
	for {
		containerInfo, err = dockerClient.ContainerInspect(startCtx, containerID)
		if err != nil {
			return fmt.Errorf("failed to inspect container %s: %w", helpers.SafeIDPrefix(containerID), err)
		}

		if containerInfo.State.Running {
			break
		}

		select {
		case <-startCtx.Done():
			return fmt.Errorf("timed out waiting for container %s to start", helpers.SafeIDPrefix(containerID))
		case <-time.After(500 * time.Millisecond):
		}
	}

	if len(initialWaitTime) > 0 && initialWaitTime[0] > 0 {
		waitTime := initialWaitTime[0]
		ui.Info("Waiting %v for container to initialize...", waitTime)

		waitTimer := time.NewTimer(waitTime)
		select {
		case <-ctx.Done():
			return fmt.Errorf("context canceled during initial wait period")
		case <-waitTimer.C:
		}
	}

	// Check if container has built-in Docker healthcheck
	if containerInfo.State.Health != nil {
		ui.Info("Container has built-in health check, status: %s", containerInfo.State.Health.Status)

		// If container has healthcheck and it's healthy, we can skip our manual check
		if containerInfo.State.Health.Status == "healthy" {
			ui.Success("Container %s is healthy according to Docker healthcheck", helpers.SafeIDPrefix(containerID))
			return nil
		}
	}

	// Check if container has built-in Docker healthcheck
	if containerInfo.State.Health != nil {
		ui.Info("Container has built-in health check, status: %s", containerInfo.State.Health.Status)

		// Wait for Docker healthcheck to transition from starting state
		if containerInfo.State.Health.Status == "starting" {
			healthCtx, cancel := context.WithTimeout(ctx, 30*time.Second)
			defer cancel()

			ui.Info("Waiting for built-in health check to complete...")
			for {
				containerInfo, err = dockerClient.ContainerInspect(healthCtx, containerID)
				if err != nil {
					return fmt.Errorf("failed to re-inspect container: %w", err)
				}

				if containerInfo.State.Health.Status != "starting" {
					break
				}

				select {
				case <-healthCtx.Done():
					return fmt.Errorf("timed out waiting for container health check to complete")
				case <-time.After(1 * time.Second):
					// Continue polling
				}
			}
		}

		// If container has healthcheck and it's healthy, we can skip our manual check
		if containerInfo.State.Health.Status == "healthy" {
			ui.Success("Container %s is healthy according to Docker healthcheck", helpers.SafeIDPrefix(containerID))
			return nil
		} else if containerInfo.State.Health.Status == "unhealthy" {
			// Log health check failure details if available
			if len(containerInfo.State.Health.Log) > 0 {
				lastLog := containerInfo.State.Health.Log[len(containerInfo.State.Health.Log)-1]
				return fmt.Errorf("container %s is unhealthy: %s", helpers.SafeIDPrefix(containerID), lastLog.Output)
			}
			return fmt.Errorf("container %s is unhealthy according to Docker healthcheck", helpers.SafeIDPrefix(containerID))
		}
	}

	// Rest of the existing HTTP health check code remains the same...
	labels, err := config.ParseContainerLabels(containerInfo.Config.Labels)
	if err != nil {
		return fmt.Errorf("failed to parse container labels: %w", err)
	}

	if labels.Port == "" {
		return fmt.Errorf("container %s has no port label set", helpers.SafeIDPrefix(containerID))
	}

	if labels.HealthCheckPath == "" {
		return fmt.Errorf("container %s has no health check path set", helpers.SafeIDPrefix(containerID))
	}

	targetIP, err := ContainerNetworkIP(containerInfo, config.DockerNetwork)
	if err != nil {
		return fmt.Errorf("failed to get container IP address: %w", err)
	}

	// Construct URL for health check
	healthCheckURL := fmt.Sprintf("http://%s:%s%s", targetIP, labels.Port, labels.HealthCheckPath)

	// Perform health check with retries
	maxRetries := 5
	backoff := 500 * time.Millisecond

	httpClient := &http.Client{
		Timeout: 5 * time.Second,
	}

	// Use traditional for loop for clarity
	for retry := 0; retry < maxRetries; retry++ {
		if retry > 0 {
			ui.Info("Retrying health check in %v... (attempt %d/%d)\n", backoff, retry+1, maxRetries)
			time.Sleep(backoff)
			backoff *= 2 // Exponential backoff
		}

		req, err := http.NewRequestWithContext(ctx, "GET", healthCheckURL, nil)
		if err != nil {
			return fmt.Errorf("failed to create health check request: %w", err)
		}

		resp, err := httpClient.Do(req)
		if err != nil {
			ui.Warn("Health check attempt failed: %v", err)
			continue
		}
		defer resp.Body.Close()

		if resp.StatusCode >= 200 && resp.StatusCode < 300 {
			ui.Success("Health check passed for container %s\n", helpers.SafeIDPrefix(containerID))
			return nil
		}

		bodyBytes, _ := io.ReadAll(io.LimitReader(resp.Body, 1024))
		ui.Warn("Health check returned status %d: %s", resp.StatusCode, string(bodyBytes))
	}

	return fmt.Errorf("container %s failed health check after %d attempts", helpers.SafeIDPrefix(containerID), maxRetries)
}
</file>

<file path="internal/manager/manager.go">
package manager

import (
	"context"
	"fmt"
	"io"
	"os"
	"os/signal"
	"strings"
	"syscall"
	"time"

	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/helpers"
	"github.com/ameistad/haloy/internal/logging"
	"github.com/ameistad/haloy/internal/version"
	"github.com/docker/docker/api/types/container"
	"github.com/docker/docker/api/types/events"
	"github.com/docker/docker/api/types/filters"
	"github.com/docker/docker/client"
)

const (
	RefreshInterval    = 30 * time.Minute
	HAProxyConfigPath  = "/haproxy-config"
	CertificatesPath   = "/cert-storage"
	LogsPath           = "/logs"
	HTTPProviderPort   = "8080"
	EventDebounceDelay = 1 * time.Second // Delay for debouncing container events
	UpdateTimeout      = 2 * time.Minute // Max time for a single update operation
)

type ContainerEvent struct {
	Event     events.Message
	Container container.InspectResponse
	Labels    *config.ContainerLabels
}

func RunManager(dryRun bool) {
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Initialize logging
	logLevel := logging.INFO
	if dryRun {
		logLevel = logging.DEBUG
	}
	logger, err := logging.NewLogger(logLevel)
	if err != nil {
		fmt.Fprintf(os.Stderr, "CRITICAL: Failed to initialize logging: %v", err)
	}

	logger.Info(fmt.Sprintf("Haloy manager version %s started on network %s...", version.Version, config.DockerNetwork))

	if dryRun {
		logger.Info("Running in dry-run mode, no changes will be applied to HAProxy")
		logger.Debug("Debug logging enabled")
	}

	// Initialize Docker client
	dockerClient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())
	if err != nil {
		logger.Fatal("Failed to create Docker client", err)
	}
	defer dockerClient.Close()

	sigChan := make(chan os.Signal, 1)
	signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

	// Channel for Docker events
	eventsChan := make(chan ContainerEvent)
	errorsChan := make(chan error)

	// Channel for signaling cert updates needing HAProxy reload
	// Buffered channel to prevent blocking CertManager if RunManager is busy
	certUpdateSignal := make(chan string, 5)

	// Create deployment manager
	deploymentManager := NewDeploymentManager(ctx, dockerClient, logger)

	// Create and start the certifications manager
	certManagerConfig := CertificatesManagerConfig{
		CertDir:          CertificatesPath,
		HTTPProviderPort: HTTPProviderPort,
		TlsStaging:       dryRun,
	}
	certManager, err := NewCertificatesManager(certManagerConfig, certUpdateSignal)
	if err != nil {
		logger.Fatal("Failed to create certificate manager", err)
		return
	}

	// Create the HAProxy manager
	haproxyManagerConfig := HAProxyManagerConfig{
		DockerClient: dockerClient,
		Logger:       logger,
		ConfigDir:    HAProxyConfigPath,
		DryRun:       dryRun,
	}
	haproxyManager := NewHAProxyManager(haproxyManagerConfig)

	// Updater to glue deployment manager and certificate manager and handle HAProxy updates.
	updaterConfig := UpdaterConfig{
		DeploymentManager: deploymentManager,
		CertManager:       certManager,
		HAProxyManager:    haproxyManager,
	}

	// Perform initial update
	updater := NewUpdater(updaterConfig)
	updateReason := "initial update"
	if err := updater.Update(ctx, updateReason, logger); err != nil {
		logger.Error("Background update failed", err)
	}

	// Create a debouncer to prevent multiple updates in quick succession
	debouncer := helpers.NewDebouncer(EventDebounceDelay)

	// Start Docker event listener
	go listenForDockerEvents(ctx, dockerClient, eventsChan, errorsChan, logger)

	// Start periodic full refresh
	refreshTicker := time.NewTicker(RefreshInterval)
	defer refreshTicker.Stop()

	// Track the latest deploymentID per appName. This way we use the latest ID for logging.
	latestDeploymentID := make(map[string]string)

	// Main event loop
	for {
		select {
		case <-sigChan:
			logger.Info("Received shutdown signal, stopping manager...")
			if certManager != nil {
				certManager.Stop()
			}
			cancel()
			return
		case e := <-eventsChan:
			appName := e.Labels.AppName
			deploymentID := e.Labels.DeploymentID
			reason := fmt.Sprintf("container %s: %s", e.Event.Action, appName)
			if deploymentID > latestDeploymentID[appName] {
				latestDeploymentID[appName] = deploymentID
			}

			fmt.Printf("%s: Manager event for deployment ID: %s\n", appName, e.Labels.DeploymentID)

			updateAction := func() {
				// Create a logger for this specific deployment id. This will write a log file for the specific deployment ID.
				deploymentLogger, err := logging.NewLogger(logger.Level)
				if err != nil {
					logger.Error("Failed to create logger for deployment update", err)
					return
				}
				id := latestDeploymentID[appName]
				if id != "" {
					deploymentLogger.SetDeploymentIDFileWriter(LogsPath, id)
				}
				// Create a context with a timeout for this specific update task.
				// Use the main manager context `ctx` as the parent.
				updateCtx, cancelUpdate := context.WithTimeout(ctx, UpdateTimeout)
				defer cancelUpdate()
				defer deploymentLogger.CloseLog()
				if err := updater.Update(updateCtx, reason, deploymentLogger); err != nil {
					deploymentLogger.Error("Debounced HAProxy configuration update failed", err)
				}
			}

			debouncer.Debounce(appName, updateAction)

		case domainUpdated := <-certUpdateSignal:
			reason := fmt.Sprintf("post-certificate update for %s", domainUpdated)
			logger.Debug(fmt.Sprintf("Received cert update signal: %s", reason))

			go func(updateReason string) {
				// Use a timeout context for this specific task
				updateCtx, cancelUpdate := context.WithTimeout(ctx, 60*time.Second)
				defer cancelUpdate()

				u := updater // Capture updater
				// Important: Update only needs to apply config, not full build/check
				// We assume the deployment state triggering the cert update is still valid.
				// Get the current deployment state
				currentDeployments := u.deploymentManager.Deployments()
				// Directly apply HAProxy config
				if err := u.haproxyManager.ApplyConfig(updateCtx, currentDeployments); err != nil {
					logger.Error(fmt.Sprintf("Background HAProxy update failed for reason: %s", updateReason), err)
				}
			}(reason)

		case err := <-errorsChan:
			logger.Error("Error from docker events", err)
		case <-refreshTicker.C:
			reason := "periodic full refresh"
			logger.Info("Performing Periodic full refresh")

			go func(updateReason string) {
				deploymentCtx, cancelDeployment := context.WithCancel(ctx)
				defer cancelDeployment()

				u := updater // Capture updater
				if err := u.Update(deploymentCtx, updateReason, logger); err != nil {
					logger.Error("Background update failed", err)
				}
			}(reason)
		}
	}
}

// listenForDockerEvents sets up a listener for Docker events
func listenForDockerEvents(ctx context.Context, dockerClient *client.Client, eventsChan chan ContainerEvent, errorsChan chan error, logger *logging.Logger) {
	// Set up filter for container events
	filterArgs := filters.NewArgs()
	filterArgs.Add("type", "container")

	// Start listening for events
	eventOptions := events.ListOptions{
		Filters: filterArgs,
	}

	events, errs := dockerClient.Events(ctx, eventOptions)

	// Forward events and errors to our channels
	for {
		select {
		case <-ctx.Done():
			return
		case event := <-events:
			if event.Action == "start" || event.Action == "die" || event.Action == "stop" || event.Action == "kill" {
				container, err := dockerClient.ContainerInspect(ctx, event.Actor.ID)
				if err != nil {
					logger.Error(fmt.Sprintf("Error inspecting container id %s", helpers.SafeIDPrefix(event.Actor.ID)), err)
					continue
				}
				eligible := IsAppContainer(container)

				// We'll only process events for containers that have been marked with haloy labels.
				if eligible {
					labels, err := config.ParseContainerLabels(container.Config.Labels)
					if err != nil {
						logger.Error("Error parsing container labels", err)
						return
					}

					containerEvent := ContainerEvent{
						Event:     event,
						Container: container,
						Labels:    labels,
					}
					eventsChan <- containerEvent
				} else {
					logger.Info(fmt.Sprintf("Container %s is not eligible for haloy management", helpers.SafeIDPrefix(event.Actor.ID)))
				}
			}
		case err := <-errs:
			if err != nil {
				errorsChan <- err
				// For non-fatal errors we'll try to reconnect instead of exiting
				if err != io.EOF && !strings.Contains(err.Error(), "connection refused") {
					// Attempt to reconnect
					time.Sleep(5 * time.Second)
					events, errs = dockerClient.Events(ctx, eventOptions)
					continue
				}
			}
			return
		}
	}
}

// IsAppContainer checks if a container should be handled by haloy.
func IsAppContainer(container container.InspectResponse) bool {

	// Check if the container has the correct labels.
	if container.Config.Labels[config.LabelRole] != config.AppLabelRole {
		return false
	}

	isOnNetwork := isOnNetworkCheck(container, config.DockerNetwork)
	return isOnNetwork
}

func isOnNetworkCheck(container container.InspectResponse, networkName string) bool {
	for netName := range container.NetworkSettings.Networks {
		if netName == networkName {
			return true
		}
	}
	return false
}
</file>

<file path="internal/deploy/deploy.go">
package deploy

import (
	"bufio"
	"context"
	"errors"
	"fmt"
	"os"
	"path/filepath"
	"time"

	"github.com/ameistad/haloy/internal/config"
	"github.com/ameistad/haloy/internal/docker"
	"github.com/ameistad/haloy/internal/ui"
	"github.com/docker/docker/client"
)

const (
	LogStreamAddress = "localhost:9000" // Address of the manager's log stream server
)

func DeployApp(appConfig *config.AppConfig) error {

	// printerDeployStatus, _ := pterm.DefaultSpinner.Start("Starting deployment...")

	// Create the primary context for the whole deployment + log streaming
	deployCtx, cancelDeploy := context.WithCancel(context.Background())
	// Ensure cancelDeploy is called eventually to stop the streamer and release resources
	defer cancelDeploy()

	// Create a derived context with a timeout for Docker operations
	// This context will be cancelled if deployCtx is cancelled OR if the timeout expires.
	dockerOpCtx, cancelDockerOps := context.WithTimeout(deployCtx, DefaultDeployTimeout)
	// Ensure the timeout context's resources are released
	defer cancelDockerOps()

	dockerClient, err := docker.NewClient(dockerOpCtx) // Use dockerOpCtx
	if err != nil {
		// Check if the error was due to the overall deployment context being cancelled early
		if errors.Is(err, context.Canceled) && deployCtx.Err() != nil {
			return fmt.Errorf("failed to create Docker client: deployment canceled (%w)", deployCtx.Err())
		}
		return fmt.Errorf("failed to create Docker client: %w", err)
	}
	defer dockerClient.Close()

	// Ensure that the custom docker network and required services are running.
	if err := docker.EnsureNetwork(dockerClient, dockerOpCtx); err != nil {
		return fmt.Errorf("failed to ensure Docker network exists: %w", err)
	}
	if _, err := docker.EnsureServicesIsRunning(dockerClient, dockerOpCtx); err != nil {
		return fmt.Errorf("failed to ensure dependent services are running: %w", err)
	}

	imageName, err := GetImage(dockerOpCtx, dockerClient, appConfig)
	if err != nil {
		return err
	}

	deploymentID := time.Now().Format("20060102150405")
	ui.Info("Starting deployment for app '%s' with deployment ID: %s", appConfig.Name, deploymentID)

	runResult, err := docker.RunContainer(dockerOpCtx, dockerClient, deploymentID, imageName, appConfig)
	if err != nil {
		if errors.Is(err, context.DeadlineExceeded) {
			return fmt.Errorf("failed to run new container: operation timed out after %v (%w)", DefaultDeployTimeout, err)
		} else if errors.Is(err, context.Canceled) {
			if deployCtx.Err() != nil {
				return fmt.Errorf("failed to run new container: deployment canceled (%w)", deployCtx.Err())
			}
			return fmt.Errorf("failed to run new container: docker operation canceled (%w)", err)
		}
		return fmt.Errorf("failed to run new container: %w", err)
	}
	if len(runResult) == 0 {
		return fmt.Errorf("failed to run new container: no containers started")
	}

	ui.Info("Started %d container(s) with deployment ID: %s", len(runResult), deploymentID)

	stoppedIDs, err := docker.StopContainers(dockerOpCtx, dockerClient, appConfig.Name, deploymentID)
	if err != nil {
		ui.Warn("Failed to stop old containers: %v\n", err)
	}
	removeContainersParams := docker.RemoveContainersParams{
		Context:             dockerOpCtx, // Use dockerOpCtx
		DockerClient:        dockerClient,
		AppName:             appConfig.Name,
		IgnoreDeploymentID:  deploymentID,
		MaxContainersToKeep: *appConfig.MaxContainersToKeep,
	}
	removedContainers, err := docker.RemoveContainers(removeContainersParams)
	if err != nil {
		ui.Warn("Failed to remove old containers: %v\n", err)
	}
	ui.Info("Cleanup complete:\nStopped %d container(s)\nRemoved %d old container(s)", len(stoppedIDs), len(removedContainers))

	// Explicitly cancel the primary context *before* waiting.
	// This signals the log streamer to stop.
	cancelDeploy()

	tailDeploymentLog(deploymentID)

	return nil
}

func GetImage(ctx context.Context, dockerClient *client.Client, appConfig *config.AppConfig) (string, error) {

	switch true {
	case appConfig.Source.Dockerfile != nil:
		// Source is a Dockerfile. The image name is derived from the app name.
		imageName := appConfig.Name + ":latest" // Convention for locally built images

		ui.Info("Source is Dockerfile, building image '%s'...", imageName)
		// Not using the dockerClient here, but passing it to the BuildImageCLIParams
		buildImageParams := docker.BuildImageCLIParams{
			Context: ctx,
			// DockerClient: dockerClient,
			ImageName: imageName,
			Source:    appConfig.Source.Dockerfile,
			EnvVars:   appConfig.Env,
		}
		if err := docker.BuildImageCLI(buildImageParams); err != nil {
			// Distinguish between timeout and cancellation
			if errors.Is(err, context.DeadlineExceeded) {
				return "", fmt.Errorf("failed to build image: operation timed out after %v (%w)", DefaultDeployTimeout, err)
			} else if errors.Is(err, context.Canceled) {
				// Check if the cancellation came from the parent deployCtx
				if ctx.Err() != nil {
					return "", fmt.Errorf("failed to build image: deployment canceled (%w)", ctx.Err())
				}
				// Otherwise, it might be an internal cancellation within the Docker op
				return "", fmt.Errorf("failed to build image: docker operation canceled (%w)", err)
			}
			return "", fmt.Errorf("failed to build image: %w", err)
		}

		return imageName, nil

	case appConfig.Source.Image != nil:
		// Source is a pre-existing image.
		imgSource := appConfig.Source.Image
		imageName := imgSource.Repository
		tag := imgSource.Tag
		if tag == "" {
			tag = "latest" // Default to latest tag if not specified
		}
		imageName = imageName + ":" + tag
		return imageName, nil

	default:
		return "", fmt.Errorf("invalid app source configuration: no source type (Dockerfile or Image) defined for app '%s'", appConfig.Name)
	}

}

func tailDeploymentLog(deploymentID string) error {
	if deploymentID == "" {
		return fmt.Errorf("deployment ID cannot be empty")
	}

	logsPath, err := config.LogsPath()
	if err != nil {
		return fmt.Errorf("failed to get logs path: %w", err)
	}
	logFile := filepath.Join(logsPath, deploymentID+".log")

	// Retry logic for opening the log file
	var file *os.File
	const maxWait = 10 * time.Second
	const retryInterval = 300 * time.Millisecond
	start := time.Now()
	for {
		file, err = os.Open(logFile)
		if err == nil {
			break
		}
		if !os.IsNotExist(err) {
			return fmt.Errorf("failed to open log file: %w", err)
		}
		if time.Since(start) > maxWait {
			return fmt.Errorf("log file %s did not appear after %v", logFile, maxWait)
		}
		time.Sleep(retryInterval)
	}
	defer file.Close()

	reader := bufio.NewReader(file)
	timeout := 2 * time.Minute
	timer := time.NewTimer(timeout)
	defer timer.Stop()

	for {
		lineCh := make(chan string)
		errCh := make(chan error)

		go func() {
			line, err := reader.ReadString('\n')
			if err != nil {
				errCh <- err
			} else {
				lineCh <- line
			}
		}()

		select {
		case line := <-lineCh:
			if !timer.Stop() {
				<-timer.C
			}
			timer.Reset(timeout)

			if line == "[LOG END]\n" || line == "[LOG END]\r\n" {
				return nil // Stop tailing on log end marker
			}
			// Print the log line using the custom print function
			printLogLine(line)
		case <-errCh:
			time.Sleep(300 * time.Millisecond)
		case <-timer.C:
			return fmt.Errorf("log tail timed out after %v of inactivity", timeout)
		}
	}
}

func printLogLine(line string) {
	switch {
	case len(line) > 7 && line[:7] == "[INFO] ":
		ui.Info("%s", line[7:])
	case len(line) > 7 && line[:7] == "[DEBUG] ":
		ui.Debug("%s", line[7:])
	case len(line) > 7 && line[:7] == "[WARN] ":
		ui.Warn("%s", line[7:])
	case len(line) > 8 && line[:8] == "[ERROR] ":
		ui.Error("%s", line[8:])
	default:
		fmt.Printf("%s", line)
	}
}
</file>

</files>
